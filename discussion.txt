Jess: Okay, all right, back to the notion project. I mean back to the Mobin UI labeling project that I'm building. Cool. So let's start all the way from the beginning to talk about the data flow in logic. So in this meeting there are multiple things that I have to figure out. First is my DB design. What are the things that I have to store, finalize and how do I design this table so that it's not too normalized to over normalize by the same time, like open for extension. So I need to be able to clearly articulate that. And then second thing is I need to design, yeah, the front end. It seems like there is a completely new UI page that I have to fix and come up with. Ideally I want something that is capable of impressing them, right? But at the same time I don't want to overdo it. So I'm just going to go through, oh, the high level logic. And another thing that I have to come up with is how do I build this out with good abstraction and clean code? So do I build them in different classes and then each of them will have like, you know, variables and then like how do I do this? Because I don't want it to be just functions and then passing parameters around. It's going to be very messy. Like what are the objects that I have to build that I have to define on top of, you know, my DB database so that I can keep things clean and extremely maintainable and clear for the person who's going to look at my code. So ideally I want to come across as absolutely a good engineer. So those are the two aims of going through this logic. Essentially the entire process will start with user batch uploading a number of pictures. Right? These are going to be pictures of screenshots and ui. And essentially what that does is when I get in the pictures I will do two things. I will normalize the names of the files and then I will then pre process them by making all of them the same size. Right. I will add paddings around them so that we can have a consistent size for everything. And then I will add this to a private Supabase bucket and upload them. Right. Once I upload them, these individual URLs for the pictures will then be pushed into a table which is called screenshots, I believe. Let me just see and go through this clearly.
Jess: Screenshot. Yes. So in this screenshot I will have the screenshot file name and the screenshot URL. Right? And then this screenshot right now has process status, which I still don't know how I can supposed to do that. So from there once all the images have been uploaded, ideally it should trigger a process for this batch and the process for UI extraction starts like this. For each image in the file in the batch, I will extract our Run OpenAI to extract high level UI components. So in this project I will run prompts multiple times. There are different stages. So there seems to be a need to have an abstraction that a class or a object that just runs prompts, locks the time taken to run the prompt in the token and the output too, right? So I need this to be very clear. And then as well as what stage are we in? So this is going to be the first stage, right? Well, as well as what model are we using? Are we using Claude or are we using OpenAI4O or are we using the Moon Dream Vision language model? So there are three models that we are using. But essentially for each image in the batch, I wonder whether this can be a threat, right? So because if I have six images, there's no need for six images to run sequentially, right? Six images, they can run all at the same time. So there has to be a parallelism going on there. So for each image, essentially what I do is I will call OpenAI to extract the high level UI component. Again in this prompt I will extract all the high level components in sections or visual elements. And from there also like what's the description of that? The call to action type, is it primary, secondary or informative? Is it reusable in other screens? And like, you know, other. This is going to be the high level data annotation stuff. So I need, I also need to figure out what needs to be included in the final data annotated output that I want. So anything that I need will be in this prompt, right? Like let's say if I in the future I want to be able to filter by, you know, if it's a header, if it's a card, if it's a page, so it has to be very clear, right? Anyhow, so like let's say if I want to filter by Dashboard, like there must be a way for me to extract it right now. But I don't want to over complicate things. So it's also important for me to dump things down just for the purpose of this pocket. So first for each image I will run a high level UI component. From there on I will get a list of components, high level components in this UI screenshot because in the UI screenshot there are multiple sections. So then from there I will only take the component name and description for the next step and I will use it to extract elements in details. So the next step it's UI extraction, detail UI extraction for each component. So what I'm going to use Claude. What this does is I will have the component name and the description and then have the image, right? The main image screenshot. From there I will have to call Claude to extract all the elements once, right? And Claude will output it in JSON format. And the emphasis of this is to generate all the supporting elements in the component. So let's say if I have a component which is called CartItem, the extract element will have, you know, the cart image, the card item image, cart item, name, card item, weight, card item, price card item, quantity, cart item, you know, like the control, call it quantity control, the minus and plus button, all of this, they make up a component card item, right? Or alternatively if I have something called a transaction. A transaction, then I have the, you know, the amount, the timestamp, the logos for the transactions, is it to Coca Cola and what time and etc. Right. It can also be very generic, high level, like the header, you know, to say welcome. So essentially that's what it does. I want to be able to extract the elements in detail. This is the first prompt and again I have to log the time taken to run this prompt and the token. This is in the extraction phase just now with the extract component phase. This is the extract element phase. The next one is anchoring, simplifying and anchoring. Again it's to run cloth. But this prompt is to simplify, optimize and to anchor the description with visual items to support visual detection. It's also used to catch missing elements. So that's that again we have to have time taken and then the token count. Since I'm using next js, it has to be serverless. There has to be a neat way to do this.
Jess: Oh, again. So once this done that, once this is done. So that's the high level thread, right? So that's the high level thread that runs for each image. So once we are here, the output of this face is going to be a list of components and all the elements description ready for labeling. Annotating with the Moon Dream visual language model. Right. So yeah. Oh, another thing, I want the user to have a good experience. So ideally in the front end there should be a way where I can push what I am doing now currently in the locks or in the front end so that they can in real time see what is happening in the back end. I think this will really seal the things. It will make a good demo experience because it will be able to see what is happening in the back end. Cool. So from there, once, so the thing is, through the entire process, there has to be a way in the front end for them to see extracting what element? Extracting screenshot, what element, blah, blah, in a very clear manner for the visibility of that. Then for each element that is being extracted. So just now for each image we have each component that has a list of elements, right? So now for each element that is extracted, now here, let's say if a component has, let's say an image has five components and each component has three items. So in total I will have 15 elements. Again, this can be a whole thread run by batch. So five at a time. Each detection has. Each element detection has to run on its own. So I will have to run Moon Jingle to call Moon Dream. In Moon Dream, there's no token count. There is only time that I need to log. How much time does it take to extract this element? So going back to the prompt logs that will be stored in the db, this will be in the labeling phase, annotating or labeling phase. Now from there once all the elements have been labeled and annotated, I will have essentially the time and the bounding boxes for each. So from there on I have completed the labeling process. And for each component right now I will have a high level UI output which lists the description of the component, the CTA stuff. And then I have the element extraction output from claude and then I have the simplifying output again from CLAUDE to anchor and support the vlm. And then I have the labeling output. What I'm going to do is I'm going to take the output of the anchoring or simplified phase or VRM optimization phase, take that and I'm gonna group it into one JSON. So for elements within the same component, so for example cart item, each individual element would be cart item name, car item, label, cart item weight, card item, image, cart item, price, item quantity control buttons, car item size, for example, flavor, such and such, all these will be grouped into one file, one JSON object, right? I will flatten it and make it everything into one JSON object. And with the bounding boxes I will draw one image. So in that image using the original image right now, the image will then have bounding boxes of these Label annotated. And what happens here is I will pass this JSON and this image into a accuracy prompt. The accuracy prompt essentially checks the accuracy one by one. So it will take in the JSON and the image and it will basically verify the bounding box whether they are accurate or not. Right. So it will be ranked for a number to 0 to 100. And then if it's not, I will use OpenAI to suggest new coordinates.
Jess: Yeah, so to suggest new coordinates. So from there on it will return the same JSON but with like enriched information, like what's the accuracy? Such and such. And then from there if they suggest for us to override. So essentially, if the accuracy of slow, the plot will also suggest an overwrite box relabel, essentially I will also have the suggested coordinates returned. Right. So which means each identified element will have a percentage. Okay, so now once this is done, I essentially have to write everything into db. Really. So from here I have to first the batch is already written, the screenshot already is already written. So for each screenshot I will have to first write all the components that is identified and all the components that is identified. And then I will, based on the accurate data, the accuracy check output, I will have a list of high accuracy boxes and I will draw an image with those bounding boxes and then those will be uploaded to the SUPABASE private storage in a labeled component or like label images bucket and those will be linked directly to a component. So each component will have an image of like what are the elements that make up them? And then I will also have, you know, the description from the first prompt from OpenAI, the high level UI component with the metadata that will also be stored in the same table in the same row.
Jess: Yeah, those will be done. And then after that I will be. I will need to create. I will need to insert all the identified elements, regardless whether they are accurate or not, into another table that is linked to, that has, that is linked to the component. So in the elements table, probably. So this elements table will then have comp. Will then reference component. And then let's say if I have five elements in this component, all five will go in and then the bounding boxes of each element goes in this table and the accuracy and the description, the labels, they all go in here. And if there are changes that has to be that was made, override or not, they are also there. So that's that. So how does the front end look again? The front end look, while all of this is happening in the back end, first There will be a clear logs that shows progress and give user a strong sense of like things are happening in the background. And then so there's the real time status update and then from there on, when everything is ready, the user can click on the badge and go in. And essentially what I envision is there will be a carousel of the original screenshots of the batch unlabeled on the left side maybe. And then as we click on each image, then on the right side it will show like a list of components and identify and as we click on each component, and then I will just. And then I will be able to show the image of the label element in that component. Right. So the front end will have you know, a list of components and for each component there will be elements, its accuracy and like the bounding boxes, the description. So how do I show this in a very clean manner, right? Because there are a lot of data that needs to be shown. So how do I show this in a very clean manner, right? Because a batch has a lot of images and images, has a lot of components or components and a lot of elements and elements have accuracy, description and stuff. And in the same page there will also be some data or data points essentially like how long did it take to analyze this image? So that is why we have the prompt log table. So all prompts will be logged in. Yeah. How much time did this screenshot take to analyze how many elements? What's the cost? What's the token count? What's the cost? The breakdown all has to be very clear. So yeah.
Jess: And then the each component, right. Before we list the elements, you also have to show the impact of that component, like whether it's a call to action primary button, like what does it play in that UI screenshot. So that has to be shown too. Another feature in the front end is that if I click add button I will be able to draw bounding boxes. And then this is another feature this is not. And then draw bounding boxes. When what essentially that does is when users say click generate element for this box, essentially it will run a prompt directly to call Moon Dream, the vision language model and do a query or detection, you know, like what is in this bounding box? Like come up with as clear of a description as possible. Oh yeah, that's that I think it doesn't have to be here. It can be, you know, a manual labeling thing that we do. But essentially that's the entire pipeline, right. So it's not completely straightforward. So the point again is to think of way to abstract it in a very clean manner. Clean code, clean structure, but still iterate fast and get this shape as soon as possible. So the db. Oh my God. This DB is very hierarchical and very deep, which I need help in redesigning. Essentially, the tables that I identified is batch screenshot. A batch can have one to many screenshots. A screenshot can only have one to one batch. Screenshot will have one to many components and a component will have one to one screenshot. Components belongs to one and only one screenshot. A component will have one too many elements and an element can only belong to one and only one component. So this is very hierarchical. I don't know how to make it, which means if I need to know the batch has how many elements I need to do 1, 2, 3 joins. I have to join by screenshots, join by components and join by elements. This to me feels very deep. Which I don't like, right? I really don't like. So, yeah, that's that. And then prom logs. Prom logs. Another table. Another table is prom logs, essentially prom logs. Has a problem ID time taken to run this prompt. This is essentially to Lark all the prompts ran or like the language model calls, time taken. What step is this? Is it in extraction?
Jess: The extract ui, extract element anchoring, simplification process, labeling accuracy, so that those are the steps right now you'll be logged and then which model are we running? Is it claude? Is it OpenAI? Is it boundary? And then the token count, if it's called an OpenAI, there will be input token count and output token count. So it has to be locked. And then this is the tricky part, because a prompt block belongs to multiple things. It can belong to a screenshot, it can belong to a component, and it can belong to an element id. So in here, in this table, it references three different things and all things are optional too. So it has screenshot id, component ID and element id. This to me just sounds crazy weird, honestly, like generally crazy weird. It seems like there has to be a detection, like a master table that references everything, you know. But then again, if that table references everything, then why do I need this big table? So, yeah, I need help redesigning the DB that way. Like, I'm struggling to come up with a good DB design by Action Point Abstract. This again. My stack is next js. It has to be serverless and on Vercel, deployed on Vercel. My backend is Supabase. And that's it. Thank you.