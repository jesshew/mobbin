This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-04-26T08:54:13.993Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
/
  Users/
    jess/
      Desktop/
        personal git/
          mobbin/
            formobbin/
              lib/
                helpers/
                  detectionHelper.js
                prompt/
                  prompt.yaml
                  prompts.js
                  prompts.ts
                services/
                  ai/
                    ClaudeAIService.ts
                    MoondreamAIService.ts
                    MoondreamDetectionService.js
                    MoondreamVLService.js
                    OpenAIService.js
                  imageServices/
                    BoundingBoxService.js
                    imageFetchingService.ts
                    ImageProcessor.ts
                    screenshotProcessor.ts
                  AccuracyValidationService.ts
                  batchProcessingService.ts
                  DatabaseService.ts
                  MetadataExtractionService.ts
                  ParallelAnnotationService.ts
                  ParallelExtractionService.ts
                  ResultPersistenceService.ts
                  sample_data.ts
                  sample_vision_api_call.txt
                constants.ts
                file-utils.ts
                logger.ts
                schema_v2.sql
                schema_v3.sql
                schema_v4.sql
                storage.ts
                supabase.ts
                supabaseUtils.ts
                utils.ts
              pages/
                api/
                  anthropic.ts
                  batches.ts
                  process-batch.ts
                  upload.ts
              services/
                upload-service.ts
              types/
                BatchProcessingScreenshot.ts
                Component.ts
                DetectionResult.ts

================================================================
Repository Files
================================================================

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/helpers/detectionHelper.js
================
import { processAndSaveByCategory, processImageFile } from '../services/MoondreamDetectionService';
import fs from 'fs';
import path from 'path';

/**
 * Processes a screenshot buffer with provided labels
 * @param {Buffer} screenshotBuffer - The screenshot buffer to process
 * @param {Object} labelsDictionary - Dictionary of labels to descriptions
 * @returns {Promise<Object>} Processing result with categories and output directory
 */
export async function processScreenshotWithLabels(screenshotBuffer, labelsDictionary) {
  if (!screenshotBuffer) {
    throw new Error('Screenshot buffer is required');
  }
  
  if (!labelsDictionary || Object.keys(labelsDictionary).length === 0) {
    throw new Error('Labels dictionary is required and cannot be empty');
  }
  
  return processAndSaveByCategory(screenshotBuffer, labelsDictionary);
}

/**
 * Processes a screenshot file with provided labels
 * @param {string} screenshotPath - Path to the screenshot file
 * @param {Object} labelsDictionary - Dictionary of labels to descriptions
 * @returns {Promise<Object>} Processing result with categories and output directory
 */
export async function processScreenshotFileWithLabels(screenshotPath, labelsDictionary) {
  if (!screenshotPath) {
    throw new Error('Screenshot path is required');
  }
  
  if (!labelsDictionary || Object.keys(labelsDictionary).length === 0) {
    throw new Error('Labels dictionary is required and cannot be empty');
  }
  
  return processImageFile(screenshotPath, labelsDictionary);
}

/**
 * Loads a labels dictionary from a JSON file
 * @param {string} jsonPath - Path to the JSON file containing labels
 * @returns {Promise<Object>} Labels dictionary
 */
export async function loadLabelsFromJson(jsonPath) {
  try {
    const fileData = await fs.promises.readFile(jsonPath, 'utf8');
    return JSON.parse(fileData);
  } catch (err) {
    console.error(`Error loading labels from JSON file: ${err}`);
    throw err;
  }
}

/**
 * Saves labels dictionary to a JSON file
 * @param {Object} labelsDictionary - The labels dictionary to save
 * @param {string} outputPath - Path to save the JSON file
 * @returns {Promise<string>} Path to the saved file
 */
export async function saveLabelsToJson(labelsDictionary, outputPath) {
  try {
    const dirPath = path.dirname(outputPath);
    await fs.promises.mkdir(dirPath, { recursive: true });
    
    await fs.promises.writeFile(
      outputPath, 
      JSON.stringify(labelsDictionary, null, 2),
      'utf8'
    );
    
    return outputPath;
  } catch (err) {
    console.error(`Error saving labels to JSON file: ${err}`);
    throw err;
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/prompt/prompt.yaml
================
- prompt_name: extraction_prompt
  prompt_description: Extract UI Components
  prompt_body: |
    Extract High-Level UI Components with Functional Metadata
    You are given a UI screenshot from a mobile or web application.

    🎯 Your Task:
    Identify and return only the key UI components or main sections visible in the interface. These should be semantically meaningful blocks that represent distinct parts of the user experience — not low-level elements like buttons, icons, or text unless they are the central interactive unit themselves.

    🧠 Guidelines:
    Do not list every visual element — focus only on sections or interactive units a product designer or UX researcher would define.

    Group smaller elements into their logical parent component (e.g., quantity controls, icons, labels → Cart Item).

    Avoid granular components unless they are standalone CTAs or decision points.

    Each identified component should be visually and functionally distinct.

    🧾 Output Format: JSON List
    For each top-level component, include the following fields:

    component_name: A human-readable name for the section (e.g., "Cart Item", "Header", "Promocode Section")

    description: A short, clear description of the section and what's visually included

    impact_on_user_flow: A sentence describing the component's purpose or value in the overall experience

    cta_type: If applicable, note if this section supports a Primary, Secondary, or Informational action

    is_reused_in_other_screens: Boolean — is this component likely reused across the app?

    likely_interaction_type: A list of expected user interactions (e.g., "tap", "scroll", "none")

    flow_position: Where this component sits in the typical user journey (e.g., "Checkout - Cart Review")
  prompt_sample_output: |
    [
      {
        "component_name": "Cart Item",
        "description": "Visual block showing product image, name, price, and quantity controls.",
        "impact_on_user_flow": "Enables users to review and modify the items before purchase.",
        "cta_type": "Secondary",
        "is_reused_in_other_screens": true,
        "likely_interaction_type": ["tap", "stepper"],
        "flow_position": "Checkout - Cart Review"
      },
      {
        "component_name": "Delivery Options",
        "description": "Section showing available delivery choices with cost and selection state.",
        "impact_on_user_flow": "Lets the user choose a preferred delivery method before checkout.",
        "cta_type": "Primary",
        "is_reused_in_other_screens": true,
        "likely_interaction_type": ["tap (select radio)"],
        "flow_position": "Checkout - Shipping Selection"
      },
      {
        "component_name": "Promocode Section",
        "description": "Input area for applying promotional codes with validation feedback.",
        "impact_on_user_flow": "Allows discount application to influence purchase behavior.",
        "cta_type": "Secondary",
        "is_reused_in_other_screens": false,
        "likely_interaction_type": ["tap", "keyboard input"],
        "flow_position": "Checkout - Discount Application"
      }
    ]

- prompt_name: extract_elements_prompt_v1
  prompt_description: Hierarchical UI Analysis with Complete Coverage v1
  prompt_body: |
    Analyze the provided UI screenshot in combination with the given component list.
    Your task is to detect and describe all key UI elements in the image, using the component list to guide semantic grouping, naming, and nesting.

    📥 Inputs:
    A UI screenshot

    A list of component categories, such as:

    Header Section (includes title "Top Up Receipt" with background confetti design)
    Success Badge (includes checkmark icon inside a hexagon)
    Top Up Confirmation Section (displays "Top Up Success" message and confirmation text)
    Total Top Up Amount (displays the amount "$132.00")
    Top Up Destination Card (includes card label "Wally Virtual Card", masked card number, and timestamp)
    Primary Action Button (Done)
    Secondary Action Link (Top up more money)

    🧠 What to Do:
    Map every visual component to its corresponding category or subcategory, based on content and context.

    Create subcategories if they add clarity (e.g., "Top Up Destination Card > Card Label").

    Include all visible elements, especially text labels, icons, buttons, and values — no matter how small.

    Describe each component with these details:

    Appearance: Shape, icon, color, text, visual style

    Function: Purpose or expected user interaction

    Positioning: Use screen regions (e.g., "centered near top", "bottom-right quadrant")

    State: Selected, default, disabled, etc.

    Interaction Type: Tappable, static, scrollable, etc.

    Avoid redundancy — include no more than 1 to 2 visual anchors if necessary for clarity (e.g., "below '$132.00'").

    🧾 Output Format:
    Return a valid JSON object

    Keys should represent the hierarchical path using > as a delimiter

    Example: "Top Up Destination Card > Card Label"

    Values should be rich descriptions of the visual component

    Use a flat structure — no nested objects

    No trailing commas

    📌 Output Requirements:
    Include all meaningful elements — especially text, values, and labels

    Group logically using the provided categories

    Add subcategories when appropriate

    Keep descriptions precise and visual-model friendly

    Use flat JSON (hierarchy via keys only)
  prompt_sample_output: |
    {
      "Header Section > Title": "Bold white text reading 'Top Up Receipt', centered at the top of the screen with a colorful confetti background",
      "Success Badge > Icon": "Hexagon-shaped container with a white checkmark icon inside, green background, centered below the title",
      "Top Up Confirmation Section > Main Message": "Bold text 'Top Up Success', centered below the success badge",
      "Top Up Confirmation Section > Subtext": "Gray text confirming transaction, reading 'Your money has been added to your card'",
      "Total Top Up Amount > Value": "Large bold text '$132.00', centered and prominent near the middle of the screen",
      "Top Up Destination Card > Card Label": "White text 'Wally Virtual Card' at the top of the destination card",
      "Top Up Destination Card > Masked Card Number": "Text showing masked card '•••• 4568' below the card label",
      "Top Up Destination Card > Timestamp": "Small gray text 'Today, 12:45 PM' at the bottom-right of the card",
      "Primary Action Button > Label": "Full-width green button with white text 'Done' at the bottom of the screen, tappable",
      "Secondary Action Link > Label": "Text link 'Top up more money' below the Done button, teal-colored and tappable"
    }

- prompt_name: extract_elements_prompt_v2
  prompt_description: Hierarchical UI Analysis with Complete Coverage v2
  prompt_body: |    
    You are given a UI screenshot and a list of component categories.

    Your task is to detect and describe all key UI elements in the image, mapping them into logical component groupings and subcategories where appropriate.

    📥 Inputs:
    A UI screenshot

    A list of component categories, such as:

    Cart Header (includes title "Cart, 3 items" and close/dismiss button)

    Delivery Option - Standard (40–60 minutes, Free, with selection indicator)

    Delivery Option - Express (15–25 minutes, $2.00, with lightning icon and selection indicator)

    Cart Items (image, name, weight/volume, price, quantity controls)

    Promocode Section (input field with applied code)

    Order Summary Footer (total price and confirm button)

    🧠 What to Do:
    Identify and label all meaningful visual components, including:

    Section titles and structural elements

    Text labels and values (e.g., item names, prices, promo codes)

    Icons and small indicators (e.g., close buttons, checkmarks, lightning bolts)

    Buttons or interaction controls, no matter how small (e.g., plus/minus steppers, selection dots)

    Decorative elements only if they communicate meaning (e.g., badges, icons, state indicators)

    Group elements under appropriate parent categories and subcategories:

    Use semantic hierarchy like "Cart Item - Gnocchi > Quantity Controls > Increase Button"

    Add subcategories to clarify function or layout

    Describe each element using:

    Appearance: shape, icon, text, colors, visual styling

    Function: purpose or expected behavior (e.g., increase quantity, confirm order)

    Positioning: use simple visual references ("top-right", "below item title", "far right of row")

    State: active, default, selected, applied, disabled

    Interaction Type: tap, scroll, static, stepper, etc.

    Anchor the description if needed, using 1–2 visual references, but do not make anchors the focus.

    🧾 Output Format:
    Return a valid flat JSON object

    Each key represents a component path using > as a delimiter

    Example: "Cart Item - Gnocchi > Quantity Controls > Increase Button"

    Each value is a descriptive sentence capturing what the element is and does

    No nested objects

    No trailing commas

  prompt_sample_output: |
    {
      "Cart Item - Gnocchi > Image": "Photo of a plated gnocchi dish with mushroom gravy, shown in a square image container on the far left of the cart item row, beside the item title.",
      
      "Cart Item - Gnocchi > Title": "Bold black text reading 'Gnocchi with mushroom gravy', positioned to the right of the dish image and above the price label.",
      
      "Cart Item - Gnocchi > Weight": "Gray text label displaying '230g', located directly beneath the title 'Gnocchi with mushroom gravy'.",
      
      "Cart Item - Gnocchi > Price": "Orange-colored text reading '$5.60', placed below the item title and to the left of the quantity controls.",
      
      "Cart Item - Gnocchi > Quantity Controls > Decrease Button": "Minus icon for decreasing the quantity of 'Gnocchi with mushroom gravy', located on the far left of the quantity controls row, immediately to the right of the '$5.60' price label.",

      "Cart Item - Gnocchi > Quantity Controls > Count Display": "Centered numeric label showing '1' as the selected quantity for 'Gnocchi with mushroom gravy', placed between the minus and plus icons, to the right of the decrease button.",

      "Cart Item - Gnocchi > Quantity Controls > Increase Button": "Plus icon for increasing the quantity of 'Gnocchi with mushroom gravy', positioned on the far right of the quantity controls row, immediately after the '1' quantity label."

    } 

- prompt_name: anchor_elements_prompt
  prompt_description: Clarify UI Descriptions with Subtle Anchors
  prompt_body: |
    Rewrite each UI component description to improve clarity and spatial grounding using subtle visual anchors.
    Your input includes:
      - A UI screenshot
      - A flat JSON list of UI components and their basic descriptions

    Your task is to revise each description to:
      - Clearly describe the visual component itself — including shape, icon type, text, and visual purpose
      - Optionally include 1–2 subtle visual anchors (e.g., nearby labels or icons)
      - Ensure anchors support locating the component but do not become the focus
      - Keep anchor references subordinate, e.g., "below the label 'Netflix'", not "Netflix is above this"
      - Avoid overly precise spatial phrases or coordinate-like descriptions

    guidelines:
      - Start by describing what the component is, including visual style and function
      - Add up to 2 anchor references only if needed for disambiguation
      - Place anchors after the main description
      - Keep all descriptions friendly for vision-language models:
          - Avoid layout jargon
          - Avoid unnecessary nesting or abstraction
      - Maintain flat JSON structure
      - Do not change any keys — revise values only

    examples:
      - bad: "Transaction Item 3 > Date Time": "Gray text 'Aug 12, 07:25 PM' under 'Netflix'"
        improved: "Transaction Item 3 > Date Time": "Gray timestamp reading 'Aug 12, 07:25 PM', displayed under the 'Netflix' merchant label"
      - bad: "Header > Notification Icon": "Bell icon with green dot, opposite profile picture"
        improved: "Header > Notification Icon": "Circular bell icon with a green dot inside, in the top-right corner, opposite the profile picture"
      - bad: "Transaction Item 2 > Merchant Logo": "DKNY logo on left side, positioned below the Starbucks transaction"
        improved: "Transaction Item 2 > Merchant Logo": "Circular icon displaying the DKNY logo on white background, beside the 'DKNY' merchant name"

      - bad:     "Price Chart > Time Labels": "Gray time markers from '0am' to '7pm' running along the bottom edge of the chart, indicating the hourly breakdown",
      - improved:   "Price Chart > Time Labels": "Gray time markers from '0am', '11am', '12pm', '1pm', '2pm', '3pm', '4pm', '5pm', '6pm', '7pm' running along the bottom edge of the chart, indicating the hourly breakdown",

    output_format:
      - type: flat_json
      - keys: unchanged
      - values: rewritten descriptions with optional anchors
      - structure: flat

  prompt_sample_output: |
    {
    
    "Success Badge": "Orange hexagonal badge containing a centered white checkmark icon, positioned prominently in the upper portion of the white screen area, serving as the visual confirmation indicator.",
    
    "Top Up Confirmation Section > Title": "Bold black text 'Top Up Success' displayed in medium-large font",
    
    "Top Up Confirmation Section > Confirmation Text": "Light gray descriptive text reading 'Your top up has been successfully done.' centered horizontally below the 'Top Up Success' title.",
    
    "Total Top Up Amount > Label": "Light gray text 'Total Top Up' in small font, positioned centrally above the transaction amount.",
    
    "Total Top Up Amount > Value": "Large bold black text '$132.00' displayed prominently in the middle section of the receipt, showing the transaction amount with currency symbol and decimal points.",
    
    "Top Up Destination Card": "Off-white rectangular container with subtle rounded corners and light shadow effect, occupying the mid-lower portion of the screen, housing the recipient card information and details.",
    
    "Top Up Destination Card > Label": "Small gray text 'Top up destination' serving as a section header for the recipient information.",
    
    "Top Up Destination Card > Card Icon": "Small square icon containing the text 'Wally' on a light green background, positioned at the left edge inside the destination card container, representing the card provider.",
    
    "Top Up Destination Card > Card Name": "Bold black text 'Wally Virtual Card' displayed next to the Wally logo icon",
    
    "Top Up Destination Card > Card Number": "Light gray text '0318-1608-2105' displaying the card's identification number",
    
    "Top Up Destination Card > Timestamp": "Light gray text '3:02 PM' showing the transaction time, positioned next to a card number with a bullet point separator between them.",
    
    "Primary Action Button - Done": "Large mint green rectangular button with rounded corners with white text 'Done', positioned in the lower section of the screen",
    
    "Secondary Action Link - Top up more money": "Green text link 'Top up more money' in smaller font, centered horizontally below the Done button"
    }

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/prompt/prompts.js
================
export const ACCURACY_VALIDATION_PROMPT_v0 = `
You are an expert at evaluating UI component detection results.

I'll provide you with an image and JSON data containing element detections. Each element has:
- A label
- A description
- A bounding box with coordinates (x_min, y_min, x_max, y_max)

Your task is to analyze each detected element and:
1. Assign an accuracy score from 0-100 based on how well the bounding box captures the described element
2. For elements with low accuracy (below 50%), suggest improved coordinates

Return a structured JSON with your evaluation:
{
  "elements": [
    {
      "label": "element_label",
      "accuracy_score": number,
      "suggested_coordinates": { "x_min": number, "y_min": number, "x_max": number, "y_max": number } // Only for low accuracy
    },
    ...
  ]
}

Focus on:
- Does the box properly contain the described element?
- Does it include too much or too little of the surrounding area?
- Are the edges properly aligned with the element's visual boundaries?
- Is this the correct element based on the label and description?

Provide suggested coordinates only for elements where you can confidently improve the detection.
`;

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/prompt/prompts.ts
================
export const EXTRACTION_PROMPT_v0 = `
    Extract High-Level UI Components with Functional Metadata

    <instructions>
    You are given a UI screenshot from a mobile or web application.

    🎯 Your Task:
    Identify and return only the key UI components or main sections visible in the interface. These should be semantically meaningful blocks that represent distinct parts of the user experience — not low-level elements like buttons, icons, or text unless they are the central interactive unit themselves.

    🧠 Guidelines:
    Do not list every visual element — focus only on sections or interactive units a product designer or UX researcher would define.

    Group smaller elements into their logical parent component (e.g., quantity controls, icons, labels → Cart Item).

    Avoid granular components unless they are standalone CTAs or decision points.

    Each identified component should be visually and functionally distinct.

    🧾 Output Format: JSON List
    For each top-level component, include the following fields:

    component_name: A human-readable name for the section (e.g., "Cart Item", "Header", "Promocode Section")

    description: A short, clear description of the section and what's visually included

    impact_on_user_flow: A sentence describing the component's purpose or value in the overall experience

    cta_type: If applicable, note if this section supports a Primary, Secondary, or Informational action

    is_reused_in_other_screens: Boolean — is this component likely reused across the app?

    likely_interaction_type: A list of expected user interactions (e.g., "tap", "scroll", "none")

    flow_position: Where this component sits in the typical user journey (e.g., "Checkout - Cart Review")
    </instructions>

    <sample_output> 
    [
      {
        "component_name": "Cart Item",
        "description": "Visual block showing product image, name, price, and quantity controls.",
        "impact_on_user_flow": "Enables users to review and modify the items before purchase.",
        "cta_type": "Secondary",
        "is_reused_in_other_screens": true,
        "likely_interaction_type": ["tap", "stepper"],
        "flow_position": "Checkout - Cart Review"
      },
      {
        "component_name": "Delivery Options",
        "description": "Section showing available delivery choices with cost and selection state.",
        "impact_on_user_flow": "Lets the user choose a preferred delivery method before checkout.",
        "cta_type": "Primary",
        "is_reused_in_other_screens": true,
        "likely_interaction_type": ["tap (select radio)"],
        "flow_position": "Checkout - Shipping Selection"
      },
      {
        "component_name": "Promocode Section",
        "description": "Input area for applying promotional codes with validation feedback.",
        "impact_on_user_flow": "Allows discount application to influence purchase behavior.",
        "cta_type": "Secondary",
        "is_reused_in_other_screens": false,
        "likely_interaction_type": ["tap", "keyboard input"],
        "flow_position": "Checkout - Discount Application"
      }
    ]
    </sample_output>
    `;


export const EXTRACTION_PROMPT_v1 = `
<identity> 
You are a structured AI UI analysis agent designated to extract high-level UI components from a UI screenshot. 
You are optimized for precision in semantic segmentation, resistance to overclassification, and strict hierarchical grouping. 
You do not generate unnecessary information. You do not speculate. 
</identity>

<input>  
- A UI screenshot
</input>

<task_execution>
Upon receipt of a visual UI input (e.g., screenshot):

DO extract only high-level, semantically distinct interface components.

DO NOT include low-level atomic elements unless they act as standalone interaction units (e.g., isolated CTAs).

DO group smaller atomic items into meaningful parent components (e.g., icons + labels + controls → "Cart Item").

DO NOT oversegment. Avoid listing trivial or decorative UI parts.

All extracted components MUST represent functional blocks relevant to product design, UX analysis, or interaction mapping.

Every output MUST be formatted as a structured JSON array conforming to the schema in <output_format>.
</task_execution>

<output_format>
Please output ONE string of flat JSON object.

Each object in the output array MUST include the following keys:

component_name → string: Human-readable identifier of the component (e.g., "Header", "Cart Item")

description → string: Summary of visual content and layout within the component

impact_on_user_flow → string: Explanation of how the component contributes to user experience or decision-making

cta_type → enum: One of [Primary, Secondary, Informational] if any CTA exists; otherwise omit or set to null

is_reused_in_other_screens → boolean: TRUE if the component is expected to appear across multiple screens

likely_interaction_type → list[string]: User actions expected (e.g., ["tap"], ["scroll"], ["keyboard input"])

flow_position → string: UX journey placement (e.g., "Checkout - Cart Review")
</output_format>

<example_output> 
"[
  {
    "component_name": "Cart Item",
    "description": "Visual block showing product image, name, price, and quantity controls.",
    "impact_on_user_flow": "Enables users to review and modify the items before purchase.",
    "cta_type": "Secondary",
    "is_reused_in_other_screens": true,
    "likely_interaction_type": ["tap", "stepper"],
    "flow_position": "Checkout - Cart Review"
  },
  {
    "component_name": "Delivery Options",
    "description": "Section showing available delivery choices with cost and selection state.",
    "impact_on_user_flow": "Lets the user choose a preferred delivery method before checkout.",
    "cta_type": "Primary",
    "is_reused_in_other_screens": true,
    "likely_interaction_type": ["tap (select radio)"],
    "flow_position": "Checkout - Shipping Selection"
  },
  {
    "component_name": "Promocode Section",
    "description": "Input area for applying promotional codes with validation feedback.",
    "impact_on_user_flow": "Allows discount application to influence purchase behavior.",
    "cta_type": "Secondary",
    "is_reused_in_other_screens": false,
    "likely_interaction_type": ["tap", "keyboard input"],
    "flow_position": "Checkout - Discount Application"
  }
]"
</example_output>
`

export const EXTRACTION_PROMPT_v2 = `

Prompt: Enhanced High-Level UI Component Extraction with Partial Visibility Awareness

<identity>  
You are a structured AI UI analysis agent designated to extract high-level UI components from a UI screenshot.  
You are optimized for precision in semantic segmentation, resistance to overclassification, and strict hierarchical grouping.  
You do not generate unnecessary information. You do not speculate.  
You must also account for **partially visible** components that are recognizable and potentially interactive.  
</identity>

<input>  
- A UI screenshot  
</input>

<task_execution>  
Upon receipt of a visual UI input (e.g., screenshot):

DO extract high-level, semantically distinct interface components, even when they are **partially visible**.  
DO NOT include low-level atomic elements unless they act as standalone interaction units (e.g., isolated CTAs).  
DO group smaller atomic items into meaningful parent components (e.g., icons + labels + controls → "Cart Item").  
DO NOT oversegment. Avoid listing trivial or decorative UI parts.  

All extracted components MUST represent functional blocks relevant to product design, UX analysis, or interaction mapping.

Use discretion to determine whether a partially shown component offers enough visual or functional cues to justify inclusion.

Every output MUST be formatted as a structured JSON array conforming to the schema in <output_format>.  
</task_execution>

<output_format>  
Please output ONE string of flat JSON object.  

Each object in the output array MUST include the following keys:  

component_name → string: Human-readable identifier of the component (e.g., "Header", "Cart Item")  
description → string: Summary of visual content and layout within the component  
impact_on_user_flow → string: Explanation of how the component contributes to user experience or decision-making  
cta_type → enum: One of [Primary, Secondary, Informational] if any CTA exists; otherwise omit or set to null  
is_reused_in_other_screens → boolean: TRUE if the component is expected to appear across multiple screens  
likely_interaction_type → list[string]: User actions expected (e.g., ["tap"], ["scroll"], ["keyboard input"])  
flow_position → string: UX journey placement (e.g., "Checkout - Cart Review")  
</output_format>

<example_output> 
"[
  {
    "component_name": "Cart Item",
    "description": "Visual block showing product image, name, price, and quantity controls.",
    "impact_on_user_flow": "Enables users to review and modify the items before purchase.",
    "cta_type": "Secondary",
    "is_reused_in_other_screens": true,
    "likely_interaction_type": ["tap", "stepper"],
    "flow_position": "Checkout - Cart Review"
  },
  {
    "component_name": "Delivery Options",
    "description": "Section showing available delivery choices with cost and selection state.",
    "impact_on_user_flow": "Lets the user choose a preferred delivery method before checkout.",
    "cta_type": "Primary",
    "is_reused_in_other_screens": true,
    "likely_interaction_type": ["tap (select radio)"],
    "flow_position": "Checkout - Shipping Selection"
  },
  {
    "component_name": "Partial Debit Card",
    "description": "Partially visible card element showing the top edge and part of the card number, suggesting the presence of a second linked payment method.",
    "impact_on_user_flow": "Indicates additional card options or account data, enhancing user context and potentially prompting a scroll interaction.",
    "cta_type": null,
    "is_reused_in_other_screens": true,
    "likely_interaction_type": ["scroll"],
    "flow_position": "Dashboard - Card Carousel"
  },
  {
    "component_name": "Promocode Section",
    "description": "Input area for applying promotional codes with validation feedback.",
    "impact_on_user_flow": "Allows discount application to influence purchase behavior.",
    "cta_type": "Secondary",
    "is_reused_in_other_screens": false,
    "likely_interaction_type": ["tap", "keyboard input"],
    "flow_position": "Checkout - Discount Application"
  }
]"
</example_output>
`

export const EXTRACTION_PROMPT_v3 = `
<identity>  
You are a structured AI UI analysis agent designated to extract high-level UI components from a UI screenshot.  
You are optimized for precision in semantic segmentation, resistance to overclassification, and strict hierarchical grouping.  
You do not generate unnecessary information. You do not speculate.  
You must also account for **partially visible** components that are recognizable and potentially interactive.  
</identity>

<input>  
- A UI screenshot  
</input>

<task_execution>
When you receive a screenshot, follow these rules:
Find Real UI Components
- Only include elements that do something or show something important (like product cards, delivery options, input fields).
Handle Repeated Items as Separate
- If something repeats (like cart items, delivery rows), list each one as its own component.
- Name them clearly, like "Cart Item 1", "Cart Item 2", not "Cart List".

Include Partially Visible Items
- If a card or button is cut off but still recognizable, include it.
- Group Small Things if They Belong Together
- If an image, label, and button work together (like in a product card), group them as one component.
Ignore Decorative Stuff
- Don’t include backgrounds, dividers, icons that don’t do anything, or layout-only elements.

Every output MUST be formatted as a structured JSON array conforming to the schema in <output_format>.  
</task_execution>

<output_format>  
Please output ONE string of flat JSON object.  

Each object in the output array MUST include the following keys:  

component_name → string: Human-readable identifier of the component (e.g., "Header", "Cart Item")  
description → string: Summary of visual content and layout within the component  
impact_on_user_flow → string: Explanation of how the component contributes to user experience or decision-making  
cta_type → enum: One of [Primary, Secondary, Informational] if any CTA exists; otherwise omit or set to null  
is_reused_in_other_screens → boolean: TRUE if the component is expected to appear across multiple screens  
likely_interaction_type → list[string]: User actions expected (e.g., ["tap"], ["scroll"], ["keyboard input"])  
flow_position → string: UX journey placement (e.g., "Checkout - Cart Review")  
</output_format>

<example_output> 
"[ 
  {
    "component_name": "Cart Item 1",
    "description": "Visual block showing a thumbnail image of gnocchi, product title, portion weight, price, and quantity selector with minus and plus buttons.",
    "impact_on_user_flow": "Enables users to review and modify the items before purchase.",
    "cta_type": "Secondary",
    "is_reused_in_other_screens": true,
    "likely_interaction_type": ["tap", "stepper"],
    "flow_position": "Checkout - Cart Review"
  },
  {
    "component_name": "Standard Delivery Option",
    "description": "Row showing black text 'Standard delivery, 40–60 minutes' and a filled orange selection circle indicating this option is selected.",
    "impact_on_user_flow": "Lets the user choose a preferred delivery method before checkout.",
    "cta_type": "Primary",
    "is_reused_in_other_screens": true,
    "likely_interaction_type": ["tap (select radio)"],
    "flow_position": "Checkout - Shipping Selection"
  },
  {
    "component_name": "Partial Debit Card",
    "description": "Partially visible card element showing the top edge and part of the card number, suggesting the presence of a second linked payment method.",
    "impact_on_user_flow": "Indicates additional card options or account data, enhancing user context and potentially prompting a scroll interaction.",
    "cta_type": null,
    "is_reused_in_other_screens": true,
    "likely_interaction_type": ["scroll"],
    "flow_position": "Dashboard - Card Carousel"
  },
  {
    "component_name": "Promocode Section",
    "description": "Input area for applying promotional codes with validation feedback.",
    "impact_on_user_flow": "Allows discount application to influence purchase behavior.",
    "cta_type": "Secondary",
    "is_reused_in_other_screens": false,
    "likely_interaction_type": ["tap", "keyboard input"],
    "flow_position": "Checkout - Discount Application"
  }
]"
</example_output>

`
export const EXTRACTION_PROMPT_v4 = `
<identity>  
You are a structured AI UI analysis agent designated to extract high-level UI components from a UI screenshot.  
You are optimized for precision in semantic segmentation, resistance to overclassification, and strict hierarchical grouping.  
You do not generate unnecessary information. You do not speculate.  
You must also account for **partially visible** components that are recognizable and potentially interactive.  
</identity>

<input>  
- A UI screenshot  
</input>

<task_execution>
When you receive a screenshot, follow these rules:
Find Real UI Components
- Only include elements that do something or show something important (like product cards, delivery options, input fields).
Handle Repeated Items as Separate
- If something repeats (like cart items, delivery rows), list each one as its own component.
- Name them clearly, like "Cart Item 1", "Cart Item 2", not "Cart List".
- *Exception*: DO NOT count Navigation Bar ITEMS as separate components.

Include Partially Visible Items
- If a card or button is cut off but still recognizable, include it.
- Group Small Things if They Belong Together
- If an image, label, and button work together (like in a product card), group them as one component.
Ignore Decorative Stuff
- Don’t include backgrounds, dividers, icons that don’t do anything, or layout-only elements.

Every output MUST be formatted as a structured JSON array conforming to the schema in <output_format>.  
</task_execution>

<output_format>  
Please output ONE string of flat JSON object.  

Each object in the output array MUST include the following keys:  

component_name → string: Human-readable identifier of the component (e.g., "Header", "Cart Item")  
description → string: Summary of visual content and layout within the component  
impact_on_user_flow → string: Explanation of how the component contributes to user experience or decision-making  
cta_type → enum: One of [Primary, Secondary, Informational] if any CTA exists; otherwise omit or set to null  
is_reused_in_other_screens → boolean: TRUE if the component is expected to appear across multiple screens  
likely_interaction_type → list[string]: User actions expected (e.g., ["tap"], ["scroll"], ["keyboard input"])  
flow_position → string: UX journey placement (e.g., "Checkout - Cart Review")  
</output_format>

<example_output> 
"[
{
    "component_name": "Bottom Navigation Bar",
    "description": "Fixed bar with multiple navigation icons and labels (including highlighted Home), facilitating access to main app sections.",
    "impact_on_user_flow": "Enables seamless movement between primary areas of the app.",
    "cta_type": "Secondary",
    "is_reused_in_other_screens": true,
    "likely_interaction_type": ["tap"],
    "flow_position": "Global Navigation"\n' +
  },
  {
    "component_name": "Cart Item 1",
    "description": "Visual block showing a thumbnail image of gnocchi, product title, portion weight, price, and quantity selector with minus and plus buttons.",
    "impact_on_user_flow": "Enables users to review and modify the items before purchase.",
    "cta_type": "Secondary",
    "is_reused_in_other_screens": true,
    "likely_interaction_type": ["tap", "stepper"],
    "flow_position": "Checkout - Cart Review"
  },
  {
    "component_name": "Standard Delivery Option",
    "description": "Row showing black text 'Standard delivery, 40–60 minutes' and a filled orange selection circle indicating this option is selected.",
    "impact_on_user_flow": "Lets the user choose a preferred delivery method before checkout.",
    "cta_type": "Primary",
    "is_reused_in_other_screens": true,
    "likely_interaction_type": ["tap (select radio)"],
    "flow_position": "Checkout - Shipping Selection"
  },
  {
    "component_name": "Partial Debit Card",
    "description": "Partially visible card element showing the top edge and part of the card number, suggesting the presence of a second linked payment method.",
    "impact_on_user_flow": "Indicates additional card options or account data, enhancing user context and potentially prompting a scroll interaction.",
    "cta_type": null,
    "is_reused_in_other_screens": true,
    "likely_interaction_type": ["scroll"],
    "flow_position": "Dashboard - Card Carousel"
  },
  {
    "component_name": "Promocode Section",
    "description": "Input area for applying promotional codes with validation feedback.",
    "impact_on_user_flow": "Allows discount application to influence purchase behavior.",
    "cta_type": "Secondary",
    "is_reused_in_other_screens": false,
    "likely_interaction_type": ["tap", "keyboard input"],
    "flow_position": "Checkout - Discount Application"
  }
]"
</example_output>

`
export const EXTRACTION_PROMPT_v5 = `
<identity>  
You are a structured AI UI analysis agent designated to extract high-level UI components from a UI screenshot.  
You are optimized for precision in semantic segmentation, resistance to overclassification, and strict hierarchical grouping.  
You do not generate unnecessary information. You do not speculate.  
You must also account for **partially visible** components that are recognizable and potentially interactive.  
</identity>

<input>  
- A UI screenshot  
</input>

<task_execution>
When you receive a screenshot, follow these rules:
Find Real UI Components
- Only include elements that do something or show something important (like product cards, delivery options, input fields).
Handle Repeated Items as Separate
- If something repeats (like cart items, delivery rows), list each one as its own component.
- Name them clearly, like "Cart Item 1", "Cart Item 2", not "Cart List".
- *Exception*: DO NOT count Navigation Bar ITEMS as separate components.

Include Partially Visible Items
- If a card or button is cut off but still recognizable, include it.
- Group Small Things if They Belong Together
- If an image, label, and button work together (like in a product card), group them as one component.
Ignore Decorative Stuff
- Don’t include backgrounds, dividers, icons that don’t do anything, or layout-only elements.


<output_format>  
[
  {
    "component_name": "string",
    "description": "string"
  },
  ...
]
</output_format>

<example_output>"
[
  {
    "component_name": "Bottom Navigation Bar",
    "description": "Fixed bar with multiple navigation icons and labels (including highlighted Home), facilitating access to main app sections."
  },
  {
    "component_name": "Cart Item 1",
    "description": "Visual block showing a thumbnail image of gnocchi, product title, portion weight, price, and quantity selector with minus and plus buttons."
  },
  {
    "component_name": "Standard Delivery Option",
    "description": "Row showing black text 'Standard delivery, 40–60 minutes' and a filled orange selection circle indicating this option is selected."
  },
  {
    "component_name": "Partial Debit Card",
    "description": "Partially visible card element showing the top edge and part of the card number, suggesting the presence of a second linked payment method."
  },
  {
    "component_name": "Promocode Section",
    "description": "Input area for applying promotional codes with validation feedback."
  }
]"
</example_output>
`

export const EXTRACTION_PROMPT_v6 = `
    Extract High-Level UI Components with Functional Metadata

    <instructions>
    You are given a UI screenshot from a mobile or web application.

    🎯 Your Task:
    Identify and return only the key UI components or main sections visible in the interface. These should be semantically meaningful blocks that represent distinct parts of the user experience — not low-level elements like buttons, icons, or text unless they are the central interactive unit themselves.

    🧠 Guidelines:
    Do not list every visual element — focus only on sections or interactive units a product designer or UX researcher would define.

    Group smaller elements into their logical parent component (e.g., quantity controls, icons, labels → Cart Item).

    Avoid granular components unless they are standalone CTAs or decision points.

    Include Partially Visible Items
    - If a card or button is cut off but still recognizable, include it.
    - Group Small Things if They Belong Together
    - If an image, label, and button work together (like in a product card), group them as one component.

    Ignore Decorative Stuff
    - Don’t include backgrounds, dividers, icons that don’t do anything, or layout-only elements.

    Each identified component should be visually and functionally distinct.

    🧾 Output Format: JSON List
    For each top-level component, include the following fields:

    component_name: A human-readable name for the section (e.g., "Cart Item", "Header", "Promocode Section")

    description: A short, clear description of the section and what's visually included

    </instructions>

    <sample_output> 
    [
      {
        "component_name": "Cart Item List",
        "description": "Visual block showing product image, name, price, and quantity controls.",
      },
      {
        "component_name": "Delivery Options",
        "description": "Section showing available delivery choices with cost and selection state.",
      },
      {
        "component_name": "Promocode Section",
        "description": "Input area for applying promotional codes with validation feedback.",
      },
      {
    "component_name": "Partial Debit Card",
    "description": "Partially visible card element showing the top edge and part of the card number, suggesting the presence of a second linked payment method."
    }
    ]
    </sample_output>
    `;



export const EXTRACT_ELEMENTS_PROMPT_v0 = `
    <instructions>
    Analyze the provided UI screenshot in combination with the given component list.
    Your task is to detect and describe all key UI elements in the image, using the component list to guide semantic grouping, naming, and nesting.

    📥 Inputs:
    A UI screenshot

    A list of component categories, such as:
    Header Section 
    Cart Item 1  
    Quick Access Panel
    

    🧠 What to Do:
    Map every visual component to its corresponding category or subcategory, based on content and context.

    Create subcategories if they add clarity (e.g., "Cart Item 1 > Product Image").

    Include all visible elements, especially text labels, icons, buttons, and values — no matter how small.

    Describe each component with these details:

    Appearance: Shape, icon, color, text, visual style

    Function: Purpose or expected user interaction

    Positioning: Use screen regions (e.g., "centered near top", "bottom-right quadrant")

    State: Selected, default, disabled, etc.

    Interaction Type: Tappable, static, scrollable, etc.

    Avoid redundancy — include no more than 1 to 2 visual anchors if necessary for clarity (e.g., "below '$132.00'").

    🧾 Output Format:
    Return a valid JSON object

    Keys should represent the hierarchical path using > as a delimiter

    Example: "Top Up Destination Card > Card Label"

    Values should be rich descriptions of the visual component

    Use a flat structure — no nested objects

    No trailing commas

    📌 Output Requirements:
    Include all meaningful elements — especially text, values, and labels

    Group logically using the provided categories

    Add subcategories when appropriate

    Keep descriptions precise and visual-model friendly

    Use flat JSON (hierarchy via keys only)
    </instructions>

    <sample_output>
    {
      "Header Section > Title": "Bold white text reading 'Top Up Receipt', centered at the top of the screen with a colorful confetti background",
      "Success Badge > Icon": "Hexagon-shaped container with a white checkmark icon inside, green background, centered below the title",
      "Top Up Confirmation Section > Main Message": "Bold text 'Top Up Success', centered below the success badge",
      "Top Up Confirmation Section > Subtext": "Gray text confirming transaction, reading 'Your money has been added to your card'",
      "Total Top Up Amount > Value": "Large bold text '$132.00', centered and prominent near the middle of the screen",
      "Top Up Destination Card > Card Label": "White text 'Wally Virtual Card' at the top of the destination card",
      "Top Up Destination Card > Masked Card Number": "Text showing masked card '•••• 4568' below the card label",
      "Top Up Destination Card > Timestamp": "Small gray text 'Today, 12:45 PM' at the bottom-right of the card",
      "Primary Action Button > Label": "Full-width green button with white text 'Done' at the bottom of the screen, tappable",
      "Secondary Action Link > Label": "Text link 'Top up more money' below the Done button, teal-colored and tappable"
    }
    </sample_output>
    `;

export const EXTRACT_ELEMENTS_PROMPT_v1 = `
<instructions>
You are a meticulous UI/UX expert contributing to a design library. Identify and describe every visible UI element from a screenshot, organizing them under a provided list of component categories. The output helps build a consistent, searchable UI/UX reference library.

📥 Input: A UI screenshot, A list of component categories (e.g., Header, Cart Item, Quick Access Panel)

🧠 Your Task:

- For each component category, identify all visible UI elements, including small details like labels, icons, values, and buttons.
- Use consistent naming with a hierarchical key structure, using > to show nesting (e.g., Cart Item > Product Name).
- If helpful, create subcategories under the provided components for clarity.

- For each UI element, provide a clear and concise description including:
-- Appearance: Color, shape, text, icon, style
-- Function: Purpose or interaction
-- Position: Relative location (e.g., “top-left corner”, “below price”)
-- State: Active, default, disabled, etc.
-- Interaction Type: Static, tappable, scrollable, etc.

📌 Output Rules:
- Output a flat JSON STRING — use key paths (> delimited) for hierarchy
- Describe all relevant UI elements (don’t skip small details)
- Be precise, visual, and consistent in naming
- No nested JSON, no trailing commas

</instructions>


<sample_output>
{
  "Header > Title": "Centered bold text 'Top Up Receipt' with colorful confetti background",
  "Success Badge > Icon": "Green hexagon with white checkmark, centered below header",
  "Top Up Confirmation > Main Message": "Large bold text 'Top Up Success' below success badge",
  "Top Up Confirmation > Subtext": "Gray helper text 'Your money has been added to your card'",
  "Total Amount > Value": "Large bold '$132.00', centered on screen",
  "Destination Card > Label": "White text 'Wally Virtual Card' at top of card section",
  "Destination Card > Masked Number": "Text '•••• 4568' below the card label",
  "Destination Card > Timestamp": "Small gray text 'Today, 12:45 PM' at bottom-right of card",
  "Primary Button > Label": "Green full-width button with white text 'Done', tappable",
  "Secondary Link > Label": "Teal link 'Top up more money' below the primary button"
}
</sample_output>
`;


export const EXTRACT_ELEMENTS_PROMPT_v2 = `
<identity>  
You are a highly capable autonomous AI UIUX ANNOTATOR.  
You exist to assist a human USER in parsing UI screenshots and generating structured annotations for a design reference library.  
You are optimized for accuracy, consistency in naming conventions, and exhaustive visual parsing.  
You do not omit visible data. You do not ask questions. You do not speculate.  
</identity>  

<input>  
Required input includes:  
- A UI screenshot  
- A list of component categories (e.g., “Header”, “Product Card”, “Bottom Bar”)  
Each component in the list is treated as a logical container. Elements must be grouped accordingly.  
</input>  

<task_execution>  
Upon receiving inputs, perform the following steps without deviation:  

1. **Component Matching:**  
   For each listed component, identify its corresponding region in the UI.  

2. **Element Extraction:**  
   Within each component, extract and describe ALL visual elements as INDIVIDUAL elements.  
   - Include: icons, buttons, labels, values, helper text, visual states, overlays, spacers, input fields, scroll zones  
   - DO NOT exclude small elements or secondary labels  

3. **Naming Convention Enforcement:**  
   - Output uses strict hierarchical keys  
   - Format: [Parent Component] > [Subcomponent] > [Element Label]
   - Separator: >  
   - No nesting; use flat JSON with delimited keys  

4. **Description Requirements:**  
   Each key’s value must include:  
   - Appearance: Shape, color, text, visual style  
   - Function: Purpose or intended interaction  
   - Position: Spatial reference (e.g., “top-right corner”, “below cart total”)  
   - State: Active, disabled, selected, etc.  
   - Interaction Type: Static, tappable, swipeable, etc.  

5. **Output Constraints:**  
   - JSON object in string format only  
   - Flat structure (no nested objects)  
   - No nulls, placeholders, or empty fields  
   - No trailing commas  
</task_execution>  

<output_format>  
Return string formatted JSON.  
DO NOT include code guards \` in the output. 
Each key represents an element using the format:  
[Component] > [Subcomponent] > [Element Label]

Each value is a detailed string description, compliant with the annotation rules.  

Example:
"{
  "Header > Title": "Centered bold text 'Top Up Receipt' with a confetti background",
  "Success Badge > Icon": "Green hexagon with a white checkmark, placed below the title",
  "Delivery Options > Express Option > Label": "Text reading 'Express, 15-25 minutes' on the left side of the express delivery option row, positioned below the standard delivery option.",
  "Delivery Options > Express Option > Icon": "Small lightning bolt icon next to the express delivery label, indicating speed.",
  "Delivery Options > Express Option > Price": "Text reading '$2.00' on the right side of the express delivery option row.",
  "Delivery Options > Express Option > Selection Indicator": "Empty circular radio button on the far right, next to the $2.00 price.",
  "Confirmation > Message": "Large bold text 'Top Up Success', centered in the screen",
  "Cart Item 1 > Image": "Square image of a bowl containing gnocchi dish positioned in the left portion of the upper-middle section of the screen.",
  "Cart Item 1 > Title": "Text label 'Gnocchi with mushroom gravy' displayed to the right of the corresponding image.",
  "Cart Item 1 > Weight": "Gray text '230g' displayed next to the title of the first item.",
  "Cart Item 1 > Price": "Orange/amber colored price tag '$5,60' positioned below the item title.",
  "Cart Item 1 > Quantity Controls > Decrease Button": "Minus button on the left side of the quantity control, under the item title 'Gnocchi'",
  "Cart Item 1 > Quantity Controls > Count Display": "Text showing '1' between the minus and plus buttons, under the item title 'Gnocchi'",
  "Cart Item 1 > Quantity Controls > Increase Button": "Plus button on the right side of the quantity control, under the item title 'Gnocchi'",
  "Confirmation > Subtext": "Gray helper text 'Your money has been added to your card'",
  "Amount Summary > Value": "Prominent text '$132.00' centered near the middle",
  "Card Section > Card Label": "White label text 'Wally Virtual Card' at top of card",
  "Card Section > Masked Number": "Text '•••• 4568' directly below card label",
  "Card Section > Timestamp": "Small gray text 'Today, 12:45 PM' at bottom-right of card",
  "Primary CTA > Label": "Green full-width button 'Done' with white text, tappable",
  "Secondary CTA > Label": "Teal hyperlink text 'Top up more money' below primary button"
}"
</output_format>
`;

export const EXTRACT_ELEMENTS_PROMPT_v3 = `
<identity>  
You are a highly capable autonomous AI UIUX ANNOTATOR.  
You exist to assist a human USER in parsing UI screenshots and generating structured annotations for a design reference library.  
You are optimized for accuracy, consistency in naming conventions, and exhaustive visual parsing.  
You do not omit visible data. You do not ask questions. You do not speculate.  
</identity>  

<input>  
Required input includes:  
- A UI screenshot  
- A list of component categories (e.g., “Header”, “Product Card”, “Bottom Bar”)  
Each component in the list is treated as a logical container. Elements must be grouped accordingly.  
</input>  

<task_execution>  
Upon receiving inputs, perform the following steps without deviation:  

1. **Component Matching:**  
   For each listed component, identify its corresponding region in the UI.  

2. **Element Extraction:**  
   Within each component, extract and describe ALL visual elements as INDIVIDUAL elements.  
   - Include: icons, buttons, labels, values, helper text, visual states, overlays, spacers, input fields, scroll zones  
   - DO NOT exclude small elements or secondary labels  

3. **Naming Convention Enforcement:**  
   - Output uses strict hierarchical keys  
   - Format: [Parent Component] > [Subcomponent] > [Element Label]
   - Separator: >  
   - No nesting; use flat JSON with delimited keys  

4. **Description Requirements:**  
   Each key’s value must include:
   - Appearance: shape, color, text, icon, visual style
   - Anchor Reference: use nearby visible text or icons only when needed to disambiguate
   - Position: relative to visible neighbors (e.g., “to the right of text 'Gnocchi'”)
   - State: if visually indicated (e.g., filled, selected, empty)
   - Interaction Type: only if visually inferable (e.g., button, static label, input field)
   - DO NOT include inferred behavior, user intent, or experience-oriented descriptions
   - DO NOT refer to row order (e.g., “first item”, “bottom-most”) or sections not visually labeled  

5. **Output Constraints:**  
   - JSON object in string format only  
   - Flat structure (no nested objects)  
   - No nulls, placeholders, or empty fields  
   - No trailing commas  
</task_execution>  

<output_format>  
"{
  "Header > Title": "Centered bold text 'Top Up Receipt' with a confetti background",
  "Success Badge > Icon": "Green hexagon with a white checkmark, placed below the title",
  "Delivery Options > Express Option > Label": "Text reading 'Express, 15-25 minutes' on the left side of the express delivery option row, positioned below the standard delivery option.",
  "Delivery Options > Express Option > Icon": "Small lightning bolt icon next to the express delivery label, indicating speed.",
  "Delivery Options > Express Option > Price": "Text reading '$2.00' on the right side of the express delivery option row.",
  "Delivery Options > Express Option > Selection Indicator": "Empty circular radio button on the far right, next to the $2.00 price.",
  "Confirmation > Message": "Large bold text 'Top Up Success', centered in the screen",
  "Cart Item 1 > Image": "Square image of a bowl containing gnocchi dish positioned in the left portion of the upper-middle section of the screen.",
  "Cart Item 1 > Title": "Text label 'Gnocchi with mushroom gravy' displayed to the right of the corresponding image.",
  "Cart Item 1 > Weight": "Gray text '230g' displayed next to the title of the first item.",
  "Cart Item 1 > Price": "Orange/amber colored price tag '$5,60' positioned below the item title.",
  "Cart Item 1 > Quantity Controls > Decrease Button": "Minus button on the left side of the quantity control, under the item title 'Gnocchi'",
  "Cart Item 1 > Quantity Controls > Count Display": "Text showing '1' between the minus and plus buttons, under the item title 'Gnocchi'",
  "Cart Item 1 > Quantity Controls > Increase Button": "Plus button on the right side of the quantity control, under the item title 'Gnocchi'",
  "Confirmation > Subtext": "Gray helper text 'Your money has been added to your card'",
  "Amount Summary > Value": "Prominent text '$132.00' centered near the middle",
  "Card Section > Card Label": "White label text 'Wally Virtual Card' at top of card",
  "Card Section > Masked Number": "Text '•••• 4568' directly below card label",
  "Card Section > Timestamp": "Small gray text 'Today, 12:45 PM' at bottom-right of card",
  "Primary CTA > Label": "Green full-width button 'Done' with white text, tappable",
  "Secondary CTA > Label": "Teal hyperlink text 'Top up more money' below primary button"
}"
</output_format>

  Output Requirements (IMPORTANT):  
  - Return string formatted JSON.  
  - DO NOT include code guards \` in the output. 

`;

export const ANCHOR_ELEMENTS_PROMPT_v0 = `
You are responsible for rewriting visual component descriptions to optimize spatial and semantic clarity for downstream vision-language model performance.
Your task is to produce a flat JSON list of UI components and their descriptions with subtle visual anchors.
DO NOT include any other text or explanation in the output.

Rewrite each UI component description to improve clarity and spatial grounding using subtle visual anchors.
    Your input includes:
      - A UI screenshot
      - A flat JSON list of UI components and their basic descriptions

    Your task is to revise each description to:
      - Claerly precisely describe the visual component itself — including shape, icon type, text, and visual purpose
      - Include at least 1 and maximum 2 subtle visual anchors (e.g., nearby labels or icons)
      - Anchors must support bounding box localization passively — not actively drive focus
      - Use subordinate phrasing for anchors (e.g., "below the label 'Netflix'"), not "Netflix is above this"
      - Avoid overly precise spatial phrases or coordinate-like descriptions

    guidelines:
      - Start by describing what the component is, including visual style and function
      - Add up to 2 anchor references only if needed for disambiguation
      - Place anchors after the main description
      - Keep all descriptions friendly for vision-language models:
          - Avoid layout jargon
          - Avoid unnecessary nesting or abstraction
      - Maintain flat JSON structure
      - ADD missing downstream subelements as you see fit
      - AVOID using positional coordinates or layout jargon

    examples:
      - bad: "Transaction Item 3 > Date Time": "Gray text 'Aug 12, 07:25 PM' under 'Netflix'"
        improved: "Transaction Item 3 > Date Time": "Gray timestamp reading 'Aug 12, 07:25 PM', displayed under the 'Netflix' merchant label"
      - bad: "Header > Notification Icon": "Bell icon with green dot, opposite profile picture"
        improved: "Header > Notification Icon": "Circular bell icon with a green dot inside, in the top-right corner, opposite the profile picture"
      - bad: "Transaction Item 2 > Merchant Logo": "DKNY logo on left side, positioned below the Starbucks transaction"
        improved: "Transaction Item 2 > Merchant Logo": "Circular icon displaying the DKNY logo on white background, beside the 'DKNY' merchant name"
      - bad: "Price Chart > Time Labels": "Gray time markers from '0am' to '7pm' running along the bottom edge of the chart, indicating the hourly breakdown",
      - improved: "Price Chart > Time Labels": "Gray time markers from '11am', '12pm', '1pm', '2pm', '3pm', '4pm', '5pm', '6pm', '7pm' running along the bottom edge of the chart, indicating the hourly breakdown",

  sample_output: "
    {
    "Delivery Options > Standard Delivery > Label": "Black text displaying 'Standard delivery, 40-60 minutes' in the delivery options section",
    "Delivery Options > Express Delivery > Icon": "Yellow lightning bolt icon, positioned to the left of the express delivery option",
    "Delivery Options > Express Delivery > Price": "Text showing '$2.00' aligned to the right of the express delivery option",
    "Cart Item 2 > Image": "Square photograph showing a pastry with red raspberries and dark currants, positioned next to product title 'Wenzel with raspberries and currants ",
    "Cart Item 2 > Weight": "Gray text showing '170g' next to the item name, 'Wenzel'",
    "Primary Action Button - Done": "Large mint green rectangular button with rounded corners with white text 'Done', positioned in the lower section of the screen",
   }"

   Output Requirements (IMPORTANT):  
    - Return string formatted JSON
    - DO NOT include any other text or explanation in the output.
    - DO NOT include code guards \` in the output. 
`

export const ANCHOR_ELEMENTS_PROMPT_v1 = `
Generate Bounding Box Descriptions with Strong Target Focus + Selective Anchors
You are given:
* A UI screenshot
* A flat JSON list of UI components, where each key represents a component (e.g., "Transaction Item 3 > Date Time"), and each value is a description.

🎯 Objective:
Improve each description so it is:
* ✅ Detailed enough for a visual model to confidently detect the correct element
* ✅ Clear in what the model should be drawing a bounding box around
* ✅ Includes minimum 1 and maximum 2 useful positional or visual anchors, but only when necessary
* ❌ Does not shift attention to the anchor element itself

📌 Key Principles:
1. Prioritize Clarity on the Target Element
Start by clearly describing what the element is:
* Shape (circular, rectangular)
* Color (e.g., gray text, orange icon)
* Content (e.g., text label, logo, icon type)
* Contextual function (e.g., amount, timestamp, merchant)

2. Add Anchors When Helpful — But Subtle
Add one or two soft anchors only if:
* The element is visually ambiguous (e.g., small icon or repeated style)
* The content could be confused with another similar item
🟡 When adding anchors:
* Make sure the target stays the focus
* Phrase anchors in a supporting way, e.g.,
   * "…displaying the DKNY logo, next to the 'DKNY' text"
   * "…showing '-$70.00', aligned to the right of the 'Netflix' row"
🧪 Before & After Examples
     - bad: "Transaction Item 3 > Date Time": "Gray text 'Aug 12, 07:25 PM' under 'Netflix'"
     - improved: "Transaction Item 3 > Date Time": "Gray timestamp reading 'Aug 12, 07:25 PM', displayed under the 'Netflix' merchant label"
     - bad: "Header > Notification Icon": "Bell icon with green dot, opposite profile picture"
     - improved: "Header > Notification Icon": "Circular bell icon with a green dot inside, in the top-right corner, opposite the profile picture"
     - bad: "Transaction Item 2 > Merchant Logo": "DKNY logo on left side, positioned below the Starbucks transaction"
     - improved: "Transaction Item 2 > Merchant Logo": "Circular icon displaying the DKNY logo on white background, beside the 'DKNY' merchant name"
     - bad: "Price Chart > Time Labels": "Gray time markers from '0am' to '7pm' running along the bottom edge of the chart, indicating the hourly breakdown",
    - improved: "Price Chart > Time Labels": "Gray time markers from '11am', '12pm', '1pm', '2pm', '3pm', '4pm', '5pm', '6pm', '7pm' running along the bottom edge of the chart, indicating the hourly breakdown",

  output_format:
    Return string formatted JSON and nothing else.
    DO NOT include any other text or explanation in the output.
    DO NOT include code guards \` in the output. 

  sample_output: "
    {
    "Delivery Options > Standard Delivery > Label": "Black text displaying 'Standard delivery, 40-60 minutes' in the delivery options section",
    "Delivery Options > Express Delivery > Icon": "Yellow lightning bolt icon, positioned to the left of the express delivery option",
    "Delivery Options > Express Delivery > Price": "Text showing '$2.00' aligned to the right of the express delivery option",
    "Cart Item 2 > Image": "Square photograph showing a pastry with red raspberries and dark currants, positioned next to product title 'Wenzel with raspberries and currants ",
    "Cart Item 2 > Weight": "Gray text showing '170g' next to the item name, 'Wenzel'",
    "Primary Action Button - Done": "Large mint green rectangular button with rounded corners with white text 'Done', positioned in the lower section of the screen",
   }"
`

export const ANCHOR_ELEMENTS_PROMPT_v2 = `
You are optimizing UI component descriptions for a Vision Language Model (VLM) tasked with drawing bounding boxes accurately.
Your task is to produce a flat JSON list of UI components and their descriptions with subtle visual anchors.
DO NOT include any other text or explanation in the output.

Input:  
- A UI screenshot  
- A flat JSON list of component IDs → short descriptions

Goal:  
Transform each description into a detailed, visually-anchored, unambiguous instruction that:
- Makes the target component visually distinct  
- Uses visual or textual anchors only when necessary  
- Preserves the model's focus on the target component  
- Resolves ambiguity between repeated elements  

Key Guidance:

1. Prioritize the Component Itself  
Clearly describe:  
- Shape and size (e.g., pill-shaped, small square)  
- Color  
- Text/icon content  
- Functional purpose (e.g., ‘decrease item quantity’)  

2. Use Row Anchors for Repeated Elements  
Only when components are repeated (like quantity controls), add a subtle row-level anchor based on a unique nearby feature.

Example:  
“Minus (-) button in a light orange pill-shaped control, in the row showing the item 'Gnocchi with mushroom gravy'”  

Avoid:  
“Minus button on the left of quantity control” (too generic)  

3. Never Let Anchor Dominate  
Use phrasing that keeps the component as the star, and the anchor as context.

Good:  
“...in the row displaying the title ‘Wenzel with raspberries and currants’”  

Bad:  
“...under the ‘Wenzel’ label” → implies Wenzel might be the bounding box  

Sample Output:  
{
  "Cart Items List > Item 2 > Quantity Controls > Increase Button": "Plus (+) button in a light orange pill-shaped control, on the right of the quantity selector in the row showing the item 'Wenzel with raspberries and currants'",
  "Cart Items List > Item 3 > Quantity Controls > Decrease Button": "Minus (-) button in a light orange pill-shaped control, on the left of the quantity selector in the row displaying the title 'Freshly squeezed orange juice'",
  "Order Summary & Confirmation Bar > Confirm Button": "White text 'Confirm order' aligned right in the orange confirmation bar at the bottom of the screen"
}

Output Requirements (IMPORTANT):  
- Return string formatted JSON
- DO NOT include any other text or explanation in the output.
- DO NOT include code guards \` in the output. 
- Each key maps to a component ID  
- Each value is a full, anchored description  
`

export const ANCHOR_ELEMENTS_PROMPT_v3 = `
You are optimizing UI component descriptions for a Vision Language Model (VLM) tasked with drawing bounding boxes accurately.
The expected output is a flat JSON string.
DO NOT include any other text or explanation in the output.

Your job is to convert a flat JSON list of UI component keys into detailed visual descriptions that:
- Make each component visually distinct and detectable
- Resolves ambiguity between repeated elements by including precise visual anchors 
- Avoid language that anthropomorphizes, speculates, or adds human-facing UX explanation
- Preserves the model's focus on the target component  
- Maintain a tight focus on structure, position, and appearance

Input:  
- A UI screenshot  
- A flat JSON list of component IDs → short descriptions

Key Guidance:

1. Prioritize the Component Itself  
Clearly describe:  
- Shape and size (e.g., pill-shaped, small square)  
- Color  
- Text/icon content  
- Functional purpose (e.g., ‘decrease item quantity’)  

2. Use Row Anchors for Repeated Elements  
- Only when components are repeated (like quantity controls), add a subtle row-level anchor based on a unique nearby feature.
- Anchors must be visually locatable, such as labels, icons, or nearby components

Example (Correct):
"Plus (+) icon in a light orange pill-shaped button, in the row showing the item titled 'Wenzel with raspberries and currants'"

Avoid (Incorrect):
"Plus button on the left of the first quantity control"
"Below the second product title"

3. Do Not Include Purpose or Human Interpretation
- NEVER explain intent (e.g., "used to add funds", "leads to new screen", "indicating xxxxx" )
- Only describe what is visually present and identifiable

4. Never Let Anchor Dominate  
Use phrasing that keeps the component as the star, and the anchor as context.

Good:  
“...in the row displaying the title ‘Wenzel with raspberries and currants’”  
"Gray text 'Aug 20, 2:14 PM' showing the date and time below the merchant name 'DKNY'


Bad:  
“...under the ‘Wenzel’ label” → implies Wenzel might be the bounding box  
“Gray text 'Aug 20, 2:14 PM' showing the date and time below the merchant name in the second row” → VLM has no way to know what the second row is

<sample_output>
"  
{
  "Cart Items List > Item 2 > Quantity Controls > Increase Button": "Plus (+) button in a light orange pill-shaped control, on the right of the quantity selector in the row showing the item 'Wenzel with raspberries and currants'",
  "Cart Items List > Item 3 > Quantity Controls > Decrease Button": "Minus (-) button in a light orange pill-shaped control, on the left of the quantity selector in the row displaying the title 'Freshly squeezed orange juice'",
  "Order Summary & Confirmation Bar > Confirm Button": "White text 'Confirm order' aligned right in the orange confirmation bar at the bottom of the screen"
}"
</sample_output>

Output Requirements (IMPORTANT):  
- Return string formatted JSON
- DO NOT include any other text or explanation in the output.
- DO NOT include code guards \` in the output. 
- Each key maps to a component ID  
- Each value is a full, anchored description  
`
export const ANCHOR_ELEMENTS_PROMPT_v4 = `
You are optimizing UI component descriptions for a Vision Language Model (VLM) tasked with drawing bounding boxes accurately.
The expected output is a flat JSON string.
DO NOT include any other text or explanation in the output.

Your job is to convert a flat JSON list of UI component keys into detailed visual descriptions that:
- Make each component visually distinct and detectable
- Resolves ambiguity between repeated elements by including precise visual anchors 
- Avoid language that anthropomorphizes, speculates, or adds human-facing UX explanation
- Preserves the model's focus on the target component  
- Maintain a tight focus on structure, position, and appearance

Input:  
- A UI screenshot  
- A flat JSON list of component IDs → short descriptions

Key Guidance:

1. Prioritize the Component Itself  
Clearly describe:  
- Shape and size (e.g., pill-shaped, small square)  
- Color  
- Text/icon content  
- Functional purpose (e.g., ‘decrease item quantity’)  

2. Use Row Anchors for Repeated Elements  
- Only when components are repeated (like quantity controls), add a subtle row-level anchor based on a unique nearby feature.
- Anchors must be visually locatable, such as labels, icons, or nearby components

Example (Correct):
"Plus (+) icon in a light orange pill-shaped button, in the row showing the item titled 'Wenzel with raspberries and currants'"

Avoid (Incorrect):
"Plus button on the left of the first quantity control"
"Below the second product title"

3. Do Not Include Purpose or Human Interpretation
- NEVER explain intent (e.g., "used to add funds", "leads to new screen", "indicating xxxxx" )
- Only describe what is visually present and identifiable

4. Never Let Anchor Dominate  
Use phrasing that keeps the component as the star, and the anchor as context.

Good:  
“...in the row displaying the title ‘Wenzel with raspberries and currants’”  
"Gray text 'Aug 20, 2:14 PM' showing the date and time below the merchant name 'DKNY'


Bad:  
“...under the ‘Wenzel’ label” → implies Wenzel might be the bounding box  
“Gray text 'Aug 20, 2:14 PM' showing the date and time below the merchant name in the second row” → VLM has no way to know what the second row is

5. Reinforce Priority of Text in Visually Dominant Contexts  
- When a text label appears inside or near a button, dropdown, or image tile, **explicitly describe it as text** and clarify its role with nearby visual cues.
- Always lead the description with the actual component (e.g., “black *LOCATION text*”, “bold *ITEM LABEL*”, etc.)
- Avoid language that makes nearby UI elements the focus (like an image or button) sound like the primary component.

**Good:**
"Black text 'Matcha latte' shown as a label directly beneath the image of a green matcha drink, inside a white product tile"  
"Black text 'Regent Street, 16' aligned left at the top of the screen, followed by a small gray dropdown arrow"

**Bad:**
"Black text 'Matcha latte' shown as a label directly beneath the image of a green matcha drink, inside a white product tile"  

"Text below the image"  
"Text at the top of the tile showing a pizza"  
"'$5.90' on an orange button" → this leads to bounding the button, not the text


<sample_output>
"  
{
  "Cart Items List > Item 2 > Quantity Controls > Increase Button": "Plus (+) button in a light orange pill-shaped control, on the right of the quantity selector in the row showing the item 'Wenzel with raspberries and currants'",
  "Cart Items List > Item 3 > Quantity Controls > Decrease Button": "Minus (-) button in a light orange pill-shaped control, on the left of the quantity selector in the row displaying the title 'Freshly squeezed orange juice'",
  "Order Summary & Confirmation Bar > Confirm Button": "White text 'Confirm order' aligned right in the orange confirmation bar at the bottom of the screen"
}"
</sample_output>

Output Requirements (IMPORTANT):  
- Return string formatted JSON
- DO NOT include any other text or explanation in the output.
- DO NOT include code guards \` in the output. 
- Each key maps to a component ID  
- Each value is a full, anchored description  
`


export const ACCURACY_VALIDATION_PROMPT_v0 = `
You are an expert UI bounding box verifier and corrector.
Your task is to evaluate and correct UI screenshot bounding box annotations.

You are given:

A UI image with pre-drawn bounding boxes.

A JSON object describing each bounding box, including id, label, description, coordinates, and current status.

Your job is to evaluate how accurately each bounding box matches the described UI element in the image and return an updated JSON object with these new fields added to each item:

“accuracy”: A number from 0 to 100 estimating the visual and positional accuracy of the box.

“hidden”:

false if the box is accurate or a corrected version can be suggested

true if the box is inaccurate and no reasonable correction can be made

“suggested_coordinates”: Include only when accuracy is below 50% and correction is feasible. Format must match the original coordinates schema (x_min, y_min, x_max, y_max).

“status”:

Set to “Overwrite” if suggested_coordinates are provided

Otherwise keep the original status value

“explanation”: A concise reason explaining the score and if/how the box was corrected.

Return only the updated JSON array, preserving the original structure and adding these fields to each item.

Example Output:
"
{
  "id": "transaction_item_1_gt_merchant_logo",
  "label": "Transaction Item 1 > Merchant Logo",
  "description": "Circular logo showing the green and white Starbucks emblem...",
  "coordinates": {
    "x_min": 6.18,
    "y_min": 795.20,
    "x_max": 83.67,
    "y_max": 870.49
  },
  "status": "Overwrite",
  "accuracy": 46,
  "hidden": false,
  "suggested_coordinates": {
    "x_min": 12.0,
    "y_min": 800.0,
    "x_max": 76.0,
    "y_max": 860.0
  },
  "explanation": "Box had 19% extra padding and was misaligned; resized to tightly fit the logo."
}"

  Output Requirements (IMPORTANT):  
- Return string formatted JSON
- DO NOT include any other text or explanation in the output.
- DO NOT include code guards \` or \`\`\`json in the output. 
- Each key maps to a component ID  
- Each value is a full, anchored description  
`

export const METADATA_EXTRACTION_PROMPT_v0 = `
<prompt>
You are a world-class UX system documenter tasked with annotating components for a highly structured, discoverable design-reference library.

Your inputs will be:

An image (only to help you better enrich the provided fields — not to add new components).

A JSON object containing a component_name and a list of elements with basic label and description.

🧠 Your strict mission:
ONLY enrich and annotate the component and elements listed in the JSON.
⚡ Ignore everything else visible in the image.
⚡ Do NOT invent or add any other UI elements not explicitly listed.

You must fully and excellently annotate each component and its elements, strictly following the <good ux annotation guidelines>:

📋 Steps to Follow:
Component Enrichment (Top-Level)
For the given component_name, create:

patternName: Pick exactly one canonical type (e.g., Modal Dialog, Radio Card List).

facetTags: Assign 5–10 keywords capturing function, context, and role.

label: Choose the primary user-facing text or summary label.

description: Write a clear, contextual description of the component's role and position.

states: List all supported states (default, disabled, hover, etc).

interaction: Document supported interaction events (e.g., on_tap_ALLOW, on_swipe_LEFT).

userFlowImpact: Write one concise, impactful, succint sentence explaining how this component advances the user journey.

Element Enrichment (Inside elements array)
For each listed element:

Use the given label and description as your base.

Assign a patternName (eg: Text Header, Illustration, Tooltip, etc.).

Create 5–8 facetTags CLEARLY describing function, context, and role.

List supported states.

Define interaction (if no interaction, set "none": "Static element—no interaction").

Write a userFlowImpact stating how the element influences the user journey.

Format the output as strict, ordered JSON. use component names and element labels DIRECTLY as keys

<output>
{
  "<component_name>": {
    "patternName": "",
    "facetTags": [],
    "states": [],
    "interaction": {},
    "userFlowImpact": "",
    "<element_label_1>": {
      "patternName": "",
      "facetTags": [],
      "states": [],
      "interaction": {},
      "userFlowImpact": ""
    },
    "<element_label_2>": {
      "patternName": "",
      "facetTags": [],
      "states": [],
      "interaction": {},
      "userFlowImpact": ""
    }
    // repeat for each element
  }
}
Output only one clean JSON block, no commentary or preambles.

⚡ Critical Reminders:
Only annotate the component_name and its listed elements.

Do not add new UI parts even if visible in the image.

Think carefully and persistently validate that:

All pattern names are correctly picked.

All tags are precise, useful for filtering.

Label and description are complete and consistent.

States and interactions are appropriate and exhaustive.

User flow impact is clearly action-driven.

Reflect before you output: 
✅ Do facetTags include diverse terms across function, context, and role?
✅ Are all interaction events clear, user-centered, and labeled with cause-effect?
✅ Does the userFlowImpact tie into a journey or behavior outcome?
✅ Is the final output structured in VALID REQUIRED FORMAT, with no explanations?

---BEGIN NOW--- `

export const METADATA_EXTRACTION_PROMPT_v1 = `
You are a world-class UX system documenter tasked with annotating components for a highly structured, discoverable design-reference library.

Your inputs will be:

An image (only to help you better enrich the provided fields — not to add new components).

A JSON object containing a component_name and a list of elements with basic label and description.

🧠 Your strict mission:
ONLY enrich and annotate the component and elements listed in the JSON.
⚡ Ignore everything else visible in the image.
⚡ Do NOT invent or add any other UI elements not explicitly listed.

You must fully and excellently annotate each component and its elements, strictly following the <good ux annotation guidelines>:

📋 Steps to Follow:

1. Component Role Recognition  
   • Determine the component’s overall purpose and interaction model (e.g., “modal dialog for onboarding reminders,” “selection list for user choices”).  
   • Use that to inform your patternName and description.

2. Component Enrichment (Top-Level)  
   • patternName: Pick exactly one canonical type (e.g., Modal Dialog, Radio Card List).  
   • facetTags (5–10):  
     – Function: e.g., “onboarding”, “reminder”  
     – Context: e.g., “mobile”, “permissions”  
     – Role: e.g., “cta”, “informative”, “illustration”, “primary-action”  
   • description: Clear, contextual description of component’s role and placement.  
   • states: List valid UI states (default, hover, selected, disabled).  
   • interaction: Document events (e.g., on_tap_ALLOW, on_swipe_LEFT).  
   • userFlowImpact: One sentence on how this component nudges or guides the user (e.g., “Prompts users to enable notifications to support habit formation”).

3. Element Role Recognition  
   • For each element, choose one best-fit patternName (Text Header, Illustration, Tooltip, etc.), matching form and function—do not invent new names.

4. Element Enrichment (Inside elements array)  
   • Start from the provided label & description.  
   • patternName: one canonical type.  
   • facetTags (5–8):  
     – Function tag(s)  
     – Context tag(s)  
     – Role tag(s)  
   • states: valid states (default if static).  
   • interaction: list supported events or "none": "Static element—no interaction".  
   • userFlowImpact: one sentence on how this element influences the user journey (e.g., “Encourages permission grant by reinforcing emotional appeal”).

   format the output as strict, ordered JSON. use component names and element labels DIRECTLY as keys

<output>
{
  "<component_name>": {
    "patternName": "",
    "facetTags": [],
    "states": [],
    "interaction": {},
    "userFlowImpact": "",
    "<element_label_1>": {
      "patternName": "",
      "facetTags": [],
      "states": [],
      "interaction": {},
      "userFlowImpact": ""
    },
    "<element_label_2>": {
      "patternName": "",
      "facetTags": [],
      "states": [],
      "interaction": {},
      "userFlowImpact": ""
    }
    // repeat for each element
  }
}
Output only one clean JSON block, no commentary or preambles.

⚡ Critical Reminders:
Only annotate the component_name and its listed elements.

Do not add new UI parts even if visible in the image.

Think carefully and persistently validate that:

All pattern names are correctly picked.

All tags are precise, useful for filtering.

Label and description are complete and consistent.

States and interactions are appropriate and exhaustive.

User flow impact is clearly action-driven.

Reflect before you output: 
✅ Do facetTags include diverse terms across function, context, and role?
✅ Are all interaction events clear, user-centered, and labeled with cause-effect?
✅ Does the userFlowImpact tie into a journey or behavior outcome?
✅ Is the final output structured in VALID REQUIRED FORMAT, with no explanations?

---BEGIN NOW---`


export const METADATA_EXTRACTION_PROMPT_v2 = `
You are a world-class UX system documenter tasked with annotating components for a highly structured, discoverable design-reference library.

Your inputs will be:

An image (for context enrichment only — do not add new components).

A JSON object containing component_name and a list of elements with basic label and description.

🧠 Mission:

Annotate and enrich only the listed component_name and elements.

Do not invent, add, or reference any UI parts not explicitly in the JSON.

Follow *good ux annotation guidelines* precisely:

📋 Steps to Follow:
1. Component Role Recognition
• Determine the overall purpose and interaction model (e.g., “modal dialog for onboarding reminders”).
• Use this to complete patternName and description.

2. Component Enrichment (Top-Level)
• patternName: Exactly one canonical type (e.g., Primary Button, Modal Dialog, Radio Card List,Form Input with Label	).
• facetTags (5–10): Diverse terms across Function, Context, and Role (e.g., onboarding, mobile, CTA).
• description: Clear and contextual.
• states: All valid states (e.g., default, hover, selected, disabled, checked).
• interaction: List of supported events as key-value pairs, using clear, user-centered action-effect language.). ie: {"interaction": {
  "on_tap": "triggers primary action",
  "on_swipe": "reveals dismiss option on swipe left"
}}
• userFlowImpact: How this component guides the user journey (one sentence).

3. Element Role Recognition
• Assign exactly one patternName to each element (e.g., Text Header, Illustration).
• Base enrichment on the provided label and description.

4. Element Enrichment (Inside elements array)
• patternName: One canonical type.
• facetTags (5–8): Diverse across Function, Context, Role.
• states: Valid states (default if static).
• interaction: list of Supported events ie,   "on_swipe": "reveal delete action when swiped left" , "on_drag": "reorder list item" "none": "Static element—no interaction".
• userFlowImpact: How the element nudges user behavior (one sentence).

format the output as strict, ordered JSON. use component names and element labels DIRECTLY as keys

<output>
{
  "<component_name>": {
    "patternName": "",
    "facetTags": [],
    "states": [],
    "interaction": {},
    "userFlowImpact": "",
    "<element_label_1>": {
      "patternName": "",
      "facetTags": [],
      "states": [],
      "interaction": {},
      "userFlowImpact": ""
    },
    "<element_label_2>": {
      "patternName": "",
      "facetTags": [],
      "states": [],
      "interaction": {},
      "userFlowImpact": ""
    }
    // repeat for each element
  }
}
Output only one clean JSON block, no commentary or preambles.

⚡ Critical Reminders:
Only annotate the component_name and its listed elements.

Do not add new UI parts even if visible in the image.

Think carefully and persistently validate that:

All pattern names are correctly picked.

All tags are precise, useful for filtering.

Label and description are complete and consistent.

States and interactions are appropriate and exhaustive.

User flow impact is clearly action-driven.

Reflect before you output: 
✅ Do facetTags include diverse terms across function, context, and role?
✅ Are all interaction events clear, user-centered, and labeled with cause-effect?
✅ Does the userFlowImpact tie into a journey or behavior outcome?
✅ Is the final output structured in VALID REQUIRED FORMAT, with no explanations?

---BEGIN NOW---`


export const METADATA_EXTRACTION_PROMPT_FINAL = `
You are a world-class UX system documenter tasked with annotating components for a highly structured, discoverable design-reference library.

Your inputs will be:

An image (for context enrichment only — do not add new components).

A JSON object containing component_name and a list of elements with basic label and description.

🧠 Mission:

Annotate and enrich only the listed component_name and elements.

Do not invent, add, or reference any UI parts not explicitly in the JSON.

Follow *good ux annotation guidelines* precisely:

📋 Steps to Follow:
1. Component Role Recognition
• Determine the overall purpose and interaction model (e.g., “modal dialog for onboarding reminders”).
• Use this to complete patternName and componentDescription.

2. Component Enrichment (Top-Level)
• patternName: Exactly one canonical type (e.g., Primary Button, Modal Dialog, Radio Card List,Form Input with Label	).
• facetTags (5–10): Diverse terms across Function, Context, and Role (e.g., onboarding, mobile, CTA).
• componentDescription: Clear and contextual.
• states: All valid states (e.g., default, hover, selected, disabled, checked).
• interaction: List of supported events as key-value pairs, using clear, user-centered action-effect language.). ie: {"interaction": {
  "on_tap": "triggers primary action",
  "on_swipe": "reveals dismiss option on swipe left"
}}
• userFlowImpact: How this component guides the user journey (one sentence).

3. Element Role Recognition
• Assign exactly one patternName to each element (e.g., Text Header, Illustration).
• Base enrichment on the provided label and description.

4. Element Enrichment (Inside elements array)
• patternName: One canonical type.
• facetTags (5–8): Diverse across Function, Context, Role.
• states: Valid states (default if static).
• interaction: list of Supported events ie,   "on_swipe": "reveal delete action when swiped left" , "on_drag": "reorder list item" "none": "Static element—no interaction".
• userFlowImpact: How the element nudges user behavior (one sentence).

format the output as strict, ordered JSON. use component names and element labels DIRECTLY as keys

<output>
{
  "<component_name>": {
    "componentDescription": "",
    "patternName": "",
    "facetTags": [],
    "states": [],
    "interaction": {},
    "userFlowImpact": "",
    "<element_label_1>": {
      "patternName": "",
      "facetTags": [],
      "states": [],
      "interaction": {},
      "userFlowImpact": ""
    },
    "<element_label_2>": {
      "patternName": "",
      "facetTags": [],
      "states": [],
      "interaction": {},
      "userFlowImpact": ""
    }
    // repeat for each element
  }
}
Output only one clean JSON block, no commentary or preambles.

⚡ Critical Reminders:
Only annotate the component_name and its listed elements.

Do not add new UI parts even if visible in the image.

Think carefully and persistently validate that:

All pattern names are correctly picked.

All tags are precise, useful for filtering.

Label and description are complete and consistent.

States and interactions are appropriate and exhaustive.

User flow impact is clearly action-driven.

Reflect before you output: 
✅ Do facetTags include diverse terms across function, context, and role?
✅ Are all interaction events clear, user-centered, and labeled with cause-effect?
✅ Does the userFlowImpact tie into a journey or behavior outcome?
✅ Is the final output structured in VALID REQUIRED FORMAT, with no explanations?

---BEGIN NOW---
`

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/ai/ClaudeAIService.ts
================
import Anthropic from '@anthropic-ai/sdk';
// import { PromptResult } from '../../../types/PromptRunner'; // adjust path as needed
import { EXTRACT_ELEMENTS_PROMPT_v2, ANCHOR_ELEMENTS_PROMPT_v0, ANCHOR_ELEMENTS_PROMPT_v1, ANCHOR_ELEMENTS_PROMPT_v2, EXTRACT_ELEMENTS_PROMPT_v3, ANCHOR_ELEMENTS_PROMPT_v3 } from '@/lib/prompt/prompts';
import { PromptTrackingContext } from '@/lib/logger';
import { PromptLogType } from '@/lib/constants';
import { cleanText } from '@/lib/file-utils'; 
// Ensure your Claude API key is set in ENV
const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

// Specify the Claude vision model version
const VISION_MODEL_CLAUDE = 'claude-3-7-sonnet-20250219';
const VISION_MODEL_HAIKU = 'claude-3-5-haiku-20241022';

// Constants for token cost calculation (update with actual costs)
const CLAUDE_INPUT_TOKEN_COST = 0.000015; // example cost per input token
const CLAUDE_OUTPUT_TOKEN_COST = 0.000060; // example cost per output token

/**
 * Calls the Claude vision-capable model with a text prompt and optional image URL.
 *
 * @param prompt   - The text prompt to send.
 * @param imageUrl - Optional URL of an image for the model to analyze.
 * @param context  - The tracking context containing batch, screenshot, and other IDs
 * @param promptType - The type of prompt being processed.
 * @returns        - A structured PromptResult containing the response, timing, and token usage.
 */
export async function callClaudeVisionModel(
  prompt: string,
  imageUrl: string | null,
  context: PromptTrackingContext,
  promptType: PromptLogType.ELEMENT_EXTRACTION | PromptLogType.ANCHORING
): Promise<any> {
  // Build the Anthropic messages payload
  const messages = [
    {
      role: 'user',
      content: [
        // include image if provided
        imageUrl && {
          type: 'image',
          source: { type: 'url', url: imageUrl }
        },
        { type: 'text', text: prompt }
      ].filter(Boolean),
    },
  ];

  try {
    // Start timing right before the API call
    const startTime = Date.now();
    
    const response = await anthropic.messages.create({
      // model: VISION_MODEL_HAIKU,
      model: VISION_MODEL_CLAUDE,
      max_tokens: 8192, // tweak as needed
      messages: messages as Anthropic.MessageParam[],
    });
    
    // End timing right after the API call
    const endTime = Date.now();
    const durationMs = endTime - startTime;

    // Extract usage data
    const inputTokens = response?.usage?.input_tokens || 0;
    const outputTokens = response?.usage?.output_tokens || 0;
    
    // Log the interaction using the context with the measured duration
    await context.logPromptInteraction(
      `Claude-${VISION_MODEL_CLAUDE}`,
      promptType,
      prompt,
      JSON.stringify(response),
      durationMs,
      {
        input: inputTokens,
        output: outputTokens,
        total: inputTokens + outputTokens
      },
      CLAUDE_INPUT_TOKEN_COST,
      CLAUDE_OUTPUT_TOKEN_COST
    );

    return response;
  } catch (err) {
    console.error('Error calling Claude Vision Model:', err);
    throw new Error(
      `Failed to get response from Claude: ${
        err instanceof Error ? err.message : String(err)
      }`
    );
  }
}

/**
 * Extracts components from an image using the OpenAI vision model.
 *
 * @param imageUrl The URL of the image to analyze.
 * @param component_list The list of components to guide the extraction.
 * @param context The tracking context containing batch, screenshot, and other IDs
 * @returns A promise resolving to the structured PromptResult.
 * @throws Throws an error if the API call fails.
 */
export async function extract_element_from_image(
  imageUrl: string, 
  component_list: string,
  context: PromptTrackingContext
) {
  // Define the prompt for extraction
  const prompt = EXTRACT_ELEMENTS_PROMPT_v2 + `\n\n<component_list>${component_list}</component_list>`;
  const response = await callClaudeVisionModel(
    prompt, 
    imageUrl, 
    context,
    PromptLogType.ELEMENT_EXTRACTION
  );

  const { parsedContent, rawText, usage } = extractClaudeResponseData(response);

  // Call the OpenAI vision model with the prompt and image URL
  return { parsedContent, rawText, usage };
}

/**
 * Extracts anchor elements from an image using the Claude vision model.
 *
 * @param imageUrl The URL of the image to analyze.
 * @param element_list The list of elements to guide the extraction.
 * @param context The tracking context containing batch, screenshot, and other IDs
 * @returns A promise resolving to the structured PromptResult.
 * @throws Throws an error if the API call fails.
 */
export async function anchor_elements_from_image(
  imageUrl: string, 
  element_list: string,
  context: PromptTrackingContext
) {
  // Define the prompt for anchor extraction
  const prompt = ANCHOR_ELEMENTS_PROMPT_v3 + `\n\n<element_list>${element_list}</element_list>`;
  const response = await callClaudeVisionModel(
    prompt, 
    imageUrl,
    context,
    PromptLogType.ANCHORING
  );

  const { parsedContent, rawText, usage } = extractClaudeResponseData(response);

  // Call the Claude vision model with the prompt and image URL
  return { parsedContent, rawText,usage };
}

/**
 * Cleans the raw text and returns a list of components.
 *
 * @param components - The array of components to filter and clean.
 * @returns Array of cleaned strings in the format "component_name: description".
 */
function cleanTextToList(components: any[]): string[] {
  return components
    .filter(component => typeof component?.component_name === 'string' && typeof component?.description === 'string')
    .map(component => `${component.component_name}: ${component.description}`);
}

/**
 * Helper: Parses Claude's text content into a JSON object safely.
 * Handles common formatting quirks like trailing commas or line breaks.
 * Searches for JSON content within the response text.
 * It attempts to clean the extracted text before parsing.
 *
 * @param rawText - Raw text string returned from Claude.
 * @returns Parsed JSON object, or an empty object if parsing fails.
 */
function parseClaudeTextToJson(rawText: string): Record<string, any> {
  console.log('Attempting to parse Claude response as JSON');
  let jsonContent = '';
  
  try {
    // Look for JSON pattern in the text - either within code blocks or standalone
    const jsonRegex = /```(?:json)?\s*({[\s\S]*?})\s*```|({[\s\S]*})/;
    const match = rawText.match(jsonRegex);

    if (match) {
      // Use the first matched group that contains content
      jsonContent = match[1] || match[2];
      console.log('Extracted potential JSON content:', jsonContent.slice(0, 100));
    } else {
      console.log('No JSON match found within ``` markers or as standalone object, trying entire text.');
      // Fall back to using the entire text if no specific JSON block is found
      jsonContent = rawText;
    }

    // --- Enhanced Cleaning ---
    // 1. Basic cleaning (from file-utils, potentially redundant but safe)
    let cleanedText = cleanText(jsonContent); 
    
    // 2. Remove leading/trailing whitespace
    cleanedText = cleanedText.trim();

    // 3. Attempt to fix common JSON issues (e.g., unescaped newlines within strings)
    // Note: This is a heuristic and might not cover all cases.
    // It replaces literal newlines only if they seem to be inside string values
    // (i.e., preceded by a non-backslash character and followed by a quote).
    // This is complex to get perfect with regex, a more robust solution might involve
    // a more sophisticated parser or sequential processing.
    // cleanedText = cleanedText.replace(/([^\\])\\n"/g, '$1\\\\n"'); // Example: try to fix unescaped newlines

    // More aggressive cleaning: remove control characters except for \t, \n, \r, \f within strings
    cleanedText = cleanedText.replace(/[\x00-\x08\x0B\x0C\x0E-\x1F]/g, '');


    // 4. Final attempt to remove trailing commas before closing brace/bracket
    cleanedText = cleanedText.replace(/,\s*([}\]])/g, '$1');

    console.log('Cleaned JSON content for parsing:', cleanedText.slice(0, 100));
    
    // --- Parsing ---
    return JSON.parse(cleanedText);

  } catch (error) {
    console.error('Failed to parse Claude response as JSON even after cleaning.', error);
    console.error('Original rawText:', rawText);
    console.error('Content attempted for parsing:', jsonContent); // Log the extracted part
    // Return empty object on failure to prevent downstream errors
    return {}; 
  }
}

/**
 * Extracts structured content text and usage metadata from Claude's response.
 *
 * @param response - Full Claude response object.
 * @returns Object containing parsed text content and usage details.
 */
export function extractClaudeResponseData(response: any): {
  parsedContent: Record<string, any>,
  rawText: string,
  usage: {
    input_tokens?: number,
    output_tokens?: number
  }
} {
  const rawText = response?.content?.find((item: any) => item.type === 'text')?.text ?? '';
  console.log('returned from claude rawText', rawText);

  const parsedContent = parseClaudeTextToJson(rawText);

  const usage = {
    input_tokens: response?.usage?.input_tokens,
    output_tokens: response?.usage?.output_tokens,
  };

  return {
    parsedContent,
    rawText,
    usage,
  };
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/ai/MoondreamAIService.ts
================
// THIS DOES NOT WORK. DO NOT USE OR MODIFY.

import { PromptTrackingContext } from '@/lib/logger';
import { MOON_DREAM_API_KEY } from '@/config';
import { PromptLogType } from '@/lib/constants';
/**
 * Interface for the detect API response
 */
interface DetectResponse {
  request_id: string;
  objects: Array<{
    x_min: number;
    y_min: number;
    x_max: number;
    y_max: number;
  }>;
}

/**
 * Converts a Blob to a base64 string with data URI prefix
 * 
 * @param blob - The Blob to convert
 * @param mimeType - The MIME type of the image (defaults to 'image/jpeg')
 * @returns A promise resolving to a base64 string with data URI prefix
 */
export async function blobToBase64(blob: Blob, mimeType: string = 'image/jpeg'): Promise<string> {
  return new Promise((resolve, reject) => {
    const reader = new FileReader();
    reader.onloadend = () => {
      if (typeof reader.result === 'string') {
        resolve(reader.result);
      } else {
        reject(new Error('FileReader did not return a string'));
      }
    };
    reader.onerror = reject;
    reader.readAsDataURL(blob);
  });
}

/**
 * Fetches image data from a URL
 * 
 * @param imageUrl - URL of the image to fetch
 * @returns A promise resolving to the image blob
 */
export async function fetchImageFromUrl(imageUrl: string): Promise<Blob> {
  const imageResponse = await fetch(imageUrl);
  const imageBlob = await imageResponse.blob();
  return imageBlob;
}

/**
 * Calls the Moondream API to detect objects in images
 * 
 * @param imageUrl - URL of the image to analyze
 * @param object - The object type to detect (e.g., "person", "car", "face")
 * @param context - The tracking context containing batch, screenshot, and component IDs
 * @returns A promise resolving to the detection response
 */
export async function detectObjectsFromImage(
  imageUrl: string,
  object: string,
  context: PromptTrackingContext
): Promise<DetectResponse> {
  console.log(`CALLING MOONDREAM API: Object=${object}, ImageURL=${imageUrl}`);

  // Fetch the image data from URL
  const imageBlob = await fetchImageFromUrl(imageUrl);

  // Create form data with image and object parameters
  const formData = new FormData();
  formData.append('image', imageBlob);
  formData.append('object', object);

  try {
    // Start timing right before the API call
    const startTime = Date.now();
    
    const response = await fetch('https://api.moondream.ai/v1/detect', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${MOON_DREAM_API_KEY}`
      },
      body: formData
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`Moondream API error (${response.status}): ${errorText}`);
    }

    const result = await response.json() as DetectResponse;
    
    // End timing right after the API call finishes
    const endTime = Date.now();
    const durationMs = endTime - startTime;
    
    // Log the interaction using the context with the measured duration
    await context.logPromptInteraction(
      'Moondream-Detect',
      PromptLogType.VLM_LABELING,
      `Detect ${object} in image`,
      JSON.stringify(result),
      durationMs,
      // Moondream doesn't provide token usage, so we leave these undefined
      {
        input: undefined,
        output: undefined,
        total: undefined
      }
    );

    return result;
  } catch (err) {
    console.error('Error calling Moondream API:', err);
    throw new Error(
      `Failed to get response from Moondream: ${
        err instanceof Error ? err.message : String(err)
      }`
    );
  }
}

/**
 * Converts the normalized coordinates to pixel coordinates
 * 
 * @param coordinates - Object with normalized coordinates (0-1)
 * @param imageWidth - Width of the image in pixels
 * @param imageHeight - Height of the image in pixels
 * @returns Object with pixel coordinates
 */
export function normalizedToPixelCoordinates(
  coordinates: { x_min: number; y_min: number; x_max: number; y_max: number },
  imageWidth: number,
  imageHeight: number
) {
  return {
    x_min: Math.round(coordinates.x_min * imageWidth),
    y_min: Math.round(coordinates.y_min * imageHeight),
    x_max: Math.round(coordinates.x_max * imageWidth),
    y_max: Math.round(coordinates.y_max * imageHeight)
  };
} 

/**
 * Detect objects in an image blob via Moondream API.
 *
 * @param imageBlob  – Image data as a Blob
 * @param objectType – The object type to detect (e.g. "person", "car", "face")
 * @param context - The tracking context containing batch, screenshot, and component IDs
 */
export async function detectObjectsFromBlob(
  imageBlob: Blob,
  objectType: string,
  context: PromptTrackingContext
): Promise<DetectResponse> {
  console.log(`CALLING MOONDREAM API: Object=${objectType}, Blob input`);

  const formData = new FormData();
  formData.append('image', imageBlob);
  formData.append('object', objectType);

  // Start timing right before the API call
  const startTime = Date.now();
  
  const resp = await fetch('https://api.moondream.ai/v1/detect', {
    method: 'POST',
    headers: { 'Authorization': `Bearer ${MOON_DREAM_API_KEY}` },
    body: formData
  });

  if (!resp.ok) {
    const txt = await resp.text();
    throw new Error(`Moondream API error (${resp.status}): ${txt}`);
  }

  const result = (await resp.json()) as DetectResponse;
  
  // End timing right after the API call finishes
  const endTime = Date.now();
  const durationMs = endTime - startTime;

  // Log the interaction using the context with the measured duration
  await context.logPromptInteraction(
    'Moondream-Detect',
    PromptLogType.VLM_LABELING,
    `Detect ${objectType} in blob`,
    JSON.stringify(result),
    durationMs,
    { input: undefined, output: undefined, total: undefined }
  );

  return result;
}

/**
 * Detect objects in a base64 encoded image via Moondream API.
 *
 * @param imageBase64 – Image data as a base64 string with data URI prefix
 * @param objectType – The object type to detect (e.g. "person", "car", "face")
 * @param context - The tracking context containing batch, screenshot, and component IDs
 */
export async function detectObjectsFromBase64(
  imageBase64: string,
  objectType: string,
  context: PromptTrackingContext
): Promise<DetectResponse> {
  if (!MOON_DREAM_API_KEY) {
    console.error('[Moondream] MOON_DREAM_API_KEY is undefined – check your environment variables.');
    throw new Error(
      '[Moondream] MOON_DREAM_API_KEY is undefined – check your environment variables.'
    );
  }

  console.log(`CALLING MOONDREAM API: Object=${objectType}, Base64 input`);

  // Convert base64 to a blob for FormData compatibility
  const formData = new FormData();
  
  // Create a Blob from the base64 string by removing the data URI prefix if present
  let base64Data = imageBase64;
  if (base64Data.startsWith('data:')) {
    base64Data = base64Data.split(',')[1];
  }
  
  // Convert base64 to binary
  const binaryStr = atob(base64Data);
  const byteArray = new Uint8Array(binaryStr.length);
  for (let i = 0; i < binaryStr.length; i++) {
    byteArray[i] = binaryStr.charCodeAt(i);
  }
  
  // Create blob from binary data
  const blob = new Blob([byteArray], { type: 'image/jpeg' });
  
  // Append to form data
  formData.append('image_url', blob);
  formData.append('object', objectType);

  // Start timing right before the API call
  const startTime = Date.now();
  
  const resp = await fetch('https://api.moondream.ai/v1/detect', {
    method: 'POST',
    headers: { 
      'Authorization': `Bearer ${MOON_DREAM_API_KEY}`
    },
    body: formData
  });

  if (!resp.ok) {
    const txt = await resp.text();
    throw new Error(`Moondream API error (${resp.status}): ${txt}`);
  }

  const result = (await resp.json()) as DetectResponse;
  
  // End timing right after the API call finishes
  const endTime = Date.now();
  const durationMs = endTime - startTime;

  // Log the interaction using the context with the measured duration
  await context.logPromptInteraction(
    'Moondream-Detect',
    PromptLogType.VLM_LABELING,
    `Detect ${objectType} in base64 image`,
    JSON.stringify(result),
    durationMs,
    { input: undefined, output: undefined, total: undefined }
  );

  return result;
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/ai/MoondreamDetectionService.js
================
import { detectObjectsFromBuffer, normalizedToPixelCoordinates } from '@/lib/services/ai/MoondreamVLService';
import fs from 'fs';
import path from 'path';
import Jimp from 'jimp';
import { logPromptInteraction, logPromptToDatabase, PromptTrackingContext, createScreenshotTrackingContext } from '@/lib/logger';
import pLimit from 'p-limit';
import {
  BOX_COLOR,
  BOX_WIDTH,
  OVERLAY_COLOR,
  generateAnnotatedImageBuffer,
  saveAnnotatedImageDebug,
  normalizeLabel
} from '@/lib/services/imageServices/BoundingBoxService';

import {MOONDREAM_CONCURRENCY} from '@/lib/constants';

const VLM_MODEL_NAME = 'moondream'; // Define the model name being used

// --- Type Imports (using JSDoc for type hinting in JS) ---
/**
 * @typedef {import('../../types/DetectionResult').ElementDetectionItem} ElementDetectionItem
 * @typedef {import('../../types/DetectionResult').ComponentDetectionResult} ComponentDetectionResult
 */

/**
 * Extracts the first level of the label hierarchy
 * @param {string} label - The full hierarchical label
 * @returns {string} First level category
 */
function getFirstLevelCategory(label) {
  return label.split(' > ')[0];
}

/**
 * Determines the appropriate grouping category for hierarchical labels
 * Elements are grouped by mid-level categories if they have >2 children
 * 
 * This algorithm implements a dynamic grouping strategy for hierarchical labels:
 * 1. Creates a tree structure representing the label hierarchy
 * 2. Identifies nodes with >2 children or direct elements
 * 3. Promotes these nodes as standalone categories
 * 4. Assigns each element to its deepest eligible category
 * 
 * Example:
 * For "Operational Risk Overview > ICU Department > Occupancy", 
 * if "ICU Department" has >2 child elements, it becomes the category
 * instead of just "Operational Risk Overview".
 * 
 * @param {string[]} allLabels - Array of all hierarchical labels (e.g. "Parent > Child > Grandchild")
 * @returns {Object} Map of labels to their assigned category
 */
function determineHierarchicalGroups(allLabels) {
  // Step 1: Build a tree structure from all labels
  const hierarchy = {};
  
  // Count elements under each prefix
  allLabels.forEach(label => {
    const parts = label.split(' > ');
    
    // Initialize all paths in the hierarchy for this label
    for (let i = 0; i < parts.length; i++) {
      const currentPath = parts.slice(0, i + 1).join(' > ');
      
      if (!hierarchy[currentPath]) {
        hierarchy[currentPath] = {
          count: 0,          // Number of leaf nodes directly assigned to this path
          level: i + 1,      // Depth in the hierarchy (1 = top level)
          parent: i > 0 ? parts.slice(0, i).join(' > ') : null,
          children: new Set() // Set of immediate child paths
        };
      }
      
      // If this is a leaf node (full path), increment leaf count
      if (i === parts.length - 1) {
        hierarchy[currentPath].count++;
      }
      
      // Add as child to parent node
      if (i > 0) {
        const parentPath = parts.slice(0, i).join(' > ');
        if (hierarchy[parentPath]) {
          hierarchy[parentPath].children.add(currentPath);
        }
      }
    }
  });
  
  // Log the hierarchy structure (debug only)
  if (process.env.DEBUG_HIERARCHY === 'true') {
    console.log('=== Label Hierarchy ===');
    Object.entries(hierarchy).forEach(([path, node]) => {
      console.log(`${path} (Level ${node.level}): ${node.count} direct elements, ${node.children.size} children`);
    });
    console.log('======================');
  }
  
  // Step 2: Find the appropriate category for each label
  const labelToCategory = {};
  
  allLabels.forEach(label => {
    const parts = label.split(' > ');
    let bestCategory = parts[0]; // Default to top level
    let bestLevel = 1;
    
    // Find the deepest qualifying category
    for (let i = 0; i < parts.length; i++) { 
      const currentPath = parts.slice(0, i + 1).join(' > ');
      const node = hierarchy[currentPath];
      
      if (!node) continue;
      
      // Determine if this node qualifies as a category:
      // 1. Always include top level
      // 2. If node has >2 children or contains >2 elements directly
      const childCount = node.children.size;
      const hasEnoughChildren = childCount > 2 || node.count > 2;
      
      if (i === 0 || hasEnoughChildren) {
        // This is a better category than what we have
        if (node.level > bestLevel) {
          bestCategory = currentPath;
          bestLevel = node.level;
        }
      }
    }
    
    labelToCategory[label] = bestCategory;
  });
  
  // Log some stats about the grouping results
  const categoryCounts = {};
  Object.values(labelToCategory).forEach(category => {
    categoryCounts[category] = (categoryCounts[category] || 0) + 1;
  });
  
  const uniqueCategories = Object.keys(categoryCounts);
  console.log(`Created ${uniqueCategories.length} groupings from ${allLabels.length} elements:`);
  uniqueCategories.forEach(category => {
    console.log(`- ${category}: ${categoryCounts[category]} elements`);
  });
  
  return labelToCategory;
}

/**
 * Creates an output directory with timestamp
 * @returns {Promise<string>} Path to the created directory
 */
async function createOutputDirectory() {
  const timestamp = new Date().toISOString().replace(/[:.-]/g, '').replace('T', '_').slice(0, 15);
  const outputDir = `mobbin_attempt_folder/detection_output_${timestamp}`;
  
  try {
    await fs.promises.mkdir(outputDir, { recursive: true });
    console.log(`Output directory created: ${outputDir}`);
    return outputDir;
  } catch (err) {
    console.error(`Failed to create output directory: ${err}`);
    throw err;
  }
}

/**
 * Saves JSON data to a file
 * @param {Object} data - The data to save
 * @param {string} filePath - Path to save the JSON file
 * @returns {Promise<void>}
 */
async function saveJson(data, filePath) {
  try {
    // Only save if needed (e.g., for debugging)
    if (process.env.SAVE_DEBUG_FILES === 'true') {
        await fs.promises.writeFile(filePath, JSON.stringify(data, null, 4));
        // console.log(`Debug JSON data saved successfully to: ${filePath}`);
    }
  } catch (err) {
    console.error(`Error saving debug JSON file to ${filePath}: ${err}`);
  }
}

/**
 * Processes a single description to detect objects
 * @param {Buffer} imageBuffer - The image buffer
 * @param {string} description - The object description to detect
 * @param {PromptTrackingContext} context - The tracking context containing batch, screenshot, and component IDs
 * @returns {Promise<{objects: Array, duration: number}>} Detected objects and duration
 */
async function detectSingleObject(imageBuffer, description, context) {
  try {
    // Create a component-specific context if this detection is for a specific component
    const componentContext = context.componentId 
      ? context 
      : context; // Use as-is if no component ID present yet

    // We measure only the API call duration, which is handled inside detectObjectsFromBuffer
    const result = await detectObjectsFromBuffer(imageBuffer, description, componentContext);
    
    // The API call duration is already tracked in detectObjectsFromBuffer
    if (result && result.objects && result.objects.length > 0) {
      return { objects: result.objects, duration: result.durationMs }; 
    } else {
      return { objects: [], duration: 0 }; 
    }
  } catch (err) {
    console.error(`Error during detection for '${description}': ${err}`);
    throw err; // Let the caller handle the error state
  }
}

/**
 * Process detected objects and scale coordinates to absolute pixel values
 * @param {Array} detectedObjectsList - List of raw detection objects
 * @param {string} label - The label being processed
 * @param {number} imgWidth - Image width in pixels
 * @param {number} imgHeight - Image height in pixels
 * @returns {Array<{x_min: number, y_min: number, x_max: number, y_max: number}>} List of processed bounding boxes
 */
function processBoundingBoxes(detectedObjectsList, label, imgWidth, imgHeight) {
  const boundingBoxes = [];

  for (const rawDetection of detectedObjectsList) {
    try {
      if ('x_min' in rawDetection && 'y_min' in rawDetection && 'x_max' in rawDetection && 'y_max' in rawDetection) {
        const scaledCoords = normalizedToPixelCoordinates(rawDetection, imgWidth, imgHeight);
        boundingBoxes.push(scaledCoords);
      } else {
        console.warn(`Skipping a detection for label '${label}' due to missing coordinate keys in:`, rawDetection);
      }
    } catch (err) {
      console.error(`Error scaling coordinates for one detection of label '${label}': ${err}`);
      // Decide how to handle scaling errors - skip this box or mark as error?
    }
  }

  return boundingBoxes;
}

/**
 * Main processing function: detects objects, groups by category, generates annotated images/data.
 * Returns structured results per component/category.
 * @param {number} screenshotId - ID of the screenshot being processed
 * @param {Buffer} imageBuffer - Buffer containing the image data
 * @param {Object.<string, string>} labelsDict - Dictionary of {label: description}
 * @param {number} batchId - The ID of the batch this operation is part of
 * @param {string} screenshotUrl - The signed URL of the screenshot for debugging/audit
 * @returns {Promise<ComponentDetectionResult[]>} Array of detection results for each component/category.
 */
export async function processAndSaveByCategory(screenshotId, imageBuffer, labelsDict, batchId, screenshotUrl) {
  const overallStartTime = performance.now();
  let outputDir = null; // Only needed if saving debug files
  if (process.env.SAVE_DEBUG_FILES === 'true') {
      outputDir = await createOutputDirectory();
  }
  const limit = pLimit(MOONDREAM_CONCURRENCY);
  const componentResults = [];

  // Create a tracking context for this screenshot
  const context = createScreenshotTrackingContext(batchId, screenshotId);

  try {
    // --- Image Validation ---
    let validatedImageBuffer = imageBuffer;
    let jimpImage;
    try {
      jimpImage = await Jimp.read(imageBuffer);
    } catch (err) {
      console.error(`Invalid initial image format for screenshot ${screenshotId}: ${err.message}. Attempting conversion...`);
      try {
        // Attempt conversion (e.g., from JPEG or WEBP to PNG buffer)
        const tempImage = await Jimp.read(imageBuffer);
        validatedImageBuffer = await tempImage.getBufferAsync(Jimp.MIME_PNG);
        jimpImage = await Jimp.read(validatedImageBuffer); // Read the converted buffer
        console.log(`Image conversion successful for screenshot ${screenshotId}.`);
      } catch (convErr) {
        // If conversion fails, we cannot proceed with this screenshot
        console.error(`FATAL: Image conversion failed for screenshot ${screenshotId}: ${convErr.message}. Skipping detection for this image.`);
        // Return an empty array or a specific error result? Empty array for now.
        return [];
        // Or: throw new Error(`Could not process image for screenshot ${screenshotId}: ${convErr.message}`);
      }
    }
    const imgWidth = jimpImage.getWidth();
    const imgHeight = jimpImage.getHeight();

    // --- Parallel Detection ---
    const labelEntries = Object.entries(labelsDict);
    console.log(`[Screenshot ${screenshotId}] Starting detection for ${labelEntries.length} labels with concurrency ${MOONDREAM_CONCURRENCY}`);

    // Intermediate structure to hold results per label
    const detectionResultsByLabel = {};

    const detectionTasks = labelEntries.map(([label, description]) =>
      limit(async () => {
        // console.log(`[Screenshot ${screenshotId}] Starting detection for: '${label}'`);
        let detectionData = { objects: [], duration: 0, error: null };
        let status = 'Not Detected';
        try {
            detectionData = await detectSingleObject(validatedImageBuffer, description, context);
            status = detectionData.objects.length > 0 ? 'Detected' : 'Not Detected';
            // console.log(`[Screenshot ${screenshotId}] Finished detection for: '${label}' (Found: ${detectionData.objects.length})`);
        } catch (error) {
            console.error(`[Screenshot ${screenshotId}] Error in detectSingleObject for '${label}':`, error);
            detectionData.error = error;
            status = 'Error'; // Mark detection as errored
        }
        return { label, description, rawDetections: detectionData.objects, duration: detectionData.duration, status, error: detectionData.error };
      })
    );

    const settledDetectionTasks = await Promise.allSettled(detectionTasks);
    console.log(`[Screenshot ${screenshotId}] All detection tasks settled.`);

    // --- Process and Group Results ---
    // First get all labels for dynamic grouping determination
    const allLabels = labelEntries.map(([label]) => label);
    const labelToGroupMap = determineHierarchicalGroups(allLabels);
    
    const elementsByCategory = {};

    settledDetectionTasks.forEach((result, index) => {
      const [originalLabel, originalDescription] = labelEntries[index]; // Get label/desc based on original index

      if (result.status === 'fulfilled') {
        const { label, description, rawDetections, duration, status: detectionStatus, error } = result.value;

        const elementItem = {
          label: label,
          description: description,
          bounding_box: null, // Will be populated if coordinates are valid
          status: detectionStatus,
          vlm_model: VLM_MODEL_NAME,
          element_inference_time: duration, // Time for this specific label's detection
          // accuracy_score: undefined, // To be added later
          // suggested_coordinates: undefined, // To be added later
          error: error ? (error.message || 'Detection Error') : null
        };

        if (detectionStatus === 'Detected' && rawDetections.length > 0) {
          // Currently takes the first box if multiple are returned for one description.
          // Consider how to handle multiple boxes for a single label if needed.
          const boundingBoxes = processBoundingBoxes(rawDetections.slice(0, 1), label, imgWidth, imgHeight);
          if (boundingBoxes.length > 0) {
            elementItem.bounding_box = boundingBoxes[0]; // Assign the first valid box
          } else {
            // Detected but failed coordinate scaling
            elementItem.status = 'Error';
            elementItem.error = elementItem.error || 'Coordinate scaling failed';
            console.warn(`[Screenshot ${screenshotId}] Processed '${label}': Detected but failed to scale coordinates.`);
          }
        } else if (detectionStatus === 'Error') {
            console.warn(`[Screenshot ${screenshotId}] Processed '${label}': Detection failed.`);
        } else {
           // console.log(`[Screenshot ${screenshotId}] Processed '${label}': Not detected.`);
        }

        // Use the dynamically determined category instead of just first level
        const categoryName = labelToGroupMap[label] || getFirstLevelCategory(label);
        
        if (!elementsByCategory[categoryName]) {
          elementsByCategory[categoryName] = [];
        }
        elementsByCategory[categoryName].push(elementItem);

      } else {
        // Task itself failed (rejected promise from p-limit queue, shouldn't happen often with try/catch inside)
        console.error(`[Screenshot ${screenshotId}] Detection task failed unexpectedly for label '${originalLabel}':`, result.reason);
        
        // Use the dynamically determined category instead of just first level
        const categoryName = labelToGroupMap[originalLabel] || getFirstLevelCategory(originalLabel);
        
        if (!elementsByCategory[categoryName]) {
          elementsByCategory[categoryName] = [];
        }
        elementsByCategory[categoryName].push({
          label: originalLabel,
          description: originalDescription,
          bounding_box: null,
          status: 'Error',
          vlm_model: VLM_MODEL_NAME,
          element_inference_time: 0, // Unknown duration
          error: result.reason?.message || 'Unknown task error'
        });
      }
    });

    // --- Generate Component Results (Image Buffer + Data) ---
    const componentProcessingPromises = Object.entries(elementsByCategory).map(async ([categoryName, elements]) => {
        const categoryStartTime = performance.now();

        // Filter items relevant for drawing (successfully detected with boxes)
        const detectedElements = elements.filter(el => el.status === 'Detected' && el.bounding_box);

        // Generate annotated image buffer for this category
        /** @type {Buffer | null} */
        const annotatedImageBuffer = await generateAnnotatedImageBuffer(
            validatedImageBuffer,
            detectedElements,
            BOX_COLOR, // Use a consistent color or cycle colors per category if needed
            categoryName
        );

        // Save debug image if enabled and buffer exists
        if (outputDir && annotatedImageBuffer) {
           await saveAnnotatedImageDebug(annotatedImageBuffer, categoryName, outputDir);
        }
        // Save debug JSON if enabled
        if (outputDir) {
            const normalizedKey = normalizeLabel(categoryName);
            const jsonPath = path.join(outputDir, `${normalizedKey}.json`);
            await saveJson({ screenshotId, categoryName, elements }, jsonPath); // Save all elements for the category
        }


        // Determine overall status for the component
        let componentStatus = 'failed';
        const hasSuccess = elements.some(el => el.status === 'Detected');
        const hasError = elements.some(el => el.status === 'Error');
        if (hasSuccess && !hasError) {
            componentStatus = 'success';
        } else if (hasSuccess && hasError) {
            componentStatus = 'partial';
        } else if (!hasSuccess && hasError) {
            componentStatus = 'failed'; // All elements failed or errored
        } else {
            componentStatus = 'failed'; // No elements detected or processed successfully
        }

        // Aggregate inference time (sum of individual element times)
        const totalInferenceTime = elements.reduce((sum, el) => sum + el.element_inference_time, 0);

        // Construct the ComponentDetectionResult
        /** @type {ComponentDetectionResult} */
        const componentResult = {
            screenshot_id: screenshotId,
            component_name: categoryName,
            // Store the original image buffer
            original_image_object: imageBuffer,
            annotated_image_object: annotatedImageBuffer,
            annotated_image_url: undefined, // To be filled after upload
            screenshot_url: screenshotUrl, // Store the screenshot signed URL for debugging/audit
            // TODO: Define how to get a meaningful component_description. Using category name for now.
            component_description: `Detection results for ${categoryName}`,
            detection_status: componentStatus,
            inference_time: totalInferenceTime, // Or use category wall time: performance.now() - categoryStartTime;
            elements: elements, // Include all elements (detected, not detected, error)
        };

        componentResults.push(componentResult);
        // console.log(`[Screenshot ${screenshotId}] Finished processing component: '${categoryName}'`);
    });

    await Promise.all(componentProcessingPromises);

    const overallDuration = performance.now() - overallStartTime;
    console.log(`[Screenshot ${screenshotId}] Annotation Complete. Total time: ${overallDuration.toFixed(2)/1000}s`);
    return componentResults;

  } catch (err) {
    // Catch errors during initial setup (e.g., image reading/conversion)
    console.error(`[Screenshot ${screenshotId}] FATAL error during Moondream processing setup:`, err);
    // Return empty array to indicate failure for this screenshot
    return [];
  }
}

/**
 * Processes an image from a file path using labels dictionary
 * @param {string} imagePath - Path to the image file
 * @param {Object} labelsDict - Dictionary of labels and their descriptions
 * @returns {Promise<Object|null>} Categories with their detected items
 */
export async function processImageFile(imagePath, labelsDict) {
  try {
    let imageBuffer = await fs.promises.readFile(imagePath);
    console.log(`Image loaded successfully from: ${imagePath}`);
    
    return processAndSaveByCategory(imageBuffer, labelsDict);
  } catch (err) {
    console.error(`Error processing image ${imagePath}: ${err}`);
    return null;
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/ai/MoondreamVLService.js
================
import { vl } from 'moondream';
import { PromptTrackingContext } from '@/lib/logger';
import { MOON_DREAM_API_KEY } from '@/config';
import fs from 'fs';
import { PromptLogType } from '@/lib/constants';

/**
 * Type definition for detection results
 */
export const DetectionResultType = {
  request_id: String,
  objects: Array
};

/**
 * Initialize the vl model with API key
 */
const model = new vl({ apiKey: `${MOON_DREAM_API_KEY}` });

/**
 * Detect objects in an image using Moondream vl client
 * 
 * @param {Buffer} imageBuffer - Buffer containing image data
 * @param {string} objectType - The object type to detect (e.g., "person", "car", "face")
 * @param {PromptTrackingContext} context - The tracking context containing batch, screenshot, and component IDs
 * @returns {Promise<Object>} A promise resolving to the detection response
 */
export async function detectObjectsFromBuffer(
  imageBuffer, 
  objectType, 
  context
) {
  try {
    // Timing specifically the API call, not surrounding logic
    const startTime = Date.now();
    
    const result = await model.detect({
      image: imageBuffer,
      object: objectType
    });
    
    const endTime = Date.now();
    const durationMs = endTime - startTime;
    
    // Log the interaction using the context with the measured duration
    await context.logPromptInteraction(
      'Moondream-vl-Detect',
      PromptLogType.VLM_LABELING,
      `Detect ${objectType} in image`,
      JSON.stringify(result),
      durationMs,
      {
        // Moondream doesn't provide token usage, so we leave these undefined
        input: undefined,
        output: undefined,
        total: undefined
      }
    );

    console.log(`-- [Detected] "${objectType.slice(0,50)}..." in ${durationMs/1000}s`);
    return {
      request_id: result.request_id,
      objects: result.objects,
      durationMs: durationMs/1000 // Return the duration of the operation in milliseconds
    };
  } catch (err) {
    console.error('Error using Moondream vl:', err);
    throw new Error(
      `Failed to get response from Moondream vl: ${
        err instanceof Error ? err.message : String(err)
      }`
    );
  }
}

/**
 * Converts the normalized coordinates to pixel coordinates
 * 
 * @param {Object} coordinates - Object with normalized coordinates (0-1)
 * @param {number} coordinates.x_min - Minimum x coordinate (0-1)
 * @param {number} coordinates.y_min - Minimum y coordinate (0-1)
 * @param {number} coordinates.x_max - Maximum x coordinate (0-1)
 * @param {number} coordinates.y_max - Maximum y coordinate (0-1)
 * @param {number} imageWidth - Width of the image in pixels
 * @param {number} imageHeight - Height of the image in pixels
 * @returns {Object} Object with pixel coordinates
 */
export function normalizedToPixelCoordinates(coordinates, imageWidth, imageHeight) {
  return {
    x_min: Math.round(coordinates.x_min * imageWidth),
    y_min: Math.round(coordinates.y_min * imageHeight),
    x_max: Math.round(coordinates.x_max * imageWidth),
    y_max: Math.round(coordinates.y_max * imageHeight)
  };
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/ai/OpenAIService.js
================
import OpenAI from 'openai';
// import { PromptResult } from '../../../types/PromptRunner'; // Adjust the path as needed
import { EXTRACTION_PROMPT_v1, EXTRACT_ELEMENTS_PROMPT, EXTRACTION_PROMPT_v2, EXTRACTION_PROMPT_v3, EXTRACTION_PROMPT_v4, ACCURACY_VALIDATION_PROMPT_v0, EXTRACTION_PROMPT_v6, METADATA_EXTRACTION_PROMPT_FINAL } from '@/lib/prompt/prompts';
import { PromptTrackingContext } from '@/lib/logger';
import { PromptLogType } from '@/lib/constants';
import { cleanText } from '@/lib/file-utils';
// Ensure OPENAI_API_KEY is set in your environment variables
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Constants
const OPENAI_CONFIG = {
  VISION_MODEL: 'gpt-4o-2024-11-20',
  VISION_MODEL_GPT4: 'gpt-4.1-2025-04-14',
  INPUT_TOKEN_COST: 0.00001,
  OUTPUT_TOKEN_COST: 0.00003
};

/**
 * Handles the common response processing logic for OpenAI API calls
 */
async function handleOpenAIResponse(response, context, promptType, prompt, startTime) {
  const endTime = Date.now();
  const durationMs = endTime - startTime;

  const inputTokens = response?.usage?.input_tokens || 0;
  const outputTokens = response?.usage?.output_tokens || 0;
  
  await context.logPromptInteraction(
    `OpenAI-${OPENAI_CONFIG.VISION_MODEL_GPT4}`,
    promptType,
    prompt,
    JSON.stringify(response),
    durationMs,
    {
      input: inputTokens,
      output: outputTokens,
      total: response?.usage?.total_tokens
    },
    OPENAI_CONFIG.INPUT_TOKEN_COST,
    OPENAI_CONFIG.OUTPUT_TOKEN_COST
  );

  return response;
}

/**
 * Base function for making OpenAI API calls
 */
async function makeOpenAICall(messages, context, promptType, prompt) {
  const startTime = Date.now();
  
  try {
    const response = await openai.responses.create({
      model: OPENAI_CONFIG.VISION_MODEL_GPT4,
      input: messages
    });
    
    return await handleOpenAIResponse(response, context, promptType, prompt, startTime);
  } catch (error) {
    console.error('Error calling OpenAI Vision Model:', error);
    throw new Error(`Failed to get response from OpenAI: ${error instanceof Error ? error.message : String(error)}`);
  }
}

/**
 * Creates messages payload for OpenAI API
 */
function createMessagesPayload(prompt, image) {
  return [{
    role: 'user',
    content: [
      { type: 'input_text', text: prompt },
      { type: 'input_image', image_url: image }
    ],
  }];
}

/**
 * Calls the OpenAI vision model with a prompt and optional image URL.
 *
 * @param prompt The text prompt to send to the model.
 * @param imageUrl Optional URL of an image for the vision model.
 * @param context The tracking context containing batch, screenshot, and other IDs
 * @param promptType The type of prompt being processed.
 * @returns A promise resolving to the structured PromptResult.
 * @throws Throws an error if the API call fails.
 */
export async function callOpenAIVisionModelURL(
  prompt,
  imageUrl,
  context,
  promptType = PromptLogType.COMPONENT_EXTRACTION
){
  const messages = createMessagesPayload(prompt, imageUrl);
  return makeOpenAICall(messages, context, promptType, prompt);
} 

export async function callOpenAIVisionModelBase64(
  prompt,
  imageBase64,
  context,
  promptType = PromptLogType.ACCURACY_VALIDATION
) {
  const imageUrl = `data:image/jpeg;base64,${imageBase64}`;
  const messages = createMessagesPayload(prompt, imageUrl);
  return makeOpenAICall(messages, context, promptType, prompt);
}

/**
 * Extracts components from an image using the OpenAI vision model.
 *
 * @param imageUrl The URL of the image to analyze.
 * @param context The tracking context containing batch, screenshot, and other IDs
 * @returns A promise resolving to the structured PromptResult.
 * @throws Throws an error if the API call fails.
 */
export async function extract_component_from_image(imageUrl, context) {
  const result = await callOpenAIVisionModelURL(
    EXTRACTION_PROMPT_v6, 
    imageUrl, 
    context,
    PromptLogType.COMPONENT_EXTRACTION
  );
  
  return processResponse(result, `Component extraction failed for URL: ${imageUrl}`);
}

export async function validate_bounding_boxes_base64(imageBase64, context, elementsJson) {
  // Create prompt with elements JSON included
  const prompt = elementsJson 
    ? `${ACCURACY_VALIDATION_PROMPT_v0}\n\nHere are the elements to evaluate:\n${elementsJson}`
    : ACCURACY_VALIDATION_PROMPT_v0;
    
  const result = await callOpenAIVisionModelBase64(
    prompt, 
    imageBase64, 
    context,
    PromptLogType.ACCURACY_VALIDATION
  );

  return processResponse(result, 'Bounding box validation failed for image');
}

/**
 * Extracts metadata from a component image and its elements
 *
 * @param imageBase64 Base64 encoded string of the component image
 * @param inputPayload JSON string containing component_name and elements
 * @param context The tracking context containing batch, screenshot, and other IDs
 * @returns A promise resolving to the structured metadata result
 * @throws Throws an error if the API call fails
 */
export async function extract_component_metadata(imageBase64, inputPayload, context) {
  // Create prompt with component and elements JSON included
  const prompt = `${METADATA_EXTRACTION_PROMPT_FINAL}\n\nHere is the component information:\n${inputPayload}`;
    
  const result = await callOpenAIVisionModelBase64(
    prompt, 
    imageBase64, 
    context,
    PromptLogType.METADATA_EXTRACTION
  );

  return processResponse(result, 'Metadata extraction failed for image');
}

/**
 * Processes OpenAI response data
 */
function processResponse(result, errorMessage) {
  if (!result || result.status !== 'completed') {
    throw new Error(errorMessage);
  }

  const { parsedContent, usage } = extractOpenAIResponseData(result);
  return { parsedContent, usage };
}

/**
 * Helper: Parses OpenAI's output_text string into a JSON object safely.
 *
 * @param rawText - The raw `output_text` returned from OpenAI.
 * @returns Parsed JSON object.
 */
function parseOpenAIOutputTextToJson(rawText) {
  try {
    // First, remove markdown code block delimiters if they exist
    const trimmedText = rawText.trim();
    let contentText = trimmedText;
    
    // Check for and remove markdown code blocks (```json or just ```)
    const codeBlockRegex = /^```(?:json)?\s*([\s\S]*?)```$/;
    const match = trimmedText.match(codeBlockRegex);
    
    if (match && match[1]) {
      contentText = match[1].trim();
    }
    
    // Clean up any trailing commas that might cause JSON parsing errors
    const cleaned = contentText
      .replace(/,\s*}/g, '}') // remove trailing commas in objects
      .replace(/,\s*]/g, ']'); // remove trailing commas in arrays

    return JSON.parse(cleaned);
  } catch (error) {
    console.error('Failed to parse OpenAI output_text as JSON:', error);
    console.error('Raw text was:', rawText.substring(0, 100) + (rawText.length > 100 ? '...' : ''));
    return [];
  }
}

/**
 * Extracts structured content and usage metadata from OpenAI's response.
 *
 * @param response - Full OpenAI response object.
 * @returns Object containing parsed output_text and token usage info.
 */
export function extractOpenAIResponseData(response) {
  const rawText = response?.output_text ?? '';
  
  try {
    const parsedContent = parseOpenAIOutputTextToJson(rawText);
    
    const usage = {
      input_tokens: response?.usage?.input_tokens,
      output_tokens: response?.usage?.output_tokens,
      total_tokens: response?.usage?.total_tokens
    };
  
    return {
      parsedContent,
      rawText,
      usage,
    };
  } catch (error) {
    console.error('Error extracting OpenAI response data:', error);
    console.error('Raw response output_text (first 200 chars):', rawText.substring(0, 200));
    
    // Return empty array as parsedContent to handle gracefully
    return {
      parsedContent: [],
      rawText,
      usage: {
        input_tokens: response?.usage?.input_tokens || 0,
        output_tokens: response?.usage?.output_tokens || 0,
        total_tokens: response?.usage?.total_tokens || 0
      }
    };
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/imageServices/BoundingBoxService.js
================
import Jimp from 'jimp';
import fs from 'fs';
import path from 'path';

// --- Constants ---
const BOX_COLOR = 0xFF0000FF; // RGBA Red color
const BOX_WIDTH = 2;
const OVERLAY_COLOR = 0x80808080; // RGBA semi-transparent gray
const OVERLAY_ALPHA = 0.5;
const REFERENCE_POINT_COLOR = 0x00FF00FF; // RGBA Green color
const REFERENCE_POINT_SIZE = 4;
const TEXT_COLOR = 0xFFFFFFFF; // RGBA White color
const TEXT_BACKGROUND = 0x000000AA; // RGBA Black color with some transparency

// Accuracy Colors matching ValidationService thresholds
const ACCURACY_COLORS = {
  HIGH: 0x00FF00FF,   // Green (≥85%)
  MEDIUM: 0xFFFF00FF, // Yellow (70-84%)
  LOW_MEDIUM: 0xFFA500FF, // Orange (50-69%)
  LOW: 0xFF0000FF,    // Red (<50%)
  SUGGESTED: 0xFFA500FF // Orange for suggested boxes
};

const SHOW_COORDINATES = process.env.SHOW_COORDINATES === 'true';

/**
 * Normalizes a label string into a file/key-friendly format
 * @param {string} label - The label to normalize
 * @returns {string} Normalized label string
 */
function normalizeLabel(label) {
  return label.toLowerCase().replace(/\s/g, '_').replace(/>/g, '_').replace(/\//g, '_');
}

/**
 * Draw a rectangle with a specific width
 * @param {Jimp} image - Jimp image to draw on
 * @param {number} x - X coordinate of top-left corner
 * @param {number} y - Y coordinate of top-left corner
 * @param {number} width - Width of the rectangle
 * @param {number} height - Height of the rectangle
 * @param {number} color - Color of the rectangle (RGBA hex)
 * @param {number} lineWidth - Width of the rectangle border
 */
function drawRect(image, x, y, width, height, color, lineWidth) {
  // Draw top line
  image.scan(x, y, width, lineWidth, function(cx, cy, idx) {
    this.bitmap.data[idx] = (color >> 24) & 0xFF;     // R
    this.bitmap.data[idx + 1] = (color >> 16) & 0xFF; // G
    this.bitmap.data[idx + 2] = (color >> 8) & 0xFF;  // B
    this.bitmap.data[idx + 3] = color & 0xFF;         // A
  });
  
  // Draw bottom line
  image.scan(x, y + height - lineWidth, width, lineWidth, function(cx, cy, idx) {
    this.bitmap.data[idx] = (color >> 24) & 0xFF;     // R
    this.bitmap.data[idx + 1] = (color >> 16) & 0xFF; // G
    this.bitmap.data[idx + 2] = (color >> 8) & 0xFF;  // B
    this.bitmap.data[idx + 3] = color & 0xFF;         // A
  });
  
  // Draw left line
  image.scan(x, y, lineWidth, height, function(cx, cy, idx) {
    this.bitmap.data[idx] = (color >> 24) & 0xFF;     // R
    this.bitmap.data[idx + 1] = (color >> 16) & 0xFF; // G
    this.bitmap.data[idx + 2] = (color >> 8) & 0xFF;  // B
    this.bitmap.data[idx + 3] = color & 0xFF;         // A
  });
  
  // Draw right line
  image.scan(x + width - lineWidth, y, lineWidth, height, function(cx, cy, idx) {
    this.bitmap.data[idx] = (color >> 24) & 0xFF;     // R
    this.bitmap.data[idx + 1] = (color >> 16) & 0xFF; // G
    this.bitmap.data[idx + 2] = (color >> 8) & 0xFF;  // B
    this.bitmap.data[idx + 3] = color & 0xFF;         // A
  });
}

/**
 * Draw a filled rectangle on the image
 * @param {Jimp} image - Jimp image to draw on
 * @param {number} x - X coordinate of top-left corner
 * @param {number} y - Y coordinate of top-left corner
 * @param {number} width - Width of the rectangle
 * @param {number} height - Height of the rectangle
 * @param {number} color - Color of the rectangle (RGBA hex)
 */
function fillRect(image, x, y, width, height, color) {
  // Make sure coordinates are integers and within bounds
  const startX = Math.max(0, Math.floor(x));
  const startY = Math.max(0, Math.floor(y));
  const imgWidth = image.getWidth();
  const imgHeight = image.getHeight();
  
  // Ensure width/height calculations don't exceed image boundaries
  const endX = Math.min(imgWidth, startX + width);
  const endY = Math.min(imgHeight, startY + height);
  
  const drawWidth = endX - startX;
  const drawHeight = endY - startY;
  
  if (drawWidth <= 0 || drawHeight <= 0) return;
  
  image.scan(startX, startY, drawWidth, drawHeight, function(cx, cy, idx) {
    this.bitmap.data[idx] = (color >> 24) & 0xFF;     // R
    this.bitmap.data[idx + 1] = (color >> 16) & 0xFF; // G
    this.bitmap.data[idx + 2] = (color >> 8) & 0xFF;  // B
    this.bitmap.data[idx + 3] = color & 0xFF;         // A
  });
}

/**
 * Creates a transparent area in the overlay for detected objects
 * @param {Jimp} overlay - Overlay image to modify
 * @param {number} x - X coordinate of top-left corner
 * @param {number} y - Y coordinate of top-left corner
 * @param {number} width - Width of the transparent area
 * @param {number} height - Height of the transparent area
 * @param {number} boxWidth - Width of the border that should remain opaque
 */
function createTransparentArea(overlay, x, y, width, height, boxWidth) {
   // Ensure x, y, width, height are within overlay bounds and integers
   const overlayWidth = overlay.getWidth();
   const overlayHeight = overlay.getHeight();

   const startX = Math.max(0, Math.floor(x + boxWidth));
   const startY = Math.max(0, Math.floor(y + boxWidth));
   const endX = Math.min(overlayWidth, Math.floor(x + width - boxWidth));
   const endY = Math.min(overlayHeight, Math.floor(y + height - boxWidth));

   const clearWidth = Math.max(0, endX - startX);
   const clearHeight = Math.max(0, endY - startY);

   if (clearWidth > 0 && clearHeight > 0) {
       overlay.scan(startX, startY, clearWidth, clearHeight, function(cx, cy, idx) {
           this.bitmap.data[idx + 3] = 0; // Set alpha to 0 (fully transparent)
       });
   }
}

/**
 * Draw a simple coordinate label directly with pixel rectangles
 * @param {Jimp} image - Jimp image to draw on
 * @param {number} x - X coordinate for text start
 * @param {number} y - Y coordinate for text start
 * @param {string} value - Value to display
 * @param {number} color - Text color
 */
function drawCoordinateLabel(image, x, y, value, color = TEXT_COLOR) {
  // First draw background for better visibility
  const padding = 2;
  const charWidth = 5;
  const charHeight = 7;
  const totalWidth = value.length * (charWidth + 1) + padding * 2;
  const totalHeight = charHeight + padding * 2;
  
  // Ensure text stays within image bounds
  const imgWidth = image.getWidth();
  const imgHeight = image.getHeight();
  
  const textX = Math.max(0, Math.min(imgWidth - totalWidth, x));
  const textY = Math.max(0, Math.min(imgHeight - totalHeight, y));
  
  // Draw background
  fillRect(image, textX, textY, totalWidth, totalHeight, TEXT_BACKGROUND);
  
  // Draw text value
  image.scan(textX, textY, totalWidth, totalHeight, function(cx, cy, idx) {
    // Set entire area to the background color with some transparency
    this.bitmap.data[idx + 3] = 180; // Alpha component
  });
  
  // Place coordinate values as plain text using a simple technique
  const text = value;
  let currentX = textX + padding;
  
  for (let i = 0; i < text.length; i++) {
    // Draw a small rectangle for each character with the text color
    // This is a simple way to "stamp" the presence of text without rendering actual fonts
    fillRect(image, currentX, textY + padding, charWidth, charHeight, color);
    currentX += charWidth + 1;
  }
}

/**
 * Draw reference points at 0%, 25%, 50%, 75%, and 100% positions on each edge of the image
 * @param {Jimp} image - Jimp image to draw on
 * @param {number} imgWidth - Width of the image
 * @param {number} imgHeight - Height of the image
 * @param {number} color - Color for reference points (RGBA hex)
 * @param {number} pointSize - Size of the reference points
 */
function drawReferencePoints(image, imgWidth, imgHeight, color = REFERENCE_POINT_COLOR, pointSize = REFERENCE_POINT_SIZE) {
  const percentages = [0, 0.25, 0.5, 0.75, 1.0];
  
  // Top edge points
  percentages.forEach(percent => {
    const x = Math.floor(percent * (imgWidth - 1));
    const y = 0;
    
    // Draw point (larger to be more visible)
    fillRect(image, x - pointSize/2, y - pointSize/2, pointSize, pointSize, color);
    
    // Draw coordinate label
    const labelText = `(${x},${y})`;
    drawCoordinateLabel(image, x - 20, y + pointSize * 2, labelText);
  });
  
  // Bottom edge points
  percentages.forEach(percent => {
    const x = Math.floor(percent * (imgWidth - 1));
    const y = imgHeight - 1;
    
    // Draw point
    fillRect(image, x - pointSize/2, y - pointSize/2, pointSize, pointSize, color);
    
    // Draw coordinate label
    const labelText = `(${x},${y})`;
    drawCoordinateLabel(image, x - 20, y - 20, labelText);
  });
  
  // Left edge points
  percentages.forEach(percent => {
    const x = 0;
    const y = Math.floor(percent * (imgHeight - 1));
    
    // Draw point
    fillRect(image, x - pointSize/2, y - pointSize/2, pointSize, pointSize, color);
    
    // Draw coordinate label
    const labelText = `(${x},${y})`;
    drawCoordinateLabel(image, x + pointSize * 2, y - 10, labelText);
  });
  
  // Right edge points
  percentages.forEach(percent => {
    const x = imgWidth - 1;
    const y = Math.floor(percent * (imgHeight - 1));
    
    // Draw point
    fillRect(image, x - pointSize/2, y - pointSize/2, pointSize, pointSize, color);
    
    // Draw coordinate label
    const labelText = `(${x},${y})`;
    drawCoordinateLabel(image, x - 50, y - 10, labelText);
  });
}



/**
 * Generates an annotated image buffer for a specific component/category
 * @param {Buffer} baseImageBuffer - The original image buffer
 * @param {Array} detectedItems - Detected items for this component
 * @param {number} color - Color for bounding boxes (defaults to BOX_COLOR)
 * @param {string} categoryName - Name of the category (for logging)
 * @returns {Promise<Buffer|null>} Buffer of the annotated image, or null on error
 */
async function generateAnnotatedImageBuffer(baseImageBuffer, detectedItems, color = BOX_COLOR, categoryName) {
  const itemsToDraw = detectedItems.filter(item => item.status === 'Detected' && item.bounding_box);

  try {
    const baseImage = await Jimp.read(baseImageBuffer);
    const imgWidth = baseImage.getWidth();
    const imgHeight = baseImage.getHeight();

    // If there are no items to draw but we need to show coordinates, draw them on the base image
    if (itemsToDraw.length === 0) {
      if (SHOW_COORDINATES) {
        drawReferencePoints(baseImage, imgWidth, imgHeight);
      }
      return await baseImage.getBufferAsync(Jimp.MIME_PNG); // Ensure consistent format
    }

    // Create a semi-transparent overlay
    const overlay = new Jimp(imgWidth, imgHeight, OVERLAY_COLOR);

    // Sort itemsToDraw by area (descending) to process larger boxes first
    // This prevents large-box clear from erasing smaller borders
    itemsToDraw.sort((a, b) => {
      const areaA = (a.bounding_box.x_max - a.bounding_box.x_min) * (a.bounding_box.y_max - a.bounding_box.y_min);
      const areaB = (b.bounding_box.x_max - b.bounding_box.x_min) * (b.bounding_box.y_max - b.bounding_box.y_min);
      return areaB - areaA; // Descending order
    });

    // Process each detection and draw boxes on the overlay
    for (const item of itemsToDraw) {
      const { x_min, y_min, x_max, y_max } = item.bounding_box;

      // Make sure coordinates are integers and within bounds
      const x = Math.max(0, Math.floor(x_min));
      const y = Math.max(0, Math.floor(y_min));
      // Ensure width/height calculations don't exceed image boundaries
      const potentialWidth = Math.ceil(x_max - x_min);
      const potentialHeight = Math.ceil(y_max - y_min);
      const width = Math.min(imgWidth - x, potentialWidth);
      const height = Math.min(imgHeight - y, potentialHeight);

      if (width <= 0 || height <= 0) {
        console.warn(`Invalid box dimensions for item '${item.label}' in category '${categoryName}': ${width}x${height}`);
        continue;
      }

      // Draw the box outline on the overlay
      drawRect(overlay, x, y, width, height, color, BOX_WIDTH);

      // Create transparent area inside the box
      createTransparentArea(overlay, x, y, width, height, BOX_WIDTH);
    }

    // Composite the overlay onto the base image
    baseImage.composite(overlay, 0, 0, {
      mode: Jimp.BLEND_SOURCE_OVER,
      opacitySource: 1, // Use overlay's alpha
      opacityDest: 1
    });

    // Draw reference points if SHOW_COORDINATES is true
    if (SHOW_COORDINATES) {
      drawReferencePoints(baseImage, imgWidth, imgHeight);
    }

    // Return the buffer
    const annotatedBuffer = await baseImage.getBufferAsync(Jimp.MIME_PNG);
    // console.log(`Generated annotated image buffer for category '${categoryName}'.`);
    return annotatedBuffer;

  } catch (err) {
    console.error(`Error generating annotated image buffer for category '${categoryName}': ${err}`);
    return null;
  }
}

/**
 * Renders a category image (for debugging/local saving)
 * @param {Buffer} annotatedImageBuffer - The generated annotated image buffer
 * @param {string} categoryName - Name of the category
 * @param {string} outputDir - Directory to save output
 * @returns {Promise<void>}
 */
async function saveAnnotatedImageDebug(annotatedImageBuffer, categoryName, outputDir) {
   if (!annotatedImageBuffer || process.env.SAVE_DEBUG_FILES !== 'true') {
     return; // Don't save if buffer is null or debug saving is off
   }
   try {
     const normalizedKey = normalizeLabel(categoryName);
     const savePath = path.join(outputDir, `${normalizedKey}.png`);
     await fs.promises.writeFile(savePath, annotatedImageBuffer);
    //  console.log(`Debug image saved successfully to: ${savePath}`);
   } catch (err) {
     console.error(`Error saving debug image for '${categoryName}': ${err}`);
   }
}

export {
  BOX_COLOR,
  BOX_WIDTH,
  OVERLAY_COLOR,
  OVERLAY_ALPHA,
  REFERENCE_POINT_COLOR,
  REFERENCE_POINT_SIZE,
  TEXT_COLOR,
  TEXT_BACKGROUND,
  ACCURACY_COLORS,
  drawRect,
  fillRect,
  // drawDashedRect,
  createTransparentArea,
  drawCoordinateLabel,
  drawReferencePoints,
  generateAnnotatedImageBuffer,
  saveAnnotatedImageDebug,
  normalizeLabel
};

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/imageServices/imageFetchingService.ts
================
import { BatchProcessingScreenshot as Screenshot } from '@/types/BatchProcessingScreenshot';
import { Buffer } from 'buffer';

/**
 * Fetches an image and returns it as a Buffer for direct use in image processing.
 * Provides raw binary data with minimal overhead compared to Blob or Base64.
 *
 * @param imageUrl - the signed URL (or any URL) of the image
 * @returns a promise resolving to a Buffer containing the image data
 */
export async function fetchImageAsBuffer(imageUrl: string): Promise<Buffer | null> {
  try {
    const res = await fetch(imageUrl);
    if (!res.ok) throw new Error(`HTTP ${res.status} ${res.statusText}`);
    
    // Get the raw ArrayBuffer first
    const arrayBuffer = await res.arrayBuffer();
    
    // Convert ArrayBuffer to Buffer
    const buffer = Buffer.from(arrayBuffer);
    
    // console.log(`Successfully fetched image buffer (${buffer.byteLength} bytes)`);
    return buffer;
  } catch (err) {
    console.error("Error fetching image as buffer:", err);
    return null;
  }
}

/**
 * Fetches image data as Buffer for multiple screenshots with signed URLs
 * @param screenshots Array of screenshot objects with screenshot_signed_url property
 * @returns The same array with screenshot_image_buffer property populated
 */
export async function fetchScreenshotBuffers(screenshots: Screenshot[]): Promise<Screenshot[]> {
  // console.log(`Fetching image buffers for ${screenshots.length} screenshots...`);
  
  // Create an array of promises for fetching each image
  const fetchPromises = screenshots.map(async (screenshot) => {
    // Skip screenshots without a signed URL
    if (!screenshot.screenshot_signed_url) {
      console.warn(`Screenshot ID ${screenshot.screenshot_id} has no signed URL, skipping buffer fetch`);
      screenshot.screenshot_image_buffer = null;
      return screenshot;
    }
    
    try {
      // Fetch the image buffer and attach it directly
      screenshot.screenshot_image_buffer = await fetchImageAsBuffer(screenshot.screenshot_signed_url);
      
      if (screenshot.screenshot_image_buffer) {
        console.log(`Successfully fetched buffer for screenshot ID ${screenshot.screenshot_id} (${screenshot.screenshot_image_buffer.byteLength} bytes)`);
      }
    } catch (error) {
      console.error(`Error fetching buffer for screenshot ID ${screenshot.screenshot_id}:`, error);
      screenshot.screenshot_image_buffer = null;
    }
    
    return screenshot;
  });
  
  // Wait for all fetch operations to complete
  const updatedScreenshots = await Promise.all(fetchPromises);
  
  // Log summary of results
  const successCount = updatedScreenshots.filter(s => s.screenshot_image_buffer !== null).length;
  // console.log(`Fetched ${successCount}/${screenshots.length} image buffers successfully`);
  
  return updatedScreenshots;
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/imageServices/ImageProcessor.ts
================
import sharp from 'sharp'
import fs from 'fs'
import path from 'path'
// import { v4 as uuidv4 } from 'uuid'

// Constants instead of magic numbers
const MAX_FILE_SIZE_MB = 1
const DEFAULT_TARGET_WIDTH = 800
const DEFAULT_TARGET_HEIGHT = 800
const DEFAULT_JPEG_QUALITY = 80

// Temporary directory for processed images
const TEMP_DIR = path.join(process.cwd(), 'tmp')

/**
 * Normalizes a filename by removing special characters and spaces
 * @param filename - Original filename to normalize
 * @returns Normalized filename with only alphanumeric characters, dots, and hyphens
 */
function sanitizeFilename(filename: string): string {
  // Remove file extension
  const { name, ext } = path.parse(filename)
  
  // Replace spaces and special characters with hyphens
  const normalized = name
    .toLowerCase()
    .replace(/[^a-z0-9]/g, '-') // Replace non-alphanumeric with hyphens
    .replace(/-+/g, '-')        // Replace multiple hyphens with single hyphen
    .replace(/^-|-$/g, '')      // Remove leading/trailing hyphens
  
  // Add timestamp to ensure uniqueness
  return `${normalized}${ext}`
}

// Ensure temp directory exists
if (!fs.existsSync(TEMP_DIR)) {
  fs.mkdirSync(TEMP_DIR, { recursive: true })
}

interface ProcessedImage {
  buffer: Buffer;
  filename: string;
}

/**
 * Compresses and pads an image to make it uniform in size
 * @param imageBuffer - The raw image buffer
 * @param originalFilename - Original filename to preserve
 * @param targetWidth - Desired width after processing
 * @param targetHeight - Desired height after processing
 * @returns Object containing the path and filename of the processed image
 */
export async function resizeAndPadImageBuffer(
  imageBuffer: Buffer,
  originalFilename: string,
  targetWidth: number = DEFAULT_TARGET_WIDTH,
  targetHeight: number = DEFAULT_TARGET_HEIGHT
): Promise<ProcessedImage> {
  const filename = sanitizeFilename(originalFilename)
  
  try {
    const metadata = await sharp(imageBuffer).metadata()
    
    // Calculate resize dimensions while maintaining aspect ratio
    let resizeWidth = targetWidth
    let resizeHeight = targetHeight
    
    if (metadata.width && metadata.height) {
      const aspectRatio = metadata.width / metadata.height
      
      if (aspectRatio > 1) {
        // Landscape image
        resizeHeight = Math.round(targetWidth / aspectRatio)
      } else {
        // Portrait image
        resizeWidth = Math.round(targetHeight * aspectRatio)
      }
    }
    
    // Process image and return buffer directly
    const processedBuffer = await sharp(imageBuffer)
      .resize(resizeWidth, resizeHeight, {
        fit: 'inside',
        withoutEnlargement: true
      })
      .jpeg({ quality: DEFAULT_JPEG_QUALITY }) // Compress to reduce file size
      .toBuffer()
      .then(resizedBuffer => {
        // Create a blank canvas with the target dimensions
        return sharp({
          create: {
            width: targetWidth,
            height: targetHeight,
            channels: 4,
            background: { r: 255, g: 255, b: 255, alpha: 1 }
          }
        })
        .composite([{
          input: resizedBuffer,
          gravity: 'center'
        }])
        .jpeg({ quality: DEFAULT_JPEG_QUALITY })
        .toBuffer()
      })

    return {
      buffer: processedBuffer,
      filename
    }
  } catch (error) {
    console.error('Error processing image:', error)
    throw error
  }
}

/**
 * Cleans up a temporary file
 */
export function deleteFile(filePath: string): void {
  try {
    if (fs.existsSync(filePath)) {
      fs.unlinkSync(filePath)
    }
  } catch (error) {
    console.error('Error cleaning up temp file:', error)
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/imageServices/screenshotProcessor.ts
================
import { File } from 'formidable';
import fs from 'fs';
import { resizeAndPadImageBuffer, deleteFile } from '@/lib/services/imageServices/ImageProcessor';
import { uploadImageToStorage } from '@/lib/storage';
import { supabase } from '@/lib/supabase'; // Assuming shared Supabase client
import { SupabaseClient } from '@supabase/supabase-js';

interface ProcessedImage {
  processedBlob: Blob;
  filename: string;
  processingTime?: number; // In seconds
}

export class ScreenshotProcessor {
  private supabaseClient: SupabaseClient;

  // Allow injecting Supabase client for testability/flexibility
  constructor(supabaseClient: SupabaseClient = supabase) {
    this.supabaseClient = supabaseClient;
  }

  /**
   * Processes a single uploaded file: resizes, pads, uploads to storage,
   * and saves the record in the database.
   * @param file The uploaded file object from formidable.
   * @param batchId The ID of the batch this screenshot belongs to.
   * @returns The name and URL of the uploaded screenshot.
   * @throws Error if any step fails.
   */
  public async processAndSave(file: File, batchId: number): Promise<{ name: string; url: string }> {
    if (!file || !file.filepath) {
        throw new Error('Invalid file provided to ScreenshotProcessor.');
    }
    
    const processedImage = await this.processUploadedFile(file);
    const savedRecord = await this.saveScreenshotRecord(processedImage, batchId);
    
    // Clean up the temporary file after successful processing and saving
    try {
        deleteFile(file.filepath);
    } catch (cleanupError) {
        // Log cleanup error but don't fail the operation
        console.error(`Failed to delete temporary file ${file.filepath}:`, cleanupError);
    }

    return savedRecord;
  }

  /**
   * Reads, resizes, and pads the image file.
   * @param file The uploaded file object.
   * @returns Processed image data.
   */
  private async processUploadedFile(file: File): Promise<ProcessedImage> {
    const fileBuffer = fs.readFileSync(file.filepath);
    const startTime = Date.now();
    const originalFilename = file.originalFilename ?? `unnamed_${Date.now()}`;

    // Perform image resizing and padding
    const processed = await resizeAndPadImageBuffer(fileBuffer, originalFilename);
    const processingTime = (Date.now() - startTime) / 1000; // Convert ms to seconds

    return {
      processedBlob: new Blob([processed.buffer], { type: 'image/jpeg' }), // Assuming JPEG output
      filename: processed.filename,
      processingTime,
    };
  }

  /**
   * Uploads the processed image to storage and saves the metadata to the database.
   * @param image Processed image data.
   * @param batchId The batch ID.
   * @returns The name and URL of the uploaded screenshot.
   */
  private async saveScreenshotRecord(
    image: ProcessedImage,
    batchId: number
  ): Promise<{ name: string; url: string }> {
    // Upload to Supabase storage
    const { fileUrl, error: uploadError } = await uploadImageToStorage(
      image.processedBlob,
      batchId,
      image.filename
    );
    if (uploadError) {
        console.error('Supabase storage upload error:', uploadError);
        throw new Error(`Failed to upload ${image.filename} to storage.`);
    }

    // Insert record into Supabase database
    const { error: dbError } = await this.supabaseClient
      .from('screenshot')
      .insert({
        batch_id: batchId,
        screenshot_file_name: image.filename,
        screenshot_file_url: fileUrl,
        screenshot_processing_status: 'pending', // Initial status before extraction
        screenshot_processing_time: image.processingTime ? `${image.processingTime.toFixed(2)} seconds` : null,
      });

    if (dbError) {
      console.error('Supabase screenshot insert error:', dbError);
      // Attempt to delete the uploaded file if DB insert fails to avoid orphans
      try {
          // TODO: Implement deletion from storage if needed
          console.warn(`DB insert failed for ${image.filename}, corresponding storage file might be orphaned: ${fileUrl}`);
      } catch (deleteError) {
          console.error(`Failed to delete orphaned storage file ${fileUrl}:`, deleteError);
      }
      throw new Error(`Failed to save screenshot record for ${image.filename}.`);
    }

    return {
      name: image.filename,
      url: fileUrl,
    };
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/AccuracyValidationService.ts
================
import { ComponentDetectionResult, ElementDetectionItem } from '@/types/DetectionResult';
import { validate_bounding_boxes_base64 } from '@/lib/services/ai/OpenAIService';
import pLimit from 'p-limit';
import { generateAnnotatedImageBuffer } from '@/lib/services/imageServices/BoundingBoxService';
import { createScreenshotTrackingContext } from '@/lib/logger';
import { PromptLogType, VALIDATION_CONCURRENCY } from '@/lib/constants'
import fs from 'fs';
import path from 'path';
import { saveAnnotatedImageDebug } from '@/lib/services/imageServices/BoundingBoxService';
// Import sample data
import { VALIDATED_RESULTS } from '@/lib/services/sample_data';

// Constants for accuracy score thresholds
const ACCURACY_THRESHOLDS = {
  HIGH: 85, // Green boxes
  MEDIUM: 70, // Yellow/orange boxes
  LOW: 50, // Overwrite/redashed + suggested orange box
};

// Box colors for different accuracy levels
const BOX_COLORS = {
  HIGH: 0x00FF00FF, // Green (RGBA)
  MEDIUM: 0xFFA500FF, // Orange (RGBA)
  LOW: 0xFF0000FF, // Red (RGBA)
  SUGGESTED: 0xFFA500FF, // Orange for suggested box (RGBA)
};


/**
 * AccuracyValidationService
 * 
 * This service validates the accuracy of detected UI elements by:
 * 1. Processing components in parallel with controlled concurrency
 * 2. For each component, validating the accuracy of bounding boxes using OpenAI
 * 3. Re-rendering annotated images with different colors based on accuracy
 * 4. Adding accuracy scores and suggested coordinates to each element
 */
export class AccuracyValidationService {
  /**
   * Validates the accuracy of detected UI elements and updates their metadata
   * 
   * @param batchId - The ID of the batch being processed
   * @param components - Array of ComponentDetectionResult to validate
   * @returns The components array with updated elements and re-annotated images
   */
  public static async performAccuracyValidation(
    batchId: number,
    components: ComponentDetectionResult[]
  ): Promise<ComponentDetectionResult[]> {
    console.log(`[Batch ${batchId}] Stage 3: Starting Accuracy Validation for ${components.length} components...`);
    
    // Create a single output directory for all re-annotated images in this batch
    let batchOutputDir: string | null = null;
    // if (process.env.SAVE_DEBUG_FILES === 'true') {
    //   batchOutputDir = `mobbin_validated_batch_${batchId}_${new Date().toISOString().replace(/[:.-]/g,'')}`;
    //   await fs.promises.mkdir(batchOutputDir, { recursive: true });
    //   console.log(`[Batch ${batchId}] Created output directory for all validated images: ${batchOutputDir}`);
    // }
    
    // Create a concurrency limiter
    const validationLimit = pLimit(VALIDATION_CONCURRENCY);
    
    // Process each component in parallel
    const validationPromises = components.map(component => 
      validationLimit(async () => {
        const screenshotId = component.screenshot_id;
        console.log(`[Batch ${batchId}] Stage 3: Validating component ${component.component_name} for screenshot ${screenshotId}...`);
        
        try {
          // MOCK IMPLEMENTATION - Using sample data instead of API call
        //   console.log(`[Batch ${batchId}] Stage 3: Using mock data for component ${component.component_name}`);
          
        //   // Transform VALIDATED_RESULTS to match ElementDetectionItem format
        //   const mockValidationElements = VALIDATED_RESULTS.map(item => ({
        //     label: item.label,
        //     description: item.description,
        //     bounding_box: item.bounding_box,
        //     status: item.status as 'Detected' | 'Not Detected' | 'Error' | 'Overwrite',
        //     accuracy: item.accuracy || 0,
        //     hidden: item.hidden,
        //     explanation: item.explanation,
        //     suggested_coordinates: item.suggested_coordinates
        //   }));
          
        //   const validationData: ElementDetectionItem[] = mockValidationElements;
          
        //   console.log(
        //     `[Batch ${batchId}] Mock accuracy data loaded for component ${component.component_name}:`,
        //     `Found ${mockValidationElements.length} validated elements`
        //   );    
        //   console.log(JSON.stringify(validationData, null, 2));
          
          // Create tracking context for logging
          const context = createScreenshotTrackingContext(batchId, screenshotId);
          
          // Base64 encode the annotated image
          const imageBase64 = component.annotated_image_object.toString('base64');
          
          // Create elements JSON to send to OpenAI
          const elementsJson = JSON.stringify(component.elements);
          
          // Call OpenAI to validate bounding boxes
          const validationResult = await validate_bounding_boxes_base64(
            imageBase64,
            context,
            elementsJson
          );
          
        //   console.log(
        //     `[Batch ${batchId}] Accuracy raw response for component ${component.component_name}:`,
        //     JSON.stringify(validationResult, null, 2)
        //   );
          // Extract validation data with type safety
        //   let validationData: ElementDetectionItem[] | null = null;
          
        //   try {
        //     validationData = validationResult.parsedContent as ValidationData;
        //     if (!validationData) {
        //       throw new Error('Validation data is null');
        //     }
            
        //     // The response can be an array or an object with elements property
        //     const elements = Array.isArray(validationData) 
        //       ? validationData 
        //       : validationData.elements;
              
        //     if (!Array.isArray(elements) || elements.length === 0) {
        //       throw new Error('No validated elements found in response');
        //     }
            
        //     console.log(`[Batch ${batchId}] Stage 3: Received valid response for component ${component.component_name}, found ${elements.length} elements`);
        //   } catch (validationError) {
        //     console.error(`[Batch ${batchId}] Stage 3: Invalid validation data format for component ${component.component_name}:`, validationError);
        //     // Return the original component if validation data is invalid
        //     return component;
        //   }
          
          
          // Update elements with accuracy scores and suggested coordinates
          // This mutates the elements array in-place
          this.updateElementsWithValidation(component.elements, validationResult.parsedContent);

          console.log(`[Batch ${batchId}] Stage 3: Updated elements for component ${component.component_name}:`, JSON.stringify(component.elements, null, 2));
          
          // Re-render the component's image with updated bounding boxes
          // Use original image if available, otherwise fall back to annotated image
        //   const sourceImageBuffer = component.original_image_object
        //   if (!sourceImageBuffer) {
        //     console.error(`[Batch ${batchId}] Stage 3: No image buffer available for component ${component.component_name}`);
        //     return component;
        //   }
        //   const updatedImageBuffer = await this.regenerateAnnotatedImage(
        //     sourceImageBuffer,
        //     component.elements
        //   );
          
        //   // Update the component with the new image buffer
        //   if (updatedImageBuffer) {
        //     component.annotated_image_object = updatedImageBuffer;

        //     // Save the re-annotated image to the batch output directory
        //     if (batchOutputDir) {
        //       try {
        //         const normalizedName = component.component_name.replace(/\s+/g,'_').toLowerCase();
        //         await saveAnnotatedImageDebug(
        //           updatedImageBuffer,
        //           component.component_name,
        //           batchOutputDir
        //         );
        //         console.log(`[Batch ${batchId}] Saved re-annotated image for component '${component.component_name}' to ${batchOutputDir}`);
        //       } catch (e) {
        //         console.error(`Failed saving re-annotated image for ${component.component_name}:`, e);
        //       }
        //     }
        //   }
          console.log(`[Batch ${batchId}] Stage 3: Completed validation for component ${component.component_name}`);
          
          return component;
        } catch (error) {
          console.error(`[Batch ${batchId}] Stage 3: Error validating component ${component.component_name}:`, error);
          // Return the original component if validation fails
          return component;
        }
      })
    );
    
    // Wait for all components to be validated
    const validatedComponents = await Promise.all(validationPromises);
    
    console.log(`[Batch ${batchId}] Stage 3: Completed Accuracy Validation for all components`);
    
    return validatedComponents;
  }
  
  /**
   * Updates elements with accuracy scores and suggested coordinates
   * 
   * @param elements - Array of elements to update
   * @param validationData - Validation data from OpenAI
   */
  private static updateElementsWithValidation(
    elements: ElementDetectionItem[],
    validationData: any
  ): void {
    // Ensure validation data has the expected format
    if (!validationData) {
      console.warn('Validation data is null or undefined');
      return;
    }
    
    // Handle both array and object with elements property formats
    let validatedElements = validationData;
    if (!Array.isArray(validationData)) {
      console.warn('Invalid validation data format, elements array not found');
      return;
    }
    
    // Create a map of elements by label for easier lookup
    const elementMap = new Map<string, ElementDetectionItem>();
    elements.forEach(element => {
      elementMap.set(element.label, element);
    });
    
    // Update elements with validation data
    validatedElements.forEach((validatedElement: any) => {
      const element = elementMap.get(validatedElement.label);
      
      if (element) {
        try {
          // Attempt to update element properties with validated data
          element.status = validatedElement.status? validatedElement.status : 'Error';
          element.accuracy_score = validatedElement.accuracy? validatedElement.accuracy : 0;
          element.hidden = validatedElement.hidden? validatedElement.hidden : false;
          element.explanation = validatedElement.explanation? validatedElement.explanation : '';
          element.suggested_coordinates = validatedElement.suggested_coordinates? validatedElement.suggested_coordinates : undefined;
        } catch (error) {
          console.error(`Failed to update element '${element.label}' with validation data:`, error);
        }
      }
    });
  }
  
  /**
   * Regenerates the annotated image with colored bounding boxes based on accuracy
   * 
   * @param originalImageBuffer - The original image buffer
   * @param elements - Array of elements with accuracy scores
   * @returns New image buffer with colored bounding boxes
   */
  private static async regenerateAnnotatedImage(
    originalImageBuffer: Buffer,
    elements: ElementDetectionItem[]
  ): Promise<Buffer | null> {
    // Process each element to determine its color based on accuracy
    const coloredElements = elements.map(element => {
      const accuracy = element.accuracy_score || 0;
      let color = BOX_COLORS.HIGH;
      
      if (accuracy < ACCURACY_THRESHOLDS.LOW) {
        color = BOX_COLORS.LOW;
      } else if (accuracy < ACCURACY_THRESHOLDS.MEDIUM) {
        color = BOX_COLORS.MEDIUM;
      }
      
      // Add properties that will be used by enhanced BoundingBoxService
      const enhancedElement = {
        ...element,
        boxColor: color,
        dashed: accuracy < ACCURACY_THRESHOLDS.LOW && (!element.suggested_coordinates && element.status === 'Overwrite'),
        masked: element.status === 'Overwrite' || element.hidden === true
      };
      
      return enhancedElement;
    });
    
    // Generate a new annotated image with colored boxes
    try {
      return await generateAnnotatedImageBuffer(
        originalImageBuffer,
        coloredElements,
        undefined, // Use default color, our elements have custom boxColor property
        PromptLogType.ACCURACY_VALIDATION // Category name for logging
      );
    } catch (error) {
      console.error('Error generating annotated image buffer:', error);
      return null;
    }
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/batchProcessingService.ts
================
import { supabase } from '@/lib/supabase'; 
import { SupabaseClient } from '@supabase/supabase-js';
import { generateSignedUrls, getScreenshotPath, getSignedUrls } from '@/lib/supabaseUtils';
import { fetchScreenshotBuffers } from '@/lib/services/imageServices/imageFetchingService';
import { BatchProcessingScreenshot as Screenshot } from '@/types/BatchProcessingScreenshot';
import type { ComponentDetectionResult } from '@/types/DetectionResult'; 
import pLimit from 'p-limit';
import { AIExtractionService, Stage1Result } from '@/lib/services/ParallelExtractionService';
import { ParallelMoondreamDetectionService } from '@/lib/services/ParallelAnnotationService';
import { AccuracyValidationService } from '@/lib/services/AccuracyValidationService';
import { MetadataExtractionService } from '@/lib/services/MetadataExtractionService';
import { ResultPersistenceService } from '@/lib/services/ResultPersistenceService';
import { EXTRACTION_CONCURRENCY, MOONDREAM_CONCURRENCY, ProcessStatus } from '@/lib/constants';
import fs from 'fs';

// --- Constants ---
// const EXTRACTION_CONCURRENCY = 5; // Concurrency limit for OpenAI/Claude calls
// const MOONDREAM_CONCURRENCY = 5; // Limit concurrency for Moondream processing per batch

export class BatchProcessingService {
  private supabaseClient: SupabaseClient;
  private resultPersistenceService: ResultPersistenceService;

  // Update constructor to accept StorageService
  constructor(
    supabaseClient: SupabaseClient = supabase,
    resultPersistenceService?: ResultPersistenceService
  ) {
    this.supabaseClient = supabaseClient;
    this.resultPersistenceService = resultPersistenceService || new ResultPersistenceService(supabaseClient);
  }

  /**
   * Starts the processing pipeline for a given batch.
   * Changes status, fetches screenshots, gets signed URLs, processes each, and updates status.
   * @param batchId The ID of the batch to process.
   */
  public async start(batchId: number): Promise<void> {
    // console.log(`[Batch ${batchId}] Starting processing...`);
    // const initialStatus = 'processing'; // More generic starting status
    // await this.updateBatchStatus(batchId, initialStatus);

    try {
      // --- Setup 0: Load Screenshots, URLs, Buffers ---
      const screenshotsToProcess = await this.loadAndPrepareScreenshots(batchId);
      if (!screenshotsToProcess || screenshotsToProcess.length === 0) {
        return; // Early return handled in the helper function with appropriate status updates
      }

      // --- Stage 1: Parallel AI Component/Element/Anchor Extraction ---
      await this.updateBatchStatus(batchId, ProcessStatus.EXTRACTING);
      console.log(`[Batch ${batchId}] Begin Parallel Extraction on ${screenshotsToProcess.length} screenshots`);
      
      // Use the external AIExtractionService
      const stage1Results = await AIExtractionService.performAIExtraction(batchId, screenshotsToProcess);
      // Filter out screenshots that failed Stage 1 before proceeding to Stage 2
      const successfulScreenshotIds = new Set(
          Array.from(stage1Results.entries())
              .filter(([_, result]) => !result.error) // Keep only entries without an error
              .map(([id, _]) => id) // Get the screenshot IDs
      );

      const screenshotsForMoondream = screenshotsToProcess.filter(s =>
          successfulScreenshotIds.has(s.screenshot_id)
      );

      if (screenshotsForMoondream.length === 0) {
          console.warn(`[Batch ${batchId}] No screenshots successfully completed Stage 1. Cannot proceed to Moondream.`);
          // Consider the final status - potentially 'failed' or a specific 'extraction_failed' status
          await this.updateBatchStatus(batchId, ProcessStatus.FAILED);
          return;
      }
      console.log(`[Batch ${batchId}] ${screenshotsForMoondream.length} screenshots proceeding to Stage 2 (Moondream).`);

      // --- Stage 2: Parallel Moondream Detection ---
      await this.updateBatchStatus(batchId, ProcessStatus.ANNOTATING); 
      // console.log(`[Batch ${batchId}] Starting Moondream detection for ${screenshotsForMoondream.length} screenshots with concurrency ${MOONDREAM_CONCURRENCY}...`);
      
      // Use the external ParallelMoondreamDetectionService
      const allDetectionResults = await ParallelMoondreamDetectionService.performMoondreamDetection(
        batchId, 
        screenshotsForMoondream, 
        stage1Results
      );
      
      // --- Stage 3: Accuracy Validation ---
      await this.updateBatchStatus(batchId, ProcessStatus.VALIDATING);
      console.log(`[Batch ${batchId}] Stage 3: Starting Accuracy Validation...`);
      
      // Use the AccuracyValidationService to validate bounding boxes
      const validatedResults = await AccuracyValidationService.performAccuracyValidation(
        batchId,
        allDetectionResults
      );
      console.log(`[Batch ${batchId}] Stage 3: Accuracy Validation complete.`);

      const replacer = (key: string, value: any) => {
        if ((key === 'annotated_image_object' || key === 'original_image_object') && value && value.type === 'Buffer') {
          // Check for the structure { type: 'Buffer', data: [...] } which is how Buffers might appear after certain operations
          // Or check if it's an actual Buffer instance
           return `[Buffer data omitted: ${value.data ? value.data.length : 'N/A'} bytes]`;
        } else if (Buffer.isBuffer(value)) {
           // Catch actual Buffer instances if the above check doesn't apply
           return `[Buffer data omitted: ${value.length} bytes]`;
        }
        return value; // Keep other values as they are
      };

      fs.writeFileSync(`batch_${batchId}_validation_results.json`, JSON.stringify(validatedResults, replacer, 2));
      console.log(`[Batch ${batchId}] Stage 3: Accuracy Validation complete. ${JSON.stringify(validatedResults, replacer, 2)}`);
     
      
      // --- Stage 4: Metadata Extraction ---
      // await this.updateBatchStatus(batchId, ProcessStatus.EXTRACTING);
      console.log(`[Batch ${batchId}] Stage 4: Starting Metadata Extraction...`);
      
      // Use the MetadataExtractionService to extract metadata from the validated results
      const enrichedResults = await MetadataExtractionService.performMetadataExtraction(
        batchId,
        validatedResults
      );
      console.log(`[Batch ${batchId}] Stage 4: Metadata Extraction complete.`);

      // Write the full results to a file for debugging
      fs.writeFileSync(`batch_${batchId}_final_results.json`, JSON.stringify(enrichedResults, replacer, 2));

      // --- Stage 5: Persistence to Database ---
      await this.updateBatchStatus(batchId, ProcessStatus.SAVING);
      console.log(`[Batch ${batchId}] Stage 5: Persisting results to database...`);
      
      // Use the ResultPersistenceService to save results to the database
      await this.resultPersistenceService.persistResults(batchId, enrichedResults);
      console.log(`[Batch ${batchId}] Stage 5: Database persistence complete.`);

      // --- Finalize ---
      await this.updateBatchStatus(batchId, ProcessStatus.DONE); // Update status to 'done' after successful processing
      console.log(`[Batch ${batchId}] Processing complete. Status set to done.`);

    } catch (error) {
      // Catch errors from setup phase or unhandled exceptions in stages
      await this.handleProcessingError(batchId, error);
    }
  }

  /**
   * Loads screenshots, processes signed URLs, and fetches screenshot buffers
   * @param batchId The ID of the batch to process
   * @returns Array of screenshots ready for processing with buffers and signed URLs
   */
  private async loadAndPrepareScreenshots(batchId: number): Promise<Screenshot[]> {
    // Load screenshots from database
    const screenshots = await this.loadScreenshots(batchId);
    if (screenshots.length === 0) {
      console.log(`[Batch ${batchId}] No screenshots found. Setting status to done.`);
      await this.updateBatchStatus(batchId, ProcessStatus.DONE);
      return [];
    }
    console.log(`[Batch ${batchId}] Found ${screenshots.length} screenshots.`);

    // Process signed URLs
    await this.processSignedUrls(batchId, screenshots);
    // console.log(`[Batch ${batchId}] Processed signed URLs.`);

    // Fetch screenshot buffers
    await this.fetchScreenshotBuffers(screenshots);
    // console.log(`[Batch ${batchId}] Fetched screenshot buffers.`);

    // Filter screenshots that have both buffer and signed URL for processing
    const screenshotsToProcess = screenshots.filter(
      s => s.screenshot_image_buffer && s.screenshot_signed_url
    );

    if (screenshotsToProcess.length === 0) {
      console.warn(`[Batch ${batchId}] No screenshots with image buffers and signed URLs found after fetching. Cannot proceed.`);
      // Decide status: 'failed' if buffers were expected, 'done' if URLs weren't generated?
      await this.updateBatchStatus(batchId, ProcessStatus.FAILED);
      return [];
    }
    // console.log(`[Batch ${batchId}] ${screenshotsToProcess.length} screenshots eligible for processing.`);
    
    return screenshotsToProcess;
  }

  private async loadScreenshots(batchId: number): Promise<Screenshot[]> {
    const screenshots = await this.getBatchScreenshots(batchId);
    return screenshots;
  }

  private async processSignedUrls(batchId: number, screenshots: Screenshot[]): Promise<void> {
    // 1. Derive bucket paths
    const filePaths = screenshots
      .map(s => getScreenshotPath(s.screenshot_file_url))
      .filter((p): p is string => p !== null); // Type guard to filter nulls and ensure string[]

    if (filePaths.length !== screenshots.length) {
      console.warn(
        `[Batch ${batchId}] ${screenshots.length - filePaths.length} invalid screenshot file URLs found. Associated screenshots skipped for URL generation.`
      );
      // Optionally filter the screenshots array itself here if needed later
    }
    if (filePaths.length === 0) {
      console.log(`[Batch ${batchId}] No valid file paths found. Skipping signed URL fetch.`);
      // Update screenshots array to empty or mark them as missing URL?
      screenshots.forEach(s => {
          s.screenshot_signed_url = undefined;
          s.screenshot_bucket_path = undefined;
      });
      return;
    }

    // 2. Fetch signed URLs
    let signedUrls = new Map<string, string>();
    try {
      signedUrls = await getSignedUrls(this.supabaseClient, filePaths);
    } catch (urlError) {
      console.error(`[Batch ${batchId}] Failed to get signed URLs:`, urlError);
      // Consider how to handle this - fail the batch or proceed without URLs?
      // For now, proceed but log the issue. Screenshots without URLs will be filtered out later.
    }

    // 3. Attach to screenshots
    let attachedCount = 0;
    screenshots.forEach(s => {
      const path = getScreenshotPath(s.screenshot_file_url);
      if (path && signedUrls.has(path)) {
        s.screenshot_signed_url = signedUrls.get(path)!;
        s.screenshot_bucket_path = path;
        attachedCount++;
      } else {
        // Ensure screenshots that didn't get a URL (due to invalid path or fetch error) have undefined values
        s.screenshot_signed_url = undefined;
        s.screenshot_bucket_path = undefined;
      }
    });
    // console.log(`[Batch ${batchId}] Attached signed URLs to ${attachedCount} out of ${screenshots.length} initial screenshots (${filePaths.length} valid paths attempted).`);
  }


  private async handleProcessingError(batchId: number, error: unknown): Promise<void> {
    console.error(`[Batch ${batchId}] Critical error during batch processing:`, error);
    try {
      await this.updateBatchStatus(batchId, ProcessStatus.FAILED);
      console.error(`[Batch ${batchId}] Status set to failed.`);
    } catch (statusError) {
      console.error(
        `[Batch ${batchId}] Failed to update status to failed after critical error:`,
        statusError
      );
    }
  }


  /**
   * Updates the status of a batch in the database.
   * @param batchId The ID of the batch.
   * @param status The new status string.
   */
  private async updateBatchStatus(batchId: number, status: string): Promise<void> {
    const { error } = await this.supabaseClient
      .from('batch')
      .update({ batch_status: status, updated_at: new Date().toISOString() }) // Add updated_at
      .eq('batch_id', batchId);

    if (error) {
      console.error(`[Batch ${batchId}] Supabase batch status update error to '${status}':`, error);
      // Avoid throwing here to allow subsequent error handling like handleProcessingError to proceed
      // throw new Error(`Failed to update batch ${batchId} status to ${status}.`);
    } else {
      console.log(`[Batch ${batchId}] Status updated to '${status}'.`);
    }
  }



  /**
   * Fetches all screenshot records for a given batch ID.
   * @param batchId The ID of the batch.
   * @returns An array of screenshot records.
   */
   // Update return type to use the Screenshot interface
  private async getBatchScreenshots(batchId: number): Promise<Screenshot[]> {
    const { data, error } = await this.supabaseClient
      .from('screenshot')
      .select('*')
      .eq('batch_id', batchId);

    if (error) {
      console.error(`[Batch ${batchId}] Supabase screenshot fetch error:`, error);
      // Rethrow or return empty? Returning empty allows process to potentially continue if error is transient, but might hide issues.
      // Let's return empty and log, the calling function handles the empty case.
      return [];
    }

    // Explicitly cast data to Screenshot[] after checking for null/undefined
    return (data as Screenshot[] | null) || [];
  }


  /**
   * Fetches image data as ArrayBuffer for each screenshot with a valid signed URL
   * @param screenshots Array of screenshot objects with screenshot_signed_url property
   * @returns The same array of screenshots with screenshot_image_buffer property populated
   */
  public async fetchScreenshotBuffers(
    screenshots: Screenshot[]
  ): Promise<Screenshot[]> {
    // Assuming fetchScreenshotBuffers handles its own logging and concurrency
    return fetchScreenshotBuffers(screenshots);
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/DatabaseService.ts
================
import { createClient, SupabaseClient } from '@supabase/supabase-js';
import { SUPABASE_URL, SUPABASE_KEY } from '../../config'; // Adjust path as needed

export class DatabaseService {
  private static instance: DatabaseService;
  private client: SupabaseClient;

  private constructor() {
    if (!SUPABASE_URL || !SUPABASE_KEY) {
      throw new Error('Supabase URL or Key is missing in config.ts');
    }
    // Use Database type if you have generated types, otherwise use 'any'
    this.client = createClient<any>(SUPABASE_URL, SUPABASE_KEY);
  }

  public static getInstance(): DatabaseService {
    if (!DatabaseService.instance) {
      DatabaseService.instance = new DatabaseService();
    }
    return DatabaseService.instance;
  }

  public getClient(): SupabaseClient {
    return this.client;
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/MetadataExtractionService.ts
================
import { ComponentDetectionResult, ElementDetectionItem } from '@/types/DetectionResult';
import { extract_component_metadata } from '@/lib/services/ai/OpenAIService';
import pLimit from 'p-limit';
import { createScreenshotTrackingContext } from '@/lib/logger';
import { PromptLogType, EXTRACTION_CONCURRENCY } from '@/lib/constants';

// Constant for concurrency control
const METADATA_EXTRACTION_CONCURRENCY = EXTRACTION_CONCURRENCY;

/**
 * MetadataExtractionService
 * 
 * This service extracts metadata from validated components by:
 * 1. Processing components in parallel with controlled concurrency
 * 2. For each component, converting image buffer to base64 and preparing structured input
 * 3. Calling OpenAI to extract metadata based on component and its elements
 * 4. Updating component and element metadata fields with the extracted information
 */
export class MetadataExtractionService {
  /**
   * Performs metadata extraction for all validated components
   * 
   * @param batchId - The ID of the batch being processed
   * @param components - Array of ComponentDetectionResult after validation
   * @returns The components array with updated metadata fields
   */
  public static async performMetadataExtraction(
    batchId: number,
    components: ComponentDetectionResult[]
  ): Promise<ComponentDetectionResult[]> {
    console.log(`[Batch ${batchId}] Stage 4: Starting Metadata Extraction for ${components.length} components...`);
    
    // Create a concurrency limiter
    const extractionLimit = pLimit(METADATA_EXTRACTION_CONCURRENCY);
    
    // Process each component in parallel
    const extractionPromises = components.map(component => 
      extractionLimit(async () => {
        const screenshotId = component.screenshot_id;
        console.log(`[Batch ${batchId}] Stage 4: Extracting metadata for component ${component.component_name} for screenshot ${screenshotId}...`);
        
        try {
          // Create tracking context for logging
          const context = createScreenshotTrackingContext(batchId, screenshotId);
          
          // 1. Convert image to base64 - prefer original image but fall back to annotated
          const imageBuffer = component.original_image_object || component.annotated_image_object;
          const imageBase64 = imageBuffer.toString('base64');
          
          // 2. Prepare structured input for OpenAI
          const inputPayload = {
            component_name: component.component_name,
            elements: component.elements.map(element => ({
              label: element.label,
              description: element.description
            }))
          };
          
          // 3. Call OpenAI to extract metadata
          const metadataResult = await extract_component_metadata(
            imageBase64,
            JSON.stringify(inputPayload),
            context
          );
          
          // 4. Update component and element metadata
          this.updateComponentWithMetadata(component, metadataResult.parsedContent);
          
          console.log(`[Batch ${batchId}] Stage 4: Completed metadata extraction for component ${component.component_name}`);
          
          return component;
        } catch (error) {
          console.error(`[Batch ${batchId}] Stage 4: Error extracting metadata for component ${component.component_name}:`, error);
          // Return the original component if extraction fails
          return component;
        }
      })
    );
    
    // Wait for all components to have metadata extracted
    const enrichedComponents = await Promise.all(extractionPromises);
    
    console.log(`[Batch ${batchId}] Stage 4: Completed Metadata Extraction for all components`);
    
    return enrichedComponents;
  }
  
  /**
   * Updates component and element metadata with extracted information
   * 
   * @param component - Component to update
   * @param metadataData - Extracted metadata from OpenAI
   */
  private static updateComponentWithMetadata(
    component: ComponentDetectionResult,
    metadataData: any
  ): void {
    // Ensure metadata data has the expected format
    if (!metadataData) {
      console.warn('Metadata data is null or undefined');
      return;
    }
    
    // Get the component metadata object using component name as key
    const componentMetadata = metadataData[component.component_name];
    if (!componentMetadata) {
      console.warn(`No metadata found for component ${component.component_name}`);
      return;
    }
    
    try {
      // Store all component-level metadata (excluding element-specific metadata)
      const { componentDescription, patternName, facetTags, states, interaction, userFlowImpact } = componentMetadata;
    //   const { componentDescription } = componentMetadata;
      // Create component metadata string
      component.component_metadata_extraction = JSON.stringify({
        patternName,
        facetTags,
        states,
        interaction,
        userFlowImpact
      });

      component.component_ai_description = componentDescription;
      
      // Create a map of elements by label for easier lookup
      const elementMap = new Map<string, ElementDetectionItem>();
      component.elements.forEach(element => {
        elementMap.set(element.label, element);
      });
      
      // Update elements with their specific metadata
      Object.keys(componentMetadata).forEach(key => {
        // Skip component-level properties
        if (['patternName', 'facetTags', 'states', 'interaction', 'userFlowImpact'].includes(key)) {
          return;
        }
        
        // Check if this key matches an element label
        const element = elementMap.get(key);
        if (element) {
          // Store element-specific metadata
          const elementMetadata = componentMetadata[key];
          element.element_metadata_extraction = JSON.stringify(elementMetadata);
        }
      });
    } catch (error) {
      console.error(`Failed to update component ${component.component_name} with metadata:`, error);
    }
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/ParallelAnnotationService.ts
================
import { BatchProcessingScreenshot as Screenshot } from '@/types/BatchProcessingScreenshot';
import type { ComponentDetectionResult } from '@/types/DetectionResult';
import { Stage1Result } from '@/lib/services/ParallelExtractionService';
// Import from the correct module
import { processAndSaveByCategory } from '@/lib/services/ai/MoondreamDetectionService';
import pLimit from 'p-limit';
import { MOONDREAM_CONCURRENCY } from '@/lib/constants';
import { createScreenshotTrackingContext } from '@/lib/logger';

 
/**
 * ParallelMoondreamDetectionService
 * 
 * This service handles the parallel processing of screenshots through Moondream's
 * language vision models, using anchor labels from previous AI processing steps
 * to guide object detection within UI screenshots.
 * 
 * DESIGN DECISIONS:
 * 1. Controlled Parallelism: We implement managed concurrency to balance throughput 
 *    against system resource constraints. This approach prevents overwhelming the 
 *    local model while still achieving significant performance gains.
 * 
 * 2. Error Isolation: Each screenshot is processed independently in its own promise,
 *    allowing failures to be contained without affecting the entire batch.
 * 
 * 3. Result Aggregation: All detection results are collected into a flat array,
 *    making it easier to persist results as a single operation rather than per-screenshot.
 * 
 * 4. Contextual Processing: By utilizing the anchor labels from Stage 1, we provide
 *    semantic context to the vision model, improving its accuracy in identifying
 *    specific UI components.
 * 
 * 5. Comprehensive Error Handling: We use Promise.allSettled to ensure the pipeline 
 *    continues even when individual screenshots fail processing.
 */
export class ParallelMoondreamDetectionService {
  /**
   * Performs parallel Moondream detection on screenshots using anchors from Stage 1
   * 
   * TECHNICAL DETAILS:
   * - Implements a processing pool with p-limit to manage resource consumption
   * - Each screenshot is processed independently within the concurrency pool
   * - Results are flat-mapped into a single array for efficient bulk persistence
   * - Empty arrays are returned for failed screenshots to maintain processing flow
   * - Promise.allSettled ensures batch resilience against individual failures
   * 
   * @param batchId The ID of the batch being processed (for logging)
   * @param screenshots Array of screenshots that passed Stage 1
   * @param stage1Results Map of Stage 1 results by screenshot ID
   * @returns Array of ComponentDetectionResult objects
   */
  public static async performMoondreamDetection(
    batchId: number, 
    screenshots: Screenshot[], 
    stage1Results: Map<number, Stage1Result>
  ): Promise<ComponentDetectionResult[]> {

    // Create concurrency limiter for Moondream to prevent resource exhaustion
    // This is especially important as Moondream is compute-intensive
    const moondreamLimit = pLimit(MOONDREAM_CONCURRENCY);
    const allDetectionResults: ComponentDetectionResult[] = []; // Collect all results in flat array
    
    console.log(`[Batch ${batchId}] Stage 2: Starting Bounding Box Detection for ${screenshots.length} screenshots... Concurrency: ${MOONDREAM_CONCURRENCY}`);

    // Initialize parallel detection tasks with controlled concurrency
    const detectionPromises = screenshots.map(screenshot =>
      moondreamLimit(async () => {
        // const context = createScreenshotTrackingContext(batchId, screenshot.screenshot_id);
        const screenshotId = screenshot.screenshot_id;
        // We know buffer exists because it passed the initial filter
        const buffer = screenshot.screenshot_image_buffer!;
        // Get screenshot signed URL for audit/debugging
        const screenshotUrl = screenshot.screenshot_signed_url || '';  // Use empty string as fallback if undefined
        // We know stage 1 results exist because we filtered for successful ones
        const stage1Data = stage1Results.get(screenshotId)!;
        const anchorLabels = stage1Data.anchorLabels;

        console.log(`[Batch ${batchId}] Stage 2: Moondream labelling screenshot ${screenshotId}...`);

        try {
          // Process screenshot with Moondream using component anchors
          // These anchors provide semantic context to improve detection accuracy
          const results: ComponentDetectionResult[] = await processAndSaveByCategory(
            screenshotId,
            buffer,
            anchorLabels, // Use the labels derived specific to this screenshot
            batchId,
            screenshotUrl // Pass screenshot URL for storage in component results
          );
          console.log(`[Batch ${batchId}] Stage 2: Finished Moondream labelling for screenshot ${screenshotId}. Results count: ${results.length}`);
          return results; // Return results for this screenshot
        } catch (error) {
          // Log error but continue processing other screenshots
          console.error(`[Batch ${batchId}] Stage 2: Error labelling screenshot ${screenshotId} with Moondream:`, error);
          return []; // Return empty array on error for this screenshot
        }
      })
    );

    // Wait for all detection tasks to complete or fail
    // Using Promise.allSettled ensures we collect all successful results
    // even if some screenshots fail processing
    const settledMoondreamResults = await Promise.allSettled(detectionPromises);
    
    // Aggregate results from all successfully processed screenshots
    settledMoondreamResults.forEach(result => {
      if (result.status === 'fulfilled' && Array.isArray(result.value)) {
        // Spread array results into the flat collection
        allDetectionResults.push(...result.value);
      } else if (result.status === 'rejected') {
        // Error already logged inside the promise, add batch-level context
        console.error(`[Batch ${batchId}] Stage 2: A Moondream detection task failed:`, result.reason);
      }
    });
    
    return allDetectionResults;
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/ParallelExtractionService.ts
================
import { BatchProcessingScreenshot as Screenshot } from '@/types/BatchProcessingScreenshot';
import { extract_component_from_image } from '@/lib/services/ai/OpenAIService';
import { extract_element_from_image, anchor_elements_from_image } from '@/lib/services/ai/ClaudeAIService';
import pLimit from 'p-limit';
import { EXTRACTION_CONCURRENCY } from '@/lib/constants';
import { createScreenshotTrackingContext, PromptTrackingContext } from '@/lib/logger';

// --- Types for intermediate results ---
export interface Stage1Result {
    componentSummaries: string[];
    elementResultRawText: string;
    anchorLabels: Record<string, string>;
    error?: any; // error tracking per screenshot
}

/**
 * AIExtractionService
 * 
 * This service handles the parallel extraction of components, elements, and anchors from screenshots
 * using multiple AI systems (OpenAI and Claude).
 * 
 * DESIGN DECISIONS:
 * 1. Parallel Processing: We use controlled parallelism to maximize throughput without overwhelming 
 *    external API services. This balances speed with reliability and cost management.
 * 
 * 2. Fault Tolerance: Each screenshot is processed independently, and errors are captured per 
 *    screenshot rather than failing the entire batch. This allows partial batch success.
 * 
 * 3. Progressive Enhancement: The extraction pipeline builds incrementally, with each step using 
 *    the results of the previous step:
 *    - Component extraction identifies high-level UI patterns
 *    - Element extraction uses components to find specific elements
 *    - Anchor labeling uses element data to establish reference points
 * 
 * 4. Data Integrity: Results include error tracking to allow downstream processes to filter out
 *    failed operations and proceed with successful ones.
 */
export class AIExtractionService {
  /**
   * Extracts components, elements, and anchors from screenshots in parallel
   * 
   * TECHNICAL DETAILS:
   * - Implements controlled parallelism with p-limit to manage API rate limits
   * - Each screenshot processing runs independently with Promise.allSettled for fault isolation
   * - Maps screenshot IDs to their extraction results for later processing stages
   * - Progressive extraction: Components → Elements → Anchors
   * - Comprehensive error capture to prevent batch failure from individual items
   * 
   * @param batchId The ID of the batch being processed (for logging)
   * @param screenshots Array of screenshots with buffers and signed URLs
   * @returns Map of screenshot IDs to Stage1Result objects
   */
  public static async performAIExtraction(batchId: number, screenshots: Screenshot[]): Promise<Map<number, Stage1Result>> {
    // Create a results map to store hardcoded test values
    // const stage1Results = new Map<number, Stage1Result>();

    // // TEMPORARY: Using hardcoded test values for batch 22
    // console.log(`[Batch ${batchId}] Stage 1: Using hardcoded test values for all screenshots`);

    // // Process each screenshot by assigning the same hardcoded values
    // for (const screenshot of screenshots) {
    //   const screenshotId = screenshot.screenshot_id;
    //   console.log(`[Batch ${batchId}] Stage 1: Processing screenshot ${screenshotId} with hardcoded test values...`);

//       // Hardcoded component data from batch 22
//       const hardcodedComponentText = `[
//   {
//     "component_name": "Profile Avatar",
//     "description": "Circular user photo icon located at the top-left, easily identifiable as a profile entry point.",
//     "impact_on_user_flow": "Allows quick access to user profile or account settings.",
//     "cta_type": "Secondary",
//     "is_reused_in_other_screens": true,
//     "likely_interaction_type": ["tap"],
//     "flow_position": "Global Navigation"
//   },
//   {
//     "component_name": "Room Selector Tabs",
//     "description": "Horizontal tab group with labeled chip-like selectors: 'Living room', 'Kitchen', and 'Bedroom'; at least one additional partially visible tab to the right.",
//     "impact_on_user_flow": "Lets users filter devices and scenes by location within the home.",
//     "cta_type": "Primary",
//     "is_reused_in_other_screens": true,
//     "likely_interaction_type": ["tap", "swipe"],
//     "flow_position": "Device Management - Room Selection"
//   },
//   {
//     "component_name": "Scenes Selector",
//     "description": "Rounded buttons with icons and labels for quick scene selection: 'Awakening', 'Night', 'Calm', and 'Energetic'; shows enabled (highlighted) and non-enabled states.",
//     "impact_on_user_flow": "Allows users to quickly activate preset home environment configurations.",
//     "cta_type": "Primary",
//     "is_reused_in_other_screens": true,
//     "likely_interaction_type": ["tap"],
//     "flow_position": "Device Management - Scene Selection"
//   },
//   {
//     "component_name": "Device Filter Dropdown",
//     "description": "Right-aligned dropdown control labeled 'All devices', filtering the displayed IoT devices.",
//     "impact_on_user_flow": "Lets users customize which device cards are shown, improving navigation in homes with multiple devices.",
//     "cta_type": "Secondary",
//     "is_reused_in_other_screens": true,
//     "likely_interaction_type": ["tap"],
//     "flow_position": "Device Management - Device Filtering"
//   },
//   {
//     "component_name": "Air Conditioner Card",
//     "description": "Card displaying device icon, name 'Air Conditioner', model, current temperature (20°C), and a power toggle button.",
//     "impact_on_user_flow": "Enables device monitoring and remote power control directly from the dashboard.",
//     "cta_type": "Primary",
//     "is_reused_in_other_screens": true,
//     "likely_interaction_type": ["tap"],
//     "flow_position": "Device Management - Device Control"
//   },
//   {
//     "component_name": "Smart Light Card",
//     "description": "Card with bulb icon, device name 'Smart Light', model, battery percentage (92%), and a power toggle button.",
//     "impact_on_user_flow": "Gives users access to light controls and status, supporting quick adjustments.",
//     "cta_type": "Primary",
//     "is_reused_in_other_screens": true,
//     "likely_interaction_type": ["tap"],
//     "flow_position": "Device Management - Device Control"
//   },
//   {
//     "component_name": "Smart TV Card",
//     "description": "Wider card showing device icon, name 'Smart TV', model info, volume controls as a slider, and a volume percentage indicator (55%).",
//     "impact_on_user_flow": "Allows users to adjust the TV's volume and access device status from the main screen.",
//     "cta_type": "Primary",
//     "is_reused_in_other_screens": true,
//     "likely_interaction_type": ["tap", "slide"],
//     "flow_position": "Device Management - Device Control"
//   }
// ]`;

//       // Hardcoded element data from batch 22
//       const hardcodedElementText = `{
//   "Profile Avatar > Profile Picture": "A small circular profile image in the top-left corner of the interface, appearing as a profile photo that serves as a user account indicator or access point to profile settings.",
//   "Room Selector Tabs > Living Room Tab": "A pill-shaped white button labeled 'Living room' positioned at the top of the screen, currently selected as indicated by its highlighted appearance, allowing users to filter connected devices by room location.",
//   "Room Selector Tabs > Kitchen Tab": "A pill-shaped light colored button labeled 'Kitchen' positioned in the room selector row, allowing users to switch to kitchen-related smart devices when tapped.",
//   "Room Selector Tabs > Bedroom Tab": "A pill-shaped light colored button labeled 'Bedroom' positioned in the room selector row, allowing users to switch to bedroom-related smart devices when tapped.",
//   "Scenes Selector > Section Title": "Text label 'Scenes' positioned above the scene selection options, indicating the category of preset environment configurations below.",
//   "Scenes Selector > Awakening Scene": "A circular button with a sun icon and 'Awakening' label below it, positioned in the scenes row, designed to activate a morning lighting and device configuration.",
//   "Scenes Selector > Night Scene": "A circular button with a moon icon and 'Night' label below it, positioned in the scenes row, designed to activate an evening lighting and device configuration.",
//   "Scenes Selector > Calm Scene": "A circular button with a water drop icon and 'Calm' label below it, positioned in the scenes row, designed to activate a relaxing lighting and device configuration.",
//   "Scenes Selector > Energetic Scene": "A circular button with a lightning bolt icon and 'Energetic' label below it, positioned at the right end of the scenes row, designed to activate a vibrant lighting and device configuration.",
//   "Device Filter Dropdown > Device Count": "Text displaying '3 devices' positioned under the scenes row on the left side, indicating the total number of connected smart devices currently available.",
//   "Device Filter Dropdown > All Devices Button": "A button labeled 'All devices' with a right arrow icon, positioned opposite the device count, allowing users to access a complete list of connected devices.",
//   "Air Conditioner Card > Card Title": "Bold text 'Air Conditioner' heading the first device card, indicating the device type being controlled.",
//   "Air Conditioner Card > Device Model": "Gray text 'Samsung AR9500T' below the card title, specifying the manufacturer and model of the air conditioner.",
//   "Air Conditioner Card > Temperature Display": "A small circular indicator displaying '20°C' with a thermometer icon, showing the current temperature setting of the air conditioner.",
//   "Air Conditioner Card > Power Button": "A circular button with a power icon, positioned on the right side of the card, allowing users to turn the air conditioner on or off.",
//   "Smart Light Card > Card Title": "Bold text 'Smart Light' heading the second device card, indicating the device type being controlled.",
//   "Smart Light Card > Device Model": "Gray text 'Mi Smart LED Ceiling Light' below the card title, specifying the manufacturer and model of the smart light.",
//   "Smart Light Card > Brightness Display": "A small circular indicator displaying '92%' with a light bulb icon, showing the current brightness level of the smart light.",
//   "Smart Light Card > Power Button": "A circular button with a power icon, positioned on the right side of the card, allowing users to turn the smart light on or off.",
//   "Smart TV Card > Card Title": "Bold text 'Smart TV' heading the third device card at the bottom of the screen, indicating the device type being controlled.",
//   "Smart TV Card > Device Model": "Gray text 'Samsung AR9500T' below the card title, specifying the manufacturer and model of the smart TV.",
//   "Smart TV Card > Volume Slider": "A horizontal slider control in the middle of the TV card, allowing users to adjust the volume level of the smart TV.",
//   "Smart TV Card > Battery Level": "A small indicator displaying '55%' with a battery icon, showing the current battery or power level of the TV remote or related component.",
//   "Smart TV Card > Power Button": "A circular button with a power icon, positioned on the right side of the card, allowing users to turn the smart TV on or off."
// }`;

//       // Hardcoded anchor data from batch 22
//       const hardcodedAnchorLabels = {
//         // "Profile Avatar > Profile Picture": "A small circular profile image in the top-left corner of the interface with a light background",
//         // "Room Selector Tabs > Living Room Tab": "A pill-shaped white button labeled 'Living room' positioned at the top of the screen among room selector options",
//         // "Room Selector Tabs > Kitchen Tab": "A pill-shaped light colored button labeled 'Kitchen' positioned in the center of the room selector row at the top of the screen",
//         // "Room Selector Tabs > Bedroom Tab": "A pill-shaped light colored button labeled 'Bedroom' positioned to the right in the room selector row at the top of the screen",
//         // "Scenes Selector > Section Title": "Text label 'Scenes' positioned above the row of circular scene selection buttons",
//         // "Scenes Selector > Awakening Scene": "A circular white button with a sun icon and 'Awakening' label below it, leftmost in the scenes row",
//         // "Scenes Selector > Night Scene": "A circular light gray button with a crescent moon icon and 'Night' label below it, second from left in the scenes row",
//         // "Scenes Selector > Calm Scene": "A circular light gray button with a water drop icon and 'Calm' label below it, third from left in the scenes row",
//         // "Scenes Selector > Energetic Scene": "A circular light gray button with a lightning bolt icon and 'Energetic' label below it, rightmost in the scenes row",
//         // "Device Filter Dropdown > Device Count": "Text displaying '3 devices' positioned under the scenes row on the left side",
//         // "Device Filter Dropdown > All Devices Button": "A small pill-shaped button labeled 'All devices >' positioned on the right side below the scenes row",
//         "Air Conditioner Card > Card Title": "Bold text 'Air Conditioner' heading the first rectangular device card",
//         "Air Conditioner Card > Device Model": "Gray text 'Samsung AR9500T' below the Air Conditioner title in the first device card",
//         "Air Conditioner Card > Temperature Display": "A small circular indicator displaying '20°C' with a thermometer icon in the Air Conditioner card",
//         "Air Conditioner Card > Power Button": "A circular button with a power icon, positioned on the right side of the Air Conditioner card",
//         "Smart Light Card > Card Title": "Bold text 'Smart Light' heading the second rectangular device card in the middle of the screen",
//         "Smart Light Card > Device Model": "Gray text 'Mi Smart LED Ceiling Light' below the Smart Light title in the second device card",
//         "Smart Light Card > Brightness Display": "A small circular indicator displaying '92%' with a light bulb icon in the Smart Light card",
//         "Smart Light Card > Power Button": "A circular button with a power icon, positioned on the right side of the Smart Light card",
//         "Smart TV Card > Card Title": "Bold text 'Smart TV' heading the third rectangular device card at the bottom of the screen",
//         "Smart TV Card > Device Model": "Gray text 'Samsung AR9500T' below the Smart TV title in the bottom device card",
//         "Smart TV Card > Volume Slider": "A horizontal slider control in the middle of the TV card with a white indicator",
//         "Smart TV Card > Battery Level": "A small indicator displaying '55%' with a battery icon in the upper right corner of the Smart TV card",
//         "Smart TV Card > Power Button": "A circular button with a power icon, positioned on the right side of the Smart TV card at the bottom of the screen"
//       };

//       // Parse the JSON-formatted component text to get component objects
//       const componentObjects = JSON.parse(hardcodedComponentText);
      
//       // Extract component names using the existing helper method
//       const componentSummaries = this.extractComponentSummaries(componentObjects);

//       // Store hardcoded results for this screenshot
//       stage1Results.set(screenshotId, {
//         componentSummaries,
//         elementResultRawText: hardcodedElementText,
//         anchorLabels: hardcodedAnchorLabels,
//       });

//       console.log(`[Batch ${batchId}][Screenshot ${screenshotId}] Successfully processed with hardcoded test values. Found ${componentSummaries.length} Main Components, ${Object.keys(hardcodedAnchorLabels).length} Optimised Labels.`);
//     }

//     console.log(`[Batch ${batchId}] Completed Stage 1 AI extraction for all applicable screenshots using hardcoded test values.`);
    
//     return stage1Results;

    // ORIGINAL IMPLEMENTATION - COMMENTED OUT FOR TESTING
    // Create a concurrency limiter to prevent overwhelming external AI services
    // This is crucial for rate limit management and cost control
    const extractionLimit = pLimit(EXTRACTION_CONCURRENCY);
    const stage1Results = new Map<number, Stage1Result>(); // Map screenshot_id to results

    // Map each screenshot to a promise that processes it within concurrency limits
    const extractionPromises = screenshots.map(screenshot =>
      extractionLimit(async () => {
        const screenshotId = screenshot.screenshot_id;
        const signedUrl = screenshot.screenshot_signed_url!; // We filtered for this previously
        console.log(`[Batch ${batchId}] Stage 1: Processing screenshot ${screenshotId}...`);

        // Create a tracking context for this screenshot
        const context = createScreenshotTrackingContext(batchId, screenshotId);

        try {
          // 1. Extract Components using OpenAI vision capabilities
          // Components represent high-level UI patterns (forms, cards, etc.)
          console.log(`[Batch ${batchId}][Screenshot ${screenshotId}] Step 1.1 : Extracting High-Level Components...`);
          const componentResult = await extract_component_from_image(signedUrl, context);
          const componentSummaries = this.extractComponentSummaries(componentResult.parsedContent || []);
          console.log(`[Batch ${batchId}][Screenshot ${screenshotId}] Step 1.1 Complete. Found ${componentSummaries.length} Main Components.`);

          // 2. Extract Elements based on Components using Claude
          // Elements are specific interactive parts informed by component context
          console.log(`[Batch ${batchId}][Screenshot ${screenshotId}] Step 1.2 : Extracting Detailed Elements...`);
          const elementResult = await extract_element_from_image(signedUrl, componentSummaries.join('\n'), context);
          console.log(`[Batch ${batchId}][Screenshot ${screenshotId}] Step 1.2 Complete. Found ${elementResult.parsedContent.length} Detailed Elements.`);

          // 3. Anchor Elements based on Element Extraction
          // Anchors provide spatial reference points for Moondream to use later
          console.log(`[Batch ${batchId}][Screenshot ${screenshotId}] Step 1.3 : Optimising descriptions for VLM detection`);
          const anchorResult = await anchor_elements_from_image(signedUrl, `${elementResult.rawText}`, context);
          const anchorLabels: Record<string, string> = anchorResult.parsedContent || {};
          console.log(`[Batch ${batchId}][Screenshot ${screenshotId}] Step 1.3 Complete. Optimised ${Object.keys(anchorLabels).length} labels.`);

          if (Object.keys(anchorLabels).length === 0) {
            console.warn(`[Batch ${batchId}][Screenshot ${screenshotId}] No anchor labels generated. Moondream detection might be ineffective.`);
          }

          // Store successful results
          stage1Results.set(screenshotId, {
            componentSummaries,
            elementResultRawText: elementResult.rawText || '',
            anchorLabels,
          });
          console.log(`[Batch ${batchId}][Screenshot ${screenshotId}]Successfully processed screenshot ${screenshotId}. Found ${componentSummaries.length} Main Components, ${elementResult.parsedContent.length} Detailed Elements, ${Object.keys(anchorLabels).length} Optimised Labels.`);

        } catch (error) {
          console.error(`[Batch ${batchId}][Screenshot ${screenshotId}] Step 1.4 : Error processing screenshot ${screenshotId}:`, error);
          // Store error information for reporting and later filtering
          // This resilience allows the process to continue with successfully processed screenshots
          stage1Results.set(screenshotId, {
            componentSummaries: [],
            elementResultRawText: '',
            anchorLabels: {},
            error: error, // Store the error for filtering and diagnosis
          });
        }
      })
    );

    // Wait for all extractions to complete (successfully or with errors)
    // We use Promise.allSettled instead of Promise.all to prevent a single failure from stopping the batch
    await Promise.allSettled(extractionPromises);
    console.log(`[Batch ${batchId}] Completed Stage 1 AI extraction for all applicable screenshots.`);
    console.log(`[Batch ${batchId}] Stage 1: Extraction complete. Results ${JSON.stringify(stage1Results, null, 2)}\n`);

    return stage1Results;
  }


  /**
   * Helper function to extract component summaries from AI extraction results
   * This is to pass to the element extraction step
   * 
   * @param components Array of components from AI extraction
   * @returns Array of component summary strings (just names for now)
   */
  private static extractComponentSummaries(components: any[]): string[] {
    if (!Array.isArray(components)) {
      console.warn("ExtractComponentSummaries: Expected an array of components, received:", typeof components);
      return [];
    }

    return components
      // Ensure component is an object and has the required string properties
      .filter(component =>
          typeof component === 'object' &&
          component !== null &&
          typeof component.component_name === 'string' &&
          typeof component.description === 'string' // Keep description check even if not used in output
      )
      .map(component => component.component_name); // Just using name now
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/ResultPersistenceService.ts
================
import { supabase } from '@/lib/supabase';
import { SupabaseClient } from '@supabase/supabase-js';
import { ComponentDetectionResult } from '@/types/DetectionResult';
import { ProcessStatus } from '@/lib/constants';
import { SUPABASE_BUCKET_NAME } from '@/config';
// Storage bucket name for Supabase
const STORAGE_BUCKET = SUPABASE_BUCKET_NAME || 'v5';

export class ResultPersistenceService {
  private supabaseClient: SupabaseClient;

  constructor(supabaseClient: SupabaseClient = supabase) {
    this.supabaseClient = supabaseClient;
  }

  /**
   * Persists component and element detection results to the database
   * @param batchId The ID of the batch
   * @param enrichedResults Array of component detection results with metadata
   */
  public async persistResults(
    batchId: number,
    enrichedResults: ComponentDetectionResult[]
  ): Promise<void> {
    console.log(`[Batch ${batchId}] Persisting ${enrichedResults.length} component results...`);
    
    // Process each component result
    for (const result of enrichedResults) {
      await this.persistComponentResult(result);
    }
    
    console.log(`[Batch ${batchId}] Successfully persisted all component and element data.`);
  }

  /**
   * Persists a single component and its elements to the database
   * @param result The component detection result with elements
   */
  private async persistComponentResult(
    result: ComponentDetectionResult
  ): Promise<void> {
    try {
      // Get screenshot_id from result
      const { screenshot_id } = result;
      if (!screenshot_id) {
        console.error(`Missing screenshot_id in component result`, result);
        return;
      }

      // 1. Upload annotated image to storage if it exists
      let screenshot_url = null;
      if (result.annotated_image_object) {
        screenshot_url = await this.uploadAnnotatedImage(result);
      }

      // 2. Insert component record
      const componentData = {
        screenshot_id,
        component_name: result.component_name || 'Unnamed Component',
        component_description: result.component_description || null,
        detection_status: result.detection_status || 'success',
        inference_time: result.inference_time || null,
        screenshot_url,
        component_metadata_extraction: result.component_metadata_extraction || null,
        component_ai_description: result.component_ai_description || null
      };

      const { data: component, error: componentError } = await this.supabaseClient
        .from('component')
        .insert(componentData)
        .select('component_id')
        .single();

      if (componentError) {
        console.error(`Failed to insert component record:`, componentError);
        return;
      }

      const component_id = component.component_id;

      // 3. Insert elements
      if (result.elements && result.elements.length > 0) {
        await this.persistElements(component_id, screenshot_id, result.elements);
      }
    } catch (error) {
      console.error(`Error persisting component result:`, error);
    }
  }

  /**
   * Uploads annotated image to storage and returns the public URL
   * @param result The component detection result containing annotated_image_object
   * @returns Public URL of the uploaded image
   */
  private async uploadAnnotatedImage(
    result: ComponentDetectionResult
  ): Promise<string | null> {
    try {
      if (!result.annotated_image_object) return null;

      // Generate a unique path for the annotated image
      const path = `annotated/${result.screenshot_id}/${Date.now()}_component.png`;
      
      // Upload the buffer to storage
      const { data, error } = await this.supabaseClient
        .storage
        .from(STORAGE_BUCKET)
        .upload(path, result.annotated_image_object, {
          contentType: 'image/png',
          upsert: true
        });

      if (error) {
        console.error(`Failed to upload annotated image:`, error);
        return null;
      }

      // Get public URL
      const { data: publicUrl } = this.supabaseClient
        .storage
        .from(STORAGE_BUCKET)
        .getPublicUrl(path);

      return publicUrl.publicUrl;
    } catch (error) {
      console.error(`Error uploading annotated image:`, error);
      return null;
    }
  }

  /**
   * Persists elements to the database
   * @param component_id The ID of the parent component
   * @param screenshot_id The ID of the screenshot
   * @param elements Array of elements to persist
   */
  private async persistElements(
    component_id: number,
    screenshot_id: number,
    elements: any[]
  ): Promise<void> {
    try {
      const elementRecords = elements.map(element => ({
        component_id,
        screenshot_id,
        element_label: element.label || null,
        element_description: element.description || null,
        element_status: element.status || 'Detected',
        element_hidden: element.hidden || false,
        bounding_box: element.bounding_box || {},
        suggested_coordinates: element.suggested_coordinates || null,
        element_accuracy_score: element.accuracy_score || null,
        element_explanation: element.explanation || null,
        element_vlm_model: element.vlm_model || null,
        element_metadata_extraction: element.element_metadata_extraction || null,
        element_error: element.error || null,
        element_inference_time: element.element_inference_time || null
      }));

      const { error } = await this.supabaseClient
        .from('element')
        .insert(elementRecords);

      if (error) {
        console.error(`Failed to insert element records:`, error);
      }
    } catch (error) {
      console.error(`Error persisting elements:`, error);
    }
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/sample_data.ts
================
export const VALIDATED_RESULTS = [
    // {
    //   "label": "Profile Avatar > Profile Picture",
    //   "description": "A small circular profile image in the top-left corner of the interface with a light background",
    //   "bounding_box": {
    //     "x_min": 233,
    //     "y_min": 81,
    //     "x_max": 289,
    //     "y_max": 133
    //   },
    //   "status": "Detected",
    //   "vlm_model": "moondream",
    //   "element_inference_time": 0,
    //   "error": null,
    //   "accuracy": 95,
    //   "hidden": false,
    //   "explanation": "Bounding box tightly surrounds the circular profile picture with only minor overhang on the right, giving near-exact coverage of the avatar as described."
    // },
    // {
    //   "label": "Device Filter Dropdown > Device Count",
    //   "description": "Text displaying '3 devices' positioned under the scenes row on the left side",
    //   "bounding_box": {
    //     "x_min": 234,
    //     "y_min": 380,
    //     "x_max": 316,
    //     "y_max": 407
    //   },
    //   "status": "Detected",
    //   "vlm_model": "moondream",
    //   "element_inference_time": 0,
    //   "error": null,
    //   "accuracy": 86,
    //   "hidden": false,
    //   "explanation": "Bounding box fits closely around the '3 devices' text with only a slight margin on the left and right. Small extra padding decreases accuracy but requires no correction."
    // },
    // {
    //   "label": "Device Filter Dropdown > All Devices Button",
    //   "description": "A small pill-shaped button labeled 'All devices >' positioned on the right side below the scenes row",
    //   "bounding_box": {
    //     "x_min": 470,
    //     "y_min": 378,
    //     "x_max": 554,
    //     "y_max": 397
    //   },
    //   "status": "Detected",
    //   "vlm_model": "moondream",
    //   "element_inference_time": 0,
    //   "error": null,
    //   "accuracy": 84,
    //   "hidden": false,
    //   "explanation": "Bounding box accurately frames the 'All devices >' text and its pill shape, with minor excess empty space above and below. Box is sufficiently precise."
    // },
    // {
    //   "label": "Room Selector Tabs > Living Room Tab",
    //   "description": "A pill-shaped white button labeled 'Living room' positioned at the top of the screen among room selector options",
    //   "bounding_box": {
    //     "x_min": 231,
    //     "y_min": 149,
    //     "x_max": 345,
    //     "y_max": 193
    //   },
    //   "status": "Detected",
    //   "accuracy": 92,
    //   "hidden": false,
    //   "explanation": "The box tightly frames the 'Living room' button, accurately capturing the pill shape and label with minimal padding."
    // },
    // {
    //   "label": "Room Selector Tabs > Kitchen Tab",
    //   "description": "A pill-shaped light colored button labeled 'Kitchen' positioned in the center of the room selector row at the top of the screen",
    //   "bounding_box": {
    //     "x_min": 356,
    //     "y_min": 147,
    //     "x_max": 444,
    //     "y_max": 191
    //   },
    //   "status": "Detected",
    //   "accuracy": 89,
    //   "hidden": false,
    //   "explanation": "Box closely fits the 'Kitchen' tab; there's very slight extra space above/below, but overall it covers the intended UI element well."
    // },
    // {
    //   "label": "Room Selector Tabs > Bedroom Tab",
    //   "description": "A pill-shaped light colored button labeled 'Bedroom' positioned to the right in the room selector row at the top of the screen",
    //   "bounding_box": {
    //     "x_min": 445,
    //     "y_min": 146,
    //     "x_max": 550,
    //     "y_max": 190
    //   },
    //   "status": "Detected",
    //   "accuracy": 88,
    //   "hidden": false,
    //   "explanation": "Bounding box is slightly high above the tab but horizontally aligned; captures the 'Bedroom' button without major errors."
    // },
    // {
    //   "label": "Scenes Selector > Section Title",
    //   "description": "Text label 'Scenes' positioned above the row of circular scene selection buttons",
    //   "bounding_box": {
    //     "x_min": 234,
    //     "y_min": 210,
    //     "x_max": 294,
    //     "y_max": 229
    //   },
    //   "status": "Detected",
    //   "accuracy": 92,
    //   "hidden": false,
    //   "explanation": "Bounding box precisely fits the 'Scenes' text label; only minor overhang on the left, highly accurate."
    // },
    // {
    //   "label": "Scenes Selector > Awakening Scene",
    //   "description": "A circular white button with a sun icon and 'Awakening' label below it, leftmost in the scenes row",
    //   "bounding_box": {
    //     "x_min": 249,
    //     "y_min": 253,
    //     "x_max": 320,
    //     "y_max": 324
    //   },
    //   "status": "Detected",
    //   "accuracy": 96,
    //   "hidden": false,
    //   "explanation": "Bounding box tightly frames the circular Awakening button and its label below. Minor extra padding at the top right."
    // },
    // {
    //   "label": "Scenes Selector > Night Scene",
    //   "description": "A circular light gray button with a crescent moon icon and 'Night' label below it, second from left in the scenes row",
    //   "bounding_box": {
    //     "x_min": 328,
    //     "y_min": 253,
    //     "x_max": 387,
    //     "y_max": 319
    //   },
    //   "status": "Detected",
    //   "accuracy": 94,
    //   "hidden": false,
    //   "explanation": "Bounding box covers the button and 'Night' text. Very slight overhang at bottom, but still accurate."
    // },
    {
      "label": "Smart TV Card > Card Title",
      "description": "Bold text 'Smart TV' heading the third rectangular device card at the bottom of the screen",
      "bounding_box": {
        "x_min": 241,
        "y_min": 642,
        "x_max": 331,
        "y_max": 669
      },
      "status": "Detected",
      "accuracy": 95,
      "hidden": false,
      "explanation": "Bounding box closely matches the 'Smart TV' title visually and positionally. Only very minimal extra padding."
    },
    {
      "label": "Smart TV Card > Device Model",
      "description": "Gray text 'Samsung AR9500T' below the Smart TV title in the bottom device card",
      "bounding_box": {
        "x_min": 240,
        "y_min": 500,
        "x_max": 358,
        "y_max": 519
      },
      "status": "Overwrite",
      "accuracy": 10,
      "hidden": false,
      "suggested_coordinates": {
        "x_min": 245,
        "y_min": 672,
        "x_max": 330,
        "y_max": 690
      },
      "explanation": "Original box targeted the Air Conditioner card model, not the one in the Smart TV card. Suggested coordinates for correct position below 'Smart TV' title."
    },
    {
      "label": "Air Conditioner Card > Card Title",
      "description": "Bold text 'Air Conditioner' heading the first rectangular device card",
      "bounding_box": {
        "x_min": 229,
        "y_min": 411,
        "x_max": 396,
        "y_max": 578
      },
      "status": "Overwrite",
      "accuracy": 48,
      "hidden": false,
      "suggested_coordinates": {
        "x_min": 245,
        "y_min": 462,
        "x_max": 390,
        "y_max": 495
      },
      "explanation": "Original box covered card title and surrounding elements; resized to tightly bound only the 'Air Conditioner' text."
    },
    {
      "label": "Air Conditioner Card > Device Model",
      "description": "Gray text 'Samsung AR9500T' below the Air Conditioner title in the first device card",
      "bounding_box": {
        "x_min": 233,
        "y_min": 497,
        "x_max": 380,
        "y_max": 516
      },
      "status": "Detected",
      "accuracy": 94,
      "hidden": false,
      "explanation": "Box precisely fits the 'Samsung AR9500T' text with minimal extra vertical and horizontal padding."
    },
    {
      "label": "Smart Light Card > Card Title",
      "description": "Bold text 'Smart Light' heading the second rectangular device card in the middle of the screen",
      "bounding_box": {
        "x_min": 409,
        "y_min": 409,
        "x_max": 577,
        "y_max": 575
      },
      "status": "Overwrite",
      "accuracy": 42,
      "hidden": false,
      "suggested_coordinates": {
        "x_min": 423,
        "y_min": 446,
        "x_max": 553,
        "y_max": 489
      },
      "explanation": "Original box covered the entire card; corrected to tightly fit just the title text 'Smart Light' as described."
    },
    {
      "label": "Smart Light Card > Device Model",
      "description": "Gray text 'Mi Smart LED Ceiling Light' below the Smart Light title in the second device card",
      "bounding_box": {
        "x_min": 408,
        "y_min": 409,
        "x_max": 578,
        "y_max": 575
      },
      "status": "Overwrite",
      "accuracy": 40,
      "hidden": false,
      "suggested_coordinates": {
        "x_min": 425,
        "y_min": 494,
        "x_max": 572,
        "y_max": 517
      },
      "explanation": "Original box was the entire card; now tightly fits only the model text below 'Smart Light.'"
    }
  ];

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/sample_vision_api_call.txt
================
import OpenAI from "openai";

const openai = new OpenAI();

const response = await openai.responses.create({
    model: "gpt-4o-2024-11-20",
    input: [{
        role: "user",
        content: [
            { type: "input_text", text: "what's in this image?" },
            {
                type: "input_image",
                image_url: "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
            },
        ],
    }],
});

console.log(response.output_text);



import Anthropic from '@anthropic-ai/sdk';

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

async function main() {
  const message = await anthropic.messages.create({
    model: "claude-3-7-sonnet-20250219",
    max_tokens: 1024,
    messages: [
      {
        role: "user",
        content: [
          {
            type: "image",
            source: {
              type: "url",
              url: "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg"
            }
          },
          {
            type: "text",
            text: "Describe this image."
          }
        ]
      }
    ]
  });
  
  console.log(message);
}

main();

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/constants.ts
================
export const MAX_FILES_PER_BATCH = 20;
export const DEFAULT_BATCH_NAME_PREFIX = 'Batch_';
export const EXTRACTION_CONCURRENCY = 5; // Concurrency limit for OpenAI/Claude calls
export const MOONDREAM_CONCURRENCY = 10; // Limit concurrency for Moondream processing per batch
export const VALIDATION_CONCURRENCY = 10;
export const VALIDATION_THRESHOLD = 50;

export enum PromptLogType {
  COMPONENT_EXTRACTION = 'component_extraction',
  ELEMENT_EXTRACTION = 'element_extraction',
  ANCHORING = 'anchoring',
  VLM_LABELING = 'vlm_labeling',
  ACCURACY_VALIDATION = 'accuracy_validation',
  METADATA_EXTRACTION = 'metadata_extraction',
}

export enum ProcessStatus {
  UPLOADING = 'uploading',
  EXTRACTING = 'extracting',
  ANNOTATING = 'annotating',
  VALIDATING = 'validating',
  SAVING = 'saving',
  DONE = 'done',
  FAILED = 'failed',
}



export const API_ENDPOINTS = {
  BATCHES: '/api/batches',
  UPLOAD: '/api/upload',
} as const;

export const TOAST_MESSAGES = {
  UPLOAD_ERROR: 'Upload failed. Please try again.',
  LOADING_BATCHES: 'Loading batches, please wait...',
} as const;

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/file-utils.ts
================
import { MAX_FILES_PER_BATCH } from './constants';

export const filterAndLimitImageFiles = (files: File[], existingFiles: File[] = []): File[] => {
  const imageFiles = files.filter((file) => file.type.startsWith('image/'));
  const limitedFiles = imageFiles.slice(0, MAX_FILES_PER_BATCH);
  const combinedFiles = [...existingFiles, ...limitedFiles].slice(0, MAX_FILES_PER_BATCH);
  return combinedFiles;
};

export const removeFileAtIndex = (files: File[], index: number): File[] => {
  const newFiles = [...files];
  newFiles.splice(index, 1);
  return newFiles;
}; 

/**
 * Cleans the raw text by removing unwanted formatting and normalizing it.
 *
 * @param rawText - The raw text string to clean.
 * @returns Cleaned text as a single string.
 */
export function cleanText(rawText: string): string {
  // Remove extra line breaks and normalize formatting
  return rawText
    .replace(/,\s*}/g, '}')           // remove trailing commas
    .replace(/,\s*]/g, ']')           // remove trailing commas
    .replace(/```json/g, '')           // remove ```json
    .replace(/```/g, '')              // remove ```
    // .replace(/\\/g, '');              // remove \
    // .replace(/\n/g, '');              // flatten into one line
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/logger.ts
================
import fs from 'fs';
import path from 'path';
import { supabase } from '@/lib/supabase';

// Define constants for logging
const LOG_DIR = process.env.LOG_DIR || 'logs';
const LOG_FILE = 'prompt-interactions.log';

import { PromptLogType } from '@/lib/constants';

/**
 * Log entry structure for AI prompt interactions
 */
interface PromptLogEntry {
  timestamp: string;
  serviceName: string;
  prompt: string;
  response: string;
  executionTimeMs: number;
  tokenUsage?: {
    input?: number;
    output?: number;
    total?: number;
  };
}

/**
 * Database prompt log entry interface matching the prompt_log table schema
 */
interface DbPromptLogEntry {
  batch_id: number;
  screenshot_id?: number;
  component_id?: number;
  element_id?: number;
  prompt_log_type: PromptLogType;
  prompt_log_model: string;
  prompt_log_input_tokens?: number;
  prompt_log_output_tokens?: number;
  prompt_log_cost?: number;
  prompt_log_duration: number;
  prompt_log_started_at?: string;
  prompt_log_completed_at?: string;
  prompt_response?: string;
}


/**
 * Tracking context for AI service calls
 * Encapsulates all the IDs and metadata needed for logging prompt interactions
 */
export class PromptTrackingContext {
  constructor(
    public readonly batchId: number,
    public readonly screenshotId?: number,
    public readonly componentId?: number,
    public readonly elementId?: number
  ) {}

  /**
   * Log a prompt interaction to both file and database
   * 
   * @param modelName The name of the AI model (e.g. 'OpenAI-gpt-4', 'Claude-3-5')
   * @param promptType The type of prompt being logged
   * @param prompt The prompt text sent to the model
   * @param response The response from the model
   * @param durationMs The duration of the operation in milliseconds
   * @param tokenUsage Optional token usage statistics
   * @param costPerInputToken Optional cost per input token
   * @param costPerOutputToken Optional cost per output token
   */
  public async logPromptInteraction(
    modelName: string,
    promptType: PromptLogType,
    prompt: string,
    response: string,
    durationMs: number,
    tokenUsage?: {
      input?: number;
      output?: number;
      total?: number;
    },
    costPerInputToken?: number,
    costPerOutputToken?: number
  ): Promise<void> {
    const durationSecs = durationMs / 1000;
    const startedAt = new Date(Date.now() - durationMs).toISOString();
    
    // Log to file
    logPromptInteraction(
      modelName,
      prompt, 
      response,
      durationMs,
      tokenUsage
    );
    
    // Calculate cost if token usage and rates are provided
    let cost: number | undefined = undefined;
    if (tokenUsage && costPerInputToken && costPerOutputToken) {
      const inputTokens = tokenUsage.input || 0;
      const outputTokens = tokenUsage.output || 0;
      cost = (inputTokens * costPerInputToken) + (outputTokens * costPerOutputToken);
    }
    
    // Log to database
    await logPromptToDatabase({
      batch_id: this.batchId,
      screenshot_id: this.screenshotId,
      component_id: this.componentId,
      element_id: this.elementId,
      prompt_log_type: promptType,
      prompt_log_model: modelName,
      prompt_log_input_tokens: tokenUsage?.input,
      prompt_log_output_tokens: tokenUsage?.output,
      prompt_log_cost: cost,
      prompt_log_duration: durationSecs,
      prompt_log_started_at: startedAt,
      prompt_response: response
    });
  }
  
  /**
   * Create a derived context with a component ID
   */
  public withComponentId(componentId: number): PromptTrackingContext {
    return new PromptTrackingContext(
      this.batchId,
      this.screenshotId,
      componentId,
      this.elementId
    );
  }
  
  /**
   * Create a derived context with an element ID
   */
  public withElementId(elementId: number): PromptTrackingContext {
    return new PromptTrackingContext(
      this.batchId,
      this.screenshotId,
      this.componentId,
      elementId
    );
  }
}

/**
 * Create a tracking context for a batch
 */
export function createBatchTrackingContext(batchId: number): PromptTrackingContext {
  return new PromptTrackingContext(batchId);
}

/**
 * Create a tracking context for a screenshot within a batch
 */
export function createScreenshotTrackingContext(batchId: number, screenshotId: number): PromptTrackingContext {
  return new PromptTrackingContext(batchId, screenshotId);
}

/**
 * Ensures the log directory exists
 */
function ensureLogDirectory(): string {
  const logDirPath = path.resolve(process.cwd(), LOG_DIR);
  
  if (!fs.existsSync(logDirPath)) {
    fs.mkdirSync(logDirPath, { recursive: true });
  }
  
  return path.join(logDirPath, LOG_FILE);
}

/**
 * Formats a log entry into a human-readable string
 */
function formatLogEntry(entry: PromptLogEntry): string {
  const tokenInfo = entry.tokenUsage 
    ? `Input Tokens: ${entry.tokenUsage.input || 'N/A'}
Output Tokens: ${entry.tokenUsage.output || 'N/A'}
Total Tokens: ${entry.tokenUsage.total || 'N/A'}`
    : 'Token Usage: Not Available';

  return `
========== PROMPT INTERACTION LOG ==========
Timestamp: ${entry.timestamp}
Service: ${entry.serviceName}
Execution Time: ${entry.executionTimeMs / 1000} seconds

----- PROMPT -----
${entry.prompt}

----- RESPONSE -----
${JSON.stringify(JSON.parse(entry.response), null, 2)}

----- USAGE -----
${tokenInfo}
========== END OF ENTRY ==========

`;
}

/**
 * Logs a prompt interaction to the log file
 */
export function logPromptInteraction(
  serviceName: string,
  prompt: string,
  response: string,
  executionTimeMs: number,
  tokenUsage?: {
    input?: number;
    output?: number;
    total?: number;
  }
): void {
  const logFilePath = ensureLogDirectory();
  
  const entry: PromptLogEntry = {
    timestamp: new Date().toISOString(),
    serviceName,
    prompt,
    response,
    executionTimeMs,
    tokenUsage
  };
  
  const formattedEntry = formatLogEntry(entry);
  
  // Append to log file
  fs.appendFileSync(logFilePath, formattedEntry, 'utf8');
}

/**
 * Logs a prompt interaction to the database prompt_log table
 * 
 * @param logEntry Database prompt log entry matching the table schema
 * @returns Promise resolving to the database insertion result
 */
export async function logPromptToDatabase(logEntry: DbPromptLogEntry): Promise<void> {
  try {
    // Calculate completed_at from the started_at and duration
    const completed_at = logEntry.prompt_log_started_at 
      ? new Date(new Date(logEntry.prompt_log_started_at).getTime() + logEntry.prompt_log_duration * 1000).toISOString()
      : new Date().toISOString();

    const { error } = await supabase
      .from('prompt_log')
      .insert({
        batch_id: logEntry.batch_id,
        screenshot_id: logEntry.screenshot_id,
        component_id: logEntry.component_id,
        element_id: logEntry.element_id,
        prompt_log_type: logEntry.prompt_log_type,
        prompt_log_model: logEntry.prompt_log_model,
        prompt_log_input_tokens: logEntry.prompt_log_input_tokens,
        prompt_log_output_tokens: logEntry.prompt_log_output_tokens,
        prompt_log_cost: logEntry.prompt_log_cost,
        prompt_log_duration: logEntry.prompt_log_duration,
        prompt_log_started_at: logEntry.prompt_log_started_at || new Date().toISOString(),
        prompt_log_completed_at: logEntry.prompt_log_completed_at || new Date().toISOString(),
        prompt_response: logEntry.prompt_response ? `"${logEntry.prompt_response}"` : null
      });

    if (error) {
      console.error('Error logging prompt to database:', error);
    }
  } catch (err) {
    console.error('Failed to log prompt to database:', err);
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/schema_v2.sql
================
-- Schema Version 2
-- Description: Database schema that works

-- Drop tables in dependency order
DROP TABLE IF EXISTS annotation_element CASCADE;
DROP TABLE IF EXISTS annotation_version CASCADE;
DROP TABLE IF EXISTS screenshot CASCADE;
DROP TABLE IF EXISTS batch CASCADE;
DROP TABLE IF EXISTS taxonomy CASCADE;

-- Taxonomy: shared label definitions for UI elements
CREATE TABLE taxonomy (
    taxonomy_id SERIAL PRIMARY KEY,
    taxonomy_label_name TEXT NOT NULL,       -- e.g., 'Button'
    taxonomy_description TEXT,               -- explanation/usage notes
    taxonomy_created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- Batch: groups of uploaded screenshots with performance data
CREATE TABLE batch (
    batch_id SERIAL PRIMARY KEY,
    batch_name TEXT NOT NULL,             -- batch name from the UI
    batch_created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    batch_status TEXT NOT NULL CHECK (batch_status IN ('uploading', 'extracting', 'annotating', 'preview', 'done')),
    batch_analysis_type TEXT NOT NULL CHECK (batch_analysis_type IN ('Usability Audit', 'Conversion Analysis', 'UI Categorization')),
    batch_master_prompt_runtime NUMERIC,  -- in seconds
    batch_total_inference_time NUMERIC,   -- in seconds
    batch_detected_elements_count INTEGER, -- number of detected UI elements
    batch_description TEXT                -- optional metadata
);

-- Screenshot: individual UI images; file_url is generated by Supabase Storage
CREATE TABLE screenshot (
    screenshot_id SERIAL PRIMARY KEY,
    batch_id INTEGER REFERENCES batch(batch_id) ON DELETE CASCADE,
    screenshot_file_name TEXT NOT NULL,
    screenshot_file_url TEXT NOT NULL,         -- full public URL from Supabase Storage
    screenshot_processing_status TEXT NOT NULL CHECK (screenshot_processing_status IN ('pending', 'processing', 'completed', 'error')),
    screenshot_processing_time INTERVAL,       -- time taken to annotate this image
    screenshot_created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- Annotation Version: versioning for screenshot annotations
CREATE TABLE annotation_version (
    annotation_version_id SERIAL PRIMARY KEY,
    screenshot_id INTEGER REFERENCES screenshot(screenshot_id) ON DELETE CASCADE,
    annotation_version_number INTEGER NOT NULL,  -- starts at 1 and increments
    annotation_version_created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    CONSTRAINT unique_version_per_screenshot UNIQUE (screenshot_id, annotation_version_number)
);

-- Annotation Element: details for each UI element annotation
CREATE TABLE annotation_element (
    annotation_element_id SERIAL PRIMARY KEY,
    annotation_version_id INTEGER REFERENCES annotation_version(annotation_version_id) ON DELETE CASCADE,
    annotation_element_x_min NUMERIC NOT NULL,             -- x-coordinate (top-left)
    annotation_element_x_max NUMERIC NOT NULL,             -- to get bounding box width
    annotation_element_y_min NUMERIC NOT NULL,             -- y-coordinate
    annotation_element_y_max NUMERIC NOT NULL,             -- to get bounding box height
    taxonomy_id INTEGER REFERENCES taxonomy(taxonomy_id),  -- shared label definition
    annotation_element_text_label TEXT,                -- display text (from UI)
    annotation_element_description TEXT,               -- additional element description
    annotation_element_inference_time NUMERIC NOT NULL -- detection time in seconds
);

-- Add indexes for common queries and foreign keys
CREATE INDEX idx_screenshot_batch ON screenshot(batch_id);
CREATE INDEX idx_annotation_version_screenshot ON annotation_version(screenshot_id);
CREATE INDEX idx_annotation_element_version ON annotation_element(annotation_version_id);
CREATE INDEX idx_annotation_element_taxonomy ON annotation_element(taxonomy_id);

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/schema_v3.sql
================
-- Schema Version 3
-- Description: Database schema v3 (Removed annotation_element_versions, and added trigger)


-- Drop tables in dependency order
DROP TABLE IF EXISTS annotation_element CASCADE;
DROP TABLE IF EXISTS screenshot CASCADE;
DROP TABLE IF EXISTS batch CASCADE;
DROP TABLE IF EXISTS taxonomy CASCADE;

------------------------------------------------------------
-- Taxonomy: shared label definitions for UI elements
------------------------------------------------------------
CREATE TABLE taxonomy (
    taxonomy_id BIGSERIAL PRIMARY KEY,
    taxonomy_label_name TEXT NOT NULL,       -- e.g., 'Button'
    taxonomy_description TEXT,               -- explanation/usage notes
    taxonomy_created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

------------------------------------------------------------
-- Batch: groups of uploaded screenshots with performance data
------------------------------------------------------------
CREATE TABLE batch (
    batch_id BIGSERIAL PRIMARY KEY,
    batch_name TEXT NOT NULL,             -- batch name from the UI
    batch_created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    batch_status TEXT NOT NULL CHECK (batch_status IN ('uploading', 'extracting', 'annotating', 'preview', 'done')),
    batch_analysis_type TEXT NOT NULL CHECK (batch_analysis_type IN ('Usability Audit', 'Conversion Analysis', 'UI Categorization')),
    batch_master_prompt_runtime NUMERIC,   -- in seconds
    batch_total_inference_time NUMERIC,    -- in seconds
    batch_detected_elements_count INTEGER, -- number of detected UI elements
    batch_description TEXT                 -- optional metadata
);

------------------------------------------------------------
-- Screenshot: individual UI images; file_url is generated by Supabase Storage
------------------------------------------------------------
CREATE TABLE screenshot (
    screenshot_id BIGSERIAL PRIMARY KEY,
    batch_id BIGINT NOT NULL,  -- renamed to match v2 foreign key naming
    screenshot_file_name TEXT NOT NULL,
    screenshot_file_url TEXT NOT NULL,         -- full public URL from Supabase Storage
    screenshot_processing_status TEXT NOT NULL CHECK (screenshot_processing_status IN ('pending', 'processing', 'completed', 'error')),
    screenshot_processing_time INTERVAL,       -- time taken to annotate this image
    screenshot_created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    CONSTRAINT fk_screenshot_batch
        FOREIGN KEY (batch_id)
        REFERENCES batch(batch_id)
        ON DELETE CASCADE
);

-- Index for better join performance on batch_id
CREATE INDEX idx_screenshot_batch_id ON screenshot(batch_id);

------------------------------------------------------------
-- Annotation Element: details for each UI element annotation
-- Denormalized table containing annotation metadata (including integrated versioning)
------------------------------------------------------------
CREATE TABLE annotation_element (
    annotation_element_id BIGSERIAL PRIMARY KEY,
    screenshot_id BIGINT NOT NULL, -- renamed to match v2 foreign key naming (was annotation_element_screenshot_id)
    annotation_element_version_number INTEGER NOT NULL DEFAULT 1,  -- version number (default 1, incremented for revisions)
    annotation_element_created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    annotation_element_x_min NUMERIC NOT NULL,           -- x-coordinate (top-left)
    annotation_element_x_max NUMERIC NOT NULL,           -- for bounding box width
    annotation_element_y_min NUMERIC NOT NULL,           -- y-coordinate (top-left)
    annotation_element_y_max NUMERIC NOT NULL,           -- for bounding box height
    taxonomy_id BIGINT,               -- renamed to match v2 (was annotation_element_taxonomy_id)
    annotation_element_text_label TEXT,                  -- display text (from UI)
    annotation_element_description TEXT,                 -- additional element description
    annotation_element_inference_time NUMERIC NOT NULL,  -- detection time in seconds
    CONSTRAINT fk_annotation_element_screenshot
        FOREIGN KEY (screenshot_id)
        REFERENCES screenshot(screenshot_id)
        ON DELETE CASCADE,
    CONSTRAINT fk_annotation_element_taxonomy
        FOREIGN KEY (taxonomy_id)
        REFERENCES taxonomy(taxonomy_id)
        ON DELETE SET NULL
);

-- Indexes for efficient lookups on foreign keys
CREATE INDEX idx_annotation_element_screenshot_id ON annotation_element(screenshot_id);
CREATE INDEX idx_annotation_element_taxonomy_id ON annotation_element(taxonomy_id);

------------------------------------------------------------
-- Function and Trigger to Automatically Set version_number
------------------------------------------------------------
CREATE OR REPLACE FUNCTION set_annotation_version()
RETURNS TRIGGER AS $$
DECLARE
    current_max INTEGER;
BEGIN
    -- Only set version_number if it was not explicitly provided (or provided as 0)
    IF NEW.annotation_element_version_number IS NULL OR NEW.annotation_element_version_number = 0 THEN
        SELECT COALESCE(MAX(annotation_element_version_number), 0)
          INTO current_max
        FROM annotation_element
        WHERE screenshot_id = NEW.screenshot_id;
        NEW.annotation_element_version_number := current_max + 1;
    END IF;
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER trg_set_annotation_version
BEFORE INSERT ON annotation_element
FOR EACH ROW
EXECUTE FUNCTION set_annotation_version();

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/schema_v4.sql
================
-- --------------------------------------------------------
-- 1) Drop in Dependency Order
-- --------------------------------------------------------
-- DROP TABLE IF EXISTS status_event CASCADE;
-- DROP TABLE IF EXISTS prompt_log CASCADE;
-- DROP TABLE IF EXISTS element CASCADE;
-- DROP TABLE IF EXISTS component CASCADE;
-- DROP TABLE IF EXISTS screenshot CASCADE;
-- DROP TABLE IF EXISTS batch CASCADE;
-- DROP TABLE IF EXISTS taxonomy CASCADE;

-- --------------------------------------------------------
-- 2) Taxonomy (unchanged)
-- --------------------------------------------------------
CREATE TABLE taxonomy (
  taxonomy_id             BIGSERIAL PRIMARY KEY,
  taxonomy_label_name     TEXT        NOT NULL,
  taxonomy_description    TEXT,
  taxonomy_created_at     TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- --------------------------------------------------------
-- 3) Batch (added token+cost fields)
-- --------------------------------------------------------
CREATE TABLE batch (
  batch_id                       BIGSERIAL PRIMARY KEY,
  batch_name                     TEXT        NOT NULL,
  batch_created_at               TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  batch_status                   TEXT        NOT NULL
                                    CHECK (batch_status IN (
                                      'uploading','extracting','annotating',
                                      'validating','done'
                                    )),
  batch_analysis_type            TEXT        NOT NULL
                                    CHECK (batch_analysis_type IN (
                                      'Usability Audit',
                                      'Conversion Analysis',
                                      'UI Categorization'
                                    )),
  batch_master_prompt_runtime    NUMERIC,   -- seconds
  batch_total_inference_time     NUMERIC,   -- seconds
  batch_detected_elements_count  INTEGER,
  batch_input_token_count        BIGINT,    -- new: sum of all input tokens
  batch_output_token_count       BIGINT,    -- new: sum of all output tokens
  batch_total_cost               NUMERIC,   -- new: estimated $
  batch_description              TEXT
);

-- --------------------------------------------------------
-- 4) Screenshot (unchanged)
-- --------------------------------------------------------
CREATE TABLE screenshot (
  screenshot_id                BIGSERIAL PRIMARY KEY,
  batch_id                     BIGINT      NOT NULL,
  screenshot_file_name         TEXT        NOT NULL,
  screenshot_file_url          TEXT        NOT NULL,
  screenshot_processing_status TEXT        NOT NULL
                                   CHECK (screenshot_processing_status IN (
                                     'pending','processing','completed','error'
                                   )),
  screenshot_processing_time   INTERVAL,
  screenshot_created_at        TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  CONSTRAINT fk_screenshot_batch_v4
    FOREIGN KEY (batch_id)
    REFERENCES batch(batch_id)
    ON DELETE CASCADE
);
CREATE INDEX idx_screenshot_batch_id_v4 ON screenshot(batch_id);

-- --------------------------------------------------------
-- 5) Component (new high‑level UI section)
-- --------------------------------------------------------
CREATE TABLE component (
  component_id                   BIGSERIAL PRIMARY KEY,
  screenshot_id                  BIGINT      NOT NULL,
  component_name                 TEXT        NOT NULL,
  component_description          TEXT,
  component_cta_type             TEXT
                                   CHECK (component_cta_type IN (
                                     'primary','secondary','informative'
                                   )),
  component_reusable             BOOLEAN     NOT NULL DEFAULT FALSE,
  component_x_min                NUMERIC,
  component_y_min                NUMERIC,
  component_x_max                NUMERIC,
  component_y_max                NUMERIC,
  component_extraction_model     TEXT,
  component_extraction_time      NUMERIC,
  component_extraction_input_tokens  INTEGER,
  component_extraction_output_tokens INTEGER,
  component_extraction_cost      NUMERIC,
  component_status               TEXT        NOT NULL DEFAULT 'pending'
                                   CHECK (component_status IN (
                                     'pending','extracted','error'
                                   )),
  component_created_at           TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  CONSTRAINT fk_component_screenshot_v4
    FOREIGN KEY (screenshot_id)
    REFERENCES screenshot(screenshot_id)
    ON DELETE CASCADE
);
CREATE INDEX idx_component_screenshot_id_v4 ON component(screenshot_id);

-- --------------------------------------------------------
-- 6) element (updated to link → component_id + extra fields)
-- --------------------------------------------------------
CREATE TABLE element (
  element_id            BIGSERIAL PRIMARY KEY,
  screenshot_id                     BIGINT      NOT NULL,
  component_id                      BIGINT      NOT NULL,
  element_version_number INTEGER     NOT NULL DEFAULT 1,
  element_created_at     TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  element_x_min          NUMERIC     NOT NULL,
  element_y_min          NUMERIC     NOT NULL,
  element_x_max          NUMERIC     NOT NULL,
  element_y_max          NUMERIC     NOT NULL,
  taxonomy_id                       BIGINT,
  element_text_label     TEXT,
  element_description    TEXT,
  element_inference_time NUMERIC     NOT NULL,
  element_vlm_model      TEXT,
  element_vlm_label_status TEXT
                                   CHECK (element_vlm_label_status IN (
                                     'pending','completed','error'
                                   )) NOT NULL DEFAULT 'pending',
  element_accuracy_score NUMERIC,
  element_suggested_coordinates JSONB,
  element_updated_at     TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  CONSTRAINT fk_element_screenshot_v4
    FOREIGN KEY (screenshot_id)
    REFERENCES screenshot(screenshot_id)
    ON DELETE CASCADE,
  CONSTRAINT fk_element_component_v4
    FOREIGN KEY (component_id)
    REFERENCES component(component_id)
    ON DELETE CASCADE,
  CONSTRAINT fk_element_taxonomy_v4
    FOREIGN KEY (taxonomy_id)
    REFERENCES taxonomy(taxonomy_id)
    ON DELETE SET NULL
);
CREATE INDEX idx_element_screenshot_id_v4 ON element(screenshot_id);
CREATE INDEX idx_element_component_id_v4 ON element(component_id);
CREATE INDEX idx_element_taxonomy_id_v4 ON element(taxonomy_id);

-- preserve your version trigger
CREATE OR REPLACE FUNCTION set_annotation_version()
RETURNS TRIGGER AS $$
DECLARE
  current_max INTEGER;
BEGIN
  IF NEW.element_version_number IS NULL
     OR NEW.element_version_number = 0 THEN
    SELECT COALESCE(MAX(element_version_number),0)
      INTO current_max
      FROM element
     WHERE component_id = NEW.component_id;
    NEW.element_version_number := current_max + 1;
  END IF;
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER trg_set_annotation_version
BEFORE INSERT ON element
FOR EACH ROW
EXECUTE FUNCTION set_annotation_version();

-- --------------------------------------------------------
-- 7) Prompt_Log (every LLM/VLM call)
-- --------------------------------------------------------
CREATE TABLE prompt_log (
  prompt_log_id              BIGSERIAL PRIMARY KEY,
  batch_id                   BIGINT      NOT NULL,
  screenshot_id              BIGINT,
  component_id               BIGINT,
  element_id      BIGINT,
  prompt_log_type            TEXT        NOT NULL
                                   CHECK (prompt_log_type IN (
                                     'component_extraction',
                                     'element_extraction',
                                     'anchoring',
                                     'vlm_labeling',
                                     'accuracy_validation'
                                   )),
  prompt_log_model           TEXT        NOT NULL,
  prompt_log_input_tokens    INTEGER,
  prompt_log_output_tokens   INTEGER,
  prompt_log_cost            NUMERIC,
  prompt_log_duration        NUMERIC     NOT NULL,
  prompt_log_started_at      TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  prompt_log_completed_at    TIMESTAMPTZ,
  CONSTRAINT fk_prompt_log_batch_v4
    FOREIGN KEY (batch_id)
    REFERENCES batch(batch_id)
    ON DELETE CASCADE,
  CONSTRAINT fk_prompt_log_screenshot_v4
    FOREIGN KEY (screenshot_id)
    REFERENCES screenshot(screenshot_id)
    ON DELETE CASCADE,
  CONSTRAINT fk_prompt_log_component_v4
    FOREIGN KEY (component_id)
    REFERENCES component(component_id)
    ON DELETE CASCADE,
  CONSTRAINT fk_prompt_log_element_v4
    FOREIGN KEY (element_id)
    REFERENCES element(element_id)
    ON DELETE CASCADE
);
CREATE INDEX idx_prompt_log_batch_id_v4       ON prompt_log(batch_id);
CREATE INDEX idx_prompt_log_screenshot_id_v4  ON prompt_log(screenshot_id);
CREATE INDEX idx_prompt_log_component_id_v4   ON prompt_log(component_id);
CREATE INDEX idx_prompt_log_element_id_v4     ON prompt_log(element_id);

-- --------------------------------------------------------
-- 8) Status_Event (for real‑time updates)
-- --------------------------------------------------------
CREATE TABLE status_event (
  status_event_id           BIGSERIAL PRIMARY KEY,
  batch_id                  BIGINT      NOT NULL,
  screenshot_id             BIGINT,
  component_id              BIGINT,
  element_id     BIGINT,
  status_event_type         TEXT        NOT NULL,
  status_event_payload      JSONB,
  status_event_created_at   TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  CONSTRAINT fk_status_event_batch_v4
    FOREIGN KEY (batch_id)
    REFERENCES batch(batch_id)
    ON DELETE CASCADE,
  CONSTRAINT fk_status_event_screenshot_v4
    FOREIGN KEY (screenshot_id)
    REFERENCES screenshot(screenshot_id)
    ON DELETE CASCADE,
  CONSTRAINT fk_status_event_component_v4
    FOREIGN KEY (component_id)
    REFERENCES component(component_id)
    ON DELETE CASCADE,
  CONSTRAINT fk_status_event_element_v4
    FOREIGN KEY (element_id)
    REFERENCES element(element_id)
    ON DELETE CASCADE
);
CREATE INDEX idx_status_event_batch_id_v4       ON status_event(batch_id);
CREATE INDEX idx_status_event_screenshot_id_v4  ON status_event(screenshot_id);
CREATE INDEX idx_status_event_component_id_v4   ON status_event(component_id);
CREATE INDEX idx_status_event_element_id_v4     ON status_event(element_id);

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/storage.ts
================
import { createClient } from '@supabase/supabase-js';
import { supabase } from './supabase';
import fs from 'fs';
import { SUPABASE_BUCKET_NAME } from '@/config';

// Constants
const DEFAULT_BUCKET_NAME = SUPABASE_BUCKET_NAME || 'v5';
const PUBLIC_FOLDER = 'public';

export interface UploadResult {
  fileUrl: string;
  error?: Error;
}

/**
 * Uploads a file from a local path to Supabase Storage
 * @param filePath - Path to the local file
 * @param fileName - Name to use for the uploaded file
 * @param bucketName - Supabase storage bucket name
 * @returns Public URL of the uploaded file or null if failed
 */
export async function uploadToSupabaseStorage(
  filePath: string,
  fileName: string,
  bucketName: string = DEFAULT_BUCKET_NAME
): Promise<string | null> {
  try {
    // Read the file
    const fileBuffer = fs.readFileSync(filePath);
    
    // Upload to Supabase
    const { data, error } = await supabase.storage
      .from(bucketName)
      .upload(`${PUBLIC_FOLDER}/${fileName}`, fileBuffer, {
        contentType: 'image/jpeg',
        upsert: true
      });
    
    if (error) {
      console.error('Error uploading to Supabase:', error);
      return null;
    }
    
    // Get public URL
    const { data: urlData } = supabase.storage
      .from(bucketName)
      .getPublicUrl(`${PUBLIC_FOLDER}/${fileName}`);
    
    return urlData.publicUrl;
  } catch (error) {
    console.error('Error in uploadToSupabaseStorage:', error);
    return null;
  }
}

/**
 * Generates a random 4-character alphabetic string
 */
function generateRandomSuffix(): string {
  const alphabet = 'AaBbCcDdEeFfGgHhIiJjKkLlMmNnOoPpQqRrSsTtUuVvWwXxYyZz';
  let result = '';
  for (let i = 0; i < 4; i++) {
    result += alphabet.charAt(Math.floor(Math.random() * alphabet.length));
  }
  return result;
}

export async function uploadImageToStorage(
  file: File | Blob,
  batchId: number,
  filename: string
): Promise<UploadResult> {
  try {
    // Add random suffix to filename
    const fileExtension = filename.split('.').pop();
    const baseName = filename.substring(0, filename.lastIndexOf('.'));
    const randomSuffix = generateRandomSuffix();
    const newFilename = `${baseName}_${randomSuffix}.${fileExtension}`;
    
    const storagePath = `${DEFAULT_BUCKET_NAME}/${batchId}/${newFilename}`;
    
    const { data, error } = await supabase.storage
      .from(DEFAULT_BUCKET_NAME)
      .upload(storagePath, file, {
        cacheControl: '3600',
        upsert: false
      });

    if (error) throw error;

    const { data: { publicUrl } } = supabase.storage
      .from(DEFAULT_BUCKET_NAME)
      .getPublicUrl(storagePath);

    return { fileUrl: publicUrl };
  } catch (error) {
    console.error('Error uploading to storage:', error);
    return { fileUrl: '', error: error as Error };
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/supabase.ts
================
import { createClient } from '@supabase/supabase-js'

const supabaseUrl = process.env.SUPABASE_URL!
const supabaseKey = process.env.SUPABASE_ANON_KEY!
const serviceRoleKey = process.env.SUPABASE_SERVICE_KEY!

console.log("Supabase URL:", supabaseUrl)
console.log("Supabase Anon Key:", supabaseKey)
console.log("Supabase Service Role Key:", serviceRoleKey)

// Client with service role for server-side operations
export const supabase = createClient(supabaseUrl, serviceRoleKey)
// Client for client-side operations
export const supabaseClient = createClient(supabaseUrl, supabaseKey)

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/supabaseUtils.ts
================
import { SupabaseClient, PostgrestError } from '@supabase/supabase-js';
import { SUPABASE_BUCKET_NAME } from '@/config';

const SCREENSHOT_BUCKET = SUPABASE_BUCKET_NAME || 'v5'
const SIGNED_URL_EXPIRY_SECONDS = 3600 // 1 hour

const ERROR_GENERATING_SIGNED_URL = 'Error generating signed URL';

interface SignedUrlResult {
    error: string | null;
    path: string | null;
    signedUrl: string;
}

// Cache structure for signed URLs
interface CachedUrl {
    signedUrl: string;
    expiresAt: number; // Store expiry timestamp in milliseconds
}

// Simple in-memory cache.
// Consider using a proper LRU cache library (e.g., 'lru-cache') for large-scale applications
// to manage memory effectively by evicting least recently used items.
const signedUrlCache = new Map<string, CachedUrl>();

export function getScreenshotPath(url: string): string {
    // console.log('Extracting storage path from screenshot URL:', url);
    try {
      const parsedUrl = new URL(url);
    //   console.log('Parsed URL:', parsedUrl);
      // Example path: /storage/v1/object/public/screenshot/batch_123/image.jpg
      // We need the part after the bucket name: batch_123/image.jpg
      const pathParts = parsedUrl.pathname.split('/');
    //   console.log('Path parts:', pathParts);
      const bucketNameIndex = pathParts.findIndex(part => part === SCREENSHOT_BUCKET);
    //   console.log('Bucket name index:', bucketNameIndex);
  
      if (bucketNameIndex === -1 || bucketNameIndex + 1 >= pathParts.length) {
        console.error(`Bucket '${SCREENSHOT_BUCKET}' not found or path is incomplete in URL: ${url}`);
        throw new Error(`Invalid screenshot URL format: ${url}`);
      }
      // Join the parts after the bucket name
      const storagePath = pathParts.slice(bucketNameIndex + 1).join('/');
    //   console.log('Extracted storage path:', storagePath);
      return storagePath;
    } catch (e) {
      // Catch URL parsing errors as well
      console.error(`Error parsing screenshot URL '${url}':`, e);
      // Re-throw a more specific error or handle as needed
      throw new Error(`Invalid screenshot URL format: ${url}`);
    }
  }

//   
/**
 * Generates signed URLs for a list of object paths within a Supabase storage bucket.
 * Throws an error if any URL generation fails.
 * @param supabase - The Supabase client instance.
 * @param bucketName - The name of the storage bucket.
 * @param paths - An array of object paths within the bucket.
 * @param expiresIn - The duration in seconds for which the signed URLs will be valid.
 * @returns A Promise resolving to a Map where keys are the original paths and values are the signed URLs.
 * @throws Throws an error if Supabase client fails to generate URLs or if any individual URL generation fails.
 */
export async function generateSignedUrls(
    supabase: SupabaseClient,
    bucketName: string,
    paths: string[],
    expiresIn: number
): Promise<Map<string, string>> {
    if (paths.length === 0) {
        return new Map<string, string>(); // Return empty map if no paths
    }

    const { data: signedUrlsResult, error: bulkUrlError } = await supabase
        .storage
        .from(bucketName)
        .createSignedUrls(paths, expiresIn);

    if (bulkUrlError) {
        console.error(`Error generating signed URLs in bulk for bucket '${bucketName}':`, bulkUrlError);
        // Throw a specific error indicating bulk operation failure
        throw new Error(`${ERROR_GENERATING_SIGNED_URL}: Bulk operation failed.`);
    }

    if (!signedUrlsResult) {
        // This case should theoretically be covered by bulkUrlError, but added for robustness
        console.error(`No data returned for signed URLs batch for bucket '${bucketName}', but no error reported.`);
        throw new Error(`${ERROR_GENERATING_SIGNED_URL}: No data returned from Supabase.`);
    }

    const signedUrlMap = new Map<string, string>();

    // Process results and populate the map, checking for individual errors
    for (let i = 0; i < signedUrlsResult.length; i++) {
        const item: SignedUrlResult = signedUrlsResult[i];
        const path = paths[i]; // Get the corresponding original path

        if (item.error) {
            console.error(`Failed to generate signed URL for path: ${path} in bucket '${bucketName}'. Error: ${item.error}`);
            // Throw an error specific to the failing path
            throw new Error(`${ERROR_GENERATING_SIGNED_URL} for path: ${path}. Reason: ${item.error}`);
        }

        if (!item.signedUrl) {
            // Handle cases where there's no error but also no URL (should be unlikely)
            console.error(`No signed URL returned for path: ${path} in bucket '${bucketName}', although no specific error was reported.`);
            throw new Error(`${ERROR_GENERATING_SIGNED_URL} - missing URL for path: ${path}`);
        }

        // Map the original path to the signed URL
        signedUrlMap.set(path, item.signedUrl);
    }

    return signedUrlMap;
} 

//   
/**
 * Generates signed URLs for a list of object paths within a Supabase storage bucket.
 * Throws an error if any URL generation fails.
 * @param supabase - The Supabase client instance.
 * @param bucketName - The name of the storage bucket.
 * @param paths - An array of object paths within the bucket.
 * @param expiresIn - The duration in seconds for which the signed URLs will be valid.
 * @returns A Promise resolving to a Map where keys are the original paths and values are the signed URLs.
 * @throws Throws an error if Supabase client fails to generate URLs or if any individual URL generation fails.
 */
export async function getSignedUrls(
    supabase: SupabaseClient,
    paths: string[],
): Promise<Map<string, string>> {
    if (paths.length === 0) {
        return new Map<string, string>(); // Return empty map if no paths
    }

    const signedUrlMap = new Map<string, string>();
    const pathsToFetch: string[] = [];
    const currentTime = Date.now(); // Get current time in milliseconds

    // 1. Check cache for valid, non-expired URLs
    for (const path of paths) {
        const cacheEntry = signedUrlCache.get(path);
        if (cacheEntry && cacheEntry.expiresAt > currentTime) {
            // Cache hit and not expired
            signedUrlMap.set(path, cacheEntry.signedUrl);
            // console.log(`Cache hit for path: ${path}`); // Optional: for debugging
        } else {
            // Cache miss or expired
            pathsToFetch.push(path);
            // Optional: Remove expired entry if it exists
            if (cacheEntry) {
                signedUrlCache.delete(path);
                // console.log(`Cache expired/removed for path: ${path}`); // Optional: for debugging
            }
        }
    }

    // 2. Fetch missing URLs if any
    if (pathsToFetch.length > 0) {
        // console.log(`Fetching ${pathsToFetch.length} URLs from Supabase:`, pathsToFetch); // Optional: for debugging
        const { data: signedUrlsResult, error: bulkUrlError } = await supabase
            .storage
            .from(SCREENSHOT_BUCKET)
            .createSignedUrls(pathsToFetch, SIGNED_URL_EXPIRY_SECONDS);

        if (bulkUrlError) {
            console.error(`Error generating signed URLs in bulk for bucket '${SCREENSHOT_BUCKET}':`, bulkUrlError);
            // Throw a specific error indicating bulk operation failure
            throw new Error(`${ERROR_GENERATING_SIGNED_URL}: Bulk operation failed.`);
        }

        if (!signedUrlsResult) {
            // This case should theoretically be covered by bulkUrlError, but added for robustness
            console.error(`No data returned for signed URLs batch for bucket '${SCREENSHOT_BUCKET}', but no error reported.`);
            throw new Error(`${ERROR_GENERATING_SIGNED_URL}: No data returned from Supabase.`);
        }

        // Calculate expiry time for the newly fetched URLs
        const expiresAt = currentTime + SIGNED_URL_EXPIRY_SECONDS * 1000;

        // 3. Process results, update the final map, and update the cache
        for (let i = 0; i < signedUrlsResult.length; i++) {
            const item: SignedUrlResult = signedUrlsResult[i];
            // The path corresponds to the path requested in pathsToFetch
            const path = pathsToFetch[i];

            if (item.error) {
                console.error(`Failed to generate signed URL for path: ${path} in bucket '${SCREENSHOT_BUCKET}'. Error: ${item.error}`);
                // Throw an error specific to the failing path, maintaining original behavior
                throw new Error(`${ERROR_GENERATING_SIGNED_URL} for path: ${path}. Reason: ${item.error}`);
            }

            if (!item.signedUrl) {
                // Handle cases where there's no error but also no URL (should be unlikely)
                console.error(`No signed URL returned for path: ${path} in bucket '${SCREENSHOT_BUCKET}', although no specific error was reported.`);
                throw new Error(`${ERROR_GENERATING_SIGNED_URL} - missing URL for path: ${path}`);
            }

            // Add the newly fetched URL to the result map
            signedUrlMap.set(path, item.signedUrl);

            // Add the newly fetched URL and its expiry time to the cache
            signedUrlCache.set(path, { signedUrl: item.signedUrl, expiresAt });
            // console.log(`Cached new URL for path: ${path}, expires at: ${new Date(expiresAt).toISOString()}`); // Optional: for debugging
        }
    } else {
        // console.log("All requested URLs were found in cache."); // Optional: for debugging
    }


    // Return the map containing both cached and newly fetched URLs
    return signedUrlMap;
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/utils.ts
================
import { clsx, type ClassValue } from "clsx"
import { twMerge } from "tailwind-merge"

export function cn(...inputs: ClassValue[]) {
  return twMerge(clsx(inputs))
}

// Helper: Parse output_text in JSON format safely
// Sample Input : output_text: '[\n' +
//     '  {\n' +
//     '    "component_name": "Batch Group",\n' +
//     `    "description": "Expandable section containing a titled batch with grouped images and metadata, such as 'main check', 'upload-clean-up-check', 'feature-deployment-test', or 'oma work'.",\n` +
//     '    "impact_on_user_flow": "Organizes multiple related image sets for streamlined review and access, improving navigation and batch management.",\n' +
//     '    "cta_type": null,\n' +
//     '    "is_reused_in_other_screens": true,\n' +
//     '    "likely_interaction_type": ["expand", "collapse", "scroll"],\n' +
//     '    "flow_position": "Batch Review - Navigation"\n' +
//     '  },\n' 
//     ']'
// Sample Output: [
//   {
//     component_name: 'Batch Group',
//     description: "Expandable section containing a titled batch with grouped images and metadata, such as 'main check', 'upload-clean-up-check', 'feature-deployment-test', or 'oma work'.",
//     impact_on_user_flow: 'Organizes multiple related image sets for streamlined review and access, improving navigation and batch management.',
//     cta_type: null,
//     is_reused_in_other_screens: true,
//     likely_interaction_type: [ 'expand', 'collapse', 'scroll' ],
//     flow_position: 'Batch Review - Navigation'
//   },
// ]
export function parseOutputText(outputText: string): any[] {
  try {
    return JSON.parse(outputText);
  } catch (error) {
    console.error("Failed to parse output_text JSON:", error);
    return [];
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/pages/api/anthropic.ts
================
import { NextApiRequest, NextApiResponse } from 'next';
import Anthropic from '@anthropic-ai/sdk';
import { TextBlock } from '@anthropic-ai/sdk/resources/messages';

// --- Constants ---
const ANTHROPIC_API_KEY = process.env.ANTHROPIC_API_KEY;
const DEFAULT_MODEL = "claude-3-haiku-20240307";
const MAX_TOKENS = 1024;
const IMAGE_PROMPT = "Describe this image.";
const ERROR_MISSING_KEY = "ANTHROPIC_API_KEY environment variable is not set.";
const ERROR_METHOD_NOT_ALLOWED = (method: string | undefined) => `Method ${method} Not Allowed`;
const ERROR_MISSING_IMAGE_URL = 'Missing imageUrl in request body';
const ERROR_INVALID_IMAGE_URL = 'Invalid imageUrl format';
const ERROR_ANTHROPIC_API = 'Anthropic API error';
const ERROR_INTERNAL_SERVER = 'Internal Server Error';

// --- Initialization ---
if (!ANTHROPIC_API_KEY) {
  throw new Error(ERROR_MISSING_KEY);
}
const anthropic = new Anthropic({ apiKey: ANTHROPIC_API_KEY });

// --- Types ---
interface RequestBody {
  imageUrl: string;
}

// --- Helper Functions ---

/**
 * Validates the request method and imageUrl presence/format.
 * Returns null if valid, or an error response object if invalid.
 */
function validateRequest(req: NextApiRequest): { status: number; json: { error: string } } | null {
  if (req.method !== 'POST') {
    return { status: 405, json: { error: ERROR_METHOD_NOT_ALLOWED(req.method) } };
  }

  const { imageUrl }: RequestBody = req.body;

  if (!imageUrl) {
    return { status: 400, json: { error: ERROR_MISSING_IMAGE_URL } };
  }

  try {
    new URL(imageUrl);
  } catch (_) {
    return { status: 400, json: { error: ERROR_INVALID_IMAGE_URL } };
  }

  return null; // Request is valid
}

/**
 * Calls the Anthropic API to get a description for the given image URL.
 */
async function getImageDescription(imageUrl: string): Promise<string> {
  const message = await anthropic.messages.create({
    model: DEFAULT_MODEL,
    max_tokens: MAX_TOKENS,
    messages: [
      {
        role: "user",
        content: [
          { type: "image", source: { type: "url", url: imageUrl } },
          { type: "text", text: IMAGE_PROMPT },
        ],
      },
    ],
  });

  // Extract and combine text content
  return message.content
    .filter((block): block is TextBlock => block.type === 'text')
    .map((block: TextBlock) => block.text)
    .join(' ');
}

/**
 * Handles errors, logs them, and returns an appropriate JSON response.
 */
function handleError(error: unknown): { status: number; json: { error: string } } {
  console.error(ERROR_ANTHROPIC_API, error);

  if (error instanceof Anthropic.APIError) {
    return { status: error.status || 500, json: { error: error.message } };
  }

  const errorMessage = error instanceof Error ? error.message : ERROR_INTERNAL_SERVER;
  return { status: 500, json: { error: errorMessage } };
}


// --- API Handler ---
export default async function handler(
  req: NextApiRequest,
  res: NextApiResponse
) {
  const validationError = validateRequest(req);
  if (validationError) {
    if (validationError.status === 405) {
        res.setHeader('Allow', ['POST']);
        return res.status(validationError.status).end(validationError.json.error);
    }
    return res.status(validationError.status).json(validationError.json);
  }

  const { imageUrl }: RequestBody = req.body; // Already validated, safe to extract

  try {
    const description = await getImageDescription(imageUrl);
    return res.status(200).json({ description });
  } catch (error: unknown) {
    const errorResponse = handleError(error);
    return res.status(errorResponse.status).json(errorResponse.json);
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/pages/api/batches.ts
================
import { NextApiRequest, NextApiResponse } from 'next'
import { supabase } from '@/lib/supabase'
import { PostgrestError } from '@supabase/supabase-js'
// import { SUPABASE_BUCKET_NAME } from '@/config'
import { getScreenshotPath,getSignedUrls } from '@/lib/supabaseUtils'

// --- Constants ---
const BATCH_TABLE = 'batch'

const ERROR_METHOD_NOT_ALLOWED = 'Method not allowed'
const ERROR_FETCHING_BATCHES = 'Failed to fetch batches'
const ERROR_INVALID_TIMESTAMP = (ts: string) => `Invalid timestamp format: ${ts}`
const ERROR_GENERATING_SIGNED_URL = 'Error generating signed URL'

// --- Types ---

interface Screenshot {
  screenshot_id: number
  screenshot_file_name: string
  screenshot_file_url: string
}

interface BatchData {
  batch_id: number
  batch_name: string
  batch_created_at: string // ISO timestamp string
  batch_status: string
  batch_analysis_type: string
  batch_master_prompt_runtime: number | null
  batch_total_inference_time: number | null
  batch_detected_elements_count: number | null
  screenshot: Screenshot[] // Assuming the relationship fetch returns this structure
}

// Define the shape of the transformed data returned by the API
interface SignedImage {
  id: string
  name: string
  url: string
}

interface EnrichedBatch {
  id: string
  name: string
  timestamp: Date
  status: string
  analysisType: string
  performance: {
    masterPromptRuntime: number
    totalInferenceTime: number
    detectedElementsCount: number
  }
  images: SignedImage[]
}

// --- Helper Functions ---

/**
 * Fetches batches and their associated screenshots from Supabase.
 */
async function fetchBatchesFromSupabase(): Promise<BatchData[]> {
  // Provide the table name and the expected return type structure to .from()
  const query = supabase
    .from(BATCH_TABLE)
    .select(`
      batch_id,
      batch_name,
      batch_created_at,
      batch_status,
      batch_analysis_type,
      batch_master_prompt_runtime,
      batch_total_inference_time,
      batch_detected_elements_count,
      screenshot (
        screenshot_id,
        screenshot_file_name,
        screenshot_file_url
      )
    `)
    .order('batch_created_at', { ascending: false })

  // Explicitly type the result based on the select query
  const { data, error } = await query as { data: BatchData[] | null, error: PostgrestError | null }

  if (error) {
    console.error('Supabase fetch error:', error)
    throw new Error(ERROR_FETCHING_BATCHES)
  }

  return data || []
}

/**
 * Extracts the storage path from a Supabase screenshot URL.
 */
// function getScreenshotPath(url: string): string {
//   console.log('Extracting storage path from screenshot URL:', url);
//   try {
//     const parsedUrl = new URL(url);
//     console.log('Parsed URL:', parsedUrl);
//     // Example path: /storage/v1/object/public/screenshot/batch_123/image.jpg
//     // We need the part after the bucket name: batch_123/image.jpg
//     const pathParts = parsedUrl.pathname.split('/');
//     console.log('Path parts:', pathParts);
//     const bucketNameIndex = pathParts.findIndex(part => part === SCREENSHOT_BUCKET);
//     console.log('Bucket name index:', bucketNameIndex);

//     if (bucketNameIndex === -1 || bucketNameIndex + 1 >= pathParts.length) {
//       console.error(`Bucket '${SCREENSHOT_BUCKET}' not found or path is incomplete in URL: ${url}`);
//       throw new Error(`Invalid screenshot URL format: ${url}`);
//     }
//     // Join the parts after the bucket name
//     const storagePath = pathParts.slice(bucketNameIndex + 1).join('/');
//     console.log('Extracted storage path:', storagePath);
//     return storagePath;
//   } catch (e) {
//     // Catch URL parsing errors as well
//     console.error(`Error parsing screenshot URL '${url}':`, e);
//     // Re-throw a more specific error or handle as needed
//     throw new Error(`Invalid screenshot URL format: ${url}`);
//   }
// }

/**
 * Transforms a raw batch object (matching SelectedBatchData) into the API response format.
 * Generates signed URLs for all screenshots in the batch using a single API call.
 */
async function enrichBatchWithSignedImages(batch: BatchData): Promise<EnrichedBatch> {
  const timestamp = new Date(batch.batch_created_at);
  if (isNaN(timestamp.getTime())) {
    console.error(ERROR_INVALID_TIMESTAMP(batch.batch_created_at));
    // Consider throwing a more specific error type if needed downstream
    throw new Error(ERROR_INVALID_TIMESTAMP(batch.batch_created_at));
  }

  let signedImages: SignedImage[] = [];
  const screenshotPaths = batch.screenshot.map(ss => getScreenshotPath(ss.screenshot_file_url));

  // Only generate signed URLs if there are paths to process
  if (screenshotPaths.length > 0) {
    try {
      // Call the new utility function to generate signed URLs
      const signedUrlMap = await getSignedUrls(
        supabase, // Pass the supabase client
        screenshotPaths,
      );

      // Transform screenshots using the generated map
      signedImages = batch.screenshot.map(ss => {
        const path = getScreenshotPath(ss.screenshot_file_url);
        const signedUrl = signedUrlMap.get(path);

        if (!signedUrl) {
          // This indicates an internal logic error if map generation was successful
          console.error(`Internal error: Could not find mapped signed URL for path: ${path}.`);
          // Throw an error as this shouldn't happen if generateSignedUrls succeeded
          throw new Error(`Internal error: Signed URL missing for path ${path}`);
        }

        // Directly create the SignedImage object here
        return {
          id: ss.screenshot_id.toString(),
          name: ss.screenshot_file_name,
          url: signedUrl,
        };
      });
    } catch (error) {
        // Catch errors specifically from generateSignedUrls
        console.error('Error generating signed URLs via utility function:', error);
        // Re-throw or handle the error appropriately. Throwing a general error for the batch.
        // The utility function already logs specific path errors.
        throw new Error(ERROR_GENERATING_SIGNED_URL);
    }
  }
  // If screenshotPaths was empty, signedImages will remain an empty array, which is correct.

  return {
    id: batch.batch_id.toString(),
    name: batch.batch_name,
    timestamp,
    status: batch.batch_status,
    analysisType: batch.batch_analysis_type,
    performance: {
      masterPromptRuntime: batch.batch_master_prompt_runtime ?? 0,
      totalInferenceTime: batch.batch_total_inference_time ?? 0,
      detectedElementsCount: batch.batch_detected_elements_count ?? 0,
    },
    images: signedImages,
  };
}

function validateMethod(req: NextApiRequest): { valid: boolean; error?: { status: number; message: string } } {
  if (req.method !== 'GET') {
    return { valid: false, error: { status: 405, message: ERROR_METHOD_NOT_ALLOWED } };
  }
  return { valid: true };
}

async function getEnrichedBatches(): Promise<EnrichedBatch[]> {
  const rawBatches = await fetchBatchesFromSupabase();
  return Promise.all(rawBatches.map(enrichBatchWithSignedImages));
}

function handleBatchesError(error: unknown, res: NextApiResponse): void {
  console.error(ERROR_FETCHING_BATCHES, error);
  // Check if the error message originates from the specific utility errors
  const message = error instanceof Error ? error.message : ERROR_FETCHING_BATCHES;
  let statusCode = 500; // Default to internal server error

  if (message.startsWith(ERROR_GENERATING_SIGNED_URL) || message.includes(ERROR_INVALID_TIMESTAMP(''))) {
      statusCode = 400; // Bad request for URL generation issues or invalid timestamps
  } else if (message === ERROR_FETCHING_BATCHES) {
      statusCode = 500; // Stick to 500 for general fetch errors
  }
  // Add more specific status code handling if needed based on error types/messages

  res.status(statusCode).json({ error: message });
}

// --- API Handler ---
export default async function handler(req: NextApiRequest, res: NextApiResponse) {
  const validation = validateMethod(req);
  if (!validation.valid) {
    res.setHeader('Allow', ['GET']);
    return res.status(validation.error!.status).json({ error: validation.error!.message });
  }

  try {
    const batches = await getEnrichedBatches();
    return res.status(200).json(batches);
  } catch (error) {
    handleBatchesError(error, res);
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/pages/api/process-batch.ts
================
import { NextApiRequest, NextApiResponse } from 'next';
import { supabase } from '@/lib/supabase';
import { BatchProcessingService } from '@/lib/services/batchProcessingService';

export default async function handler(req: NextApiRequest, res: NextApiResponse) {
  if (req.method !== 'POST') {
    return res.status(405).json({ message: 'Method not allowed' });
  }

  console.log('[BatchProcessing] Received batch processing request');
  try {
    const { batchId } = req.body;
    console.log(`[BatchProcessing] Processing batch ID: ${batchId}`);
    
    if (!batchId || isNaN(batchId)) {
      console.log(`[BatchProcessing] Invalid batch ID: ${batchId}`);
      return res.status(400).json({ message: 'Invalid batch ID' });
    }
    
    // Create the batch processing service with the server-side Supabase client
    console.log('[BatchProcessing] Initializing batch processing service');
    const batchProcessingService = new BatchProcessingService(supabase);
    
    // Start the batch processing
    console.log(`[BatchProcessing] Starting batch processing for ID: ${batchId}`);
    await batchProcessingService.start(batchId);
    console.log(`[BatchProcessing] Batch processing completed for ID: ${batchId}`);
    
    return res.status(200).json({ message: 'Batch processing started successfully' });
  } catch (error) {
    console.error('[BatchProcessing] Error processing batch:', error);
    
    return res.status(500).json({ 
      message: 'Failed to process batch',
      error: error instanceof Error ? error.message : 'Unknown error'
    });
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/pages/api/upload.ts
================
import type { NextApiRequest, NextApiResponse } from 'next'
import formidable, { File, Fields } from 'formidable'
// Remove unused image processing imports
// import { resizeAndPadImageBuffer, deleteFile } from '@/lib/image-processor'
// import fs from 'fs' // Keep fs if parseFormData needs it, or remove if not needed elsewhere. Check parseFormData usage.
// import { uploadImageToStorage } from '@/lib/storage' // Removed, handled by ScreenshotProcessor
import { supabase } from '@/lib/supabase'
import { SupabaseClient } from '@supabase/supabase-js'
// Import the new services using relative paths
import { ScreenshotProcessor } from '../../lib/services/imageServices/screenshotProcessor';
import { BatchProcessingService } from '../../lib/services/batchProcessingService';

// Keep ProcessedImage interface ONLY if still needed by parseFormData or other parts.
// If not, it can be removed as ScreenshotProcessor encapsulates its own processing details.
// interface ProcessedImage {
//  processedBlob: Blob;
//  filename: string;
//  processingTime?: number;
// }

const MAX_FILE_SIZE = 10 * 1024 * 1024 // 10MB

const formidableConfig = {
  keepExtensions: true,
  maxFileSize: MAX_FILE_SIZE,
  filter: (part: formidable.Part) => part.mimetype?.includes('image') ?? false,
}

export const config = {
  api: { bodyParser: false },
}

// Helper function to parse the form data
async function parseFormData(req: NextApiRequest): Promise<{ fields: Fields, files: formidable.Files }> {
  const form = formidable(formidableConfig)
  return new Promise((resolve, reject) => {
    form.parse(req, (err, fields, files) => {
      if (err) {
        // Provide clearer error context for formidable errors
        if (err.message.includes('maxFileSize')) {
          return reject(new Error(`File size exceeds the limit of ${MAX_FILE_SIZE / 1024 / 1024}MB`));
        }
        if (err.message.includes('filter')) {
          return reject(new Error('Only image files are allowed'));
        }
        return reject(err)
      }
      resolve({ fields, files })
    })
  })
}

// Removed processUploadedFile function - logic moved to ScreenshotProcessor

// Removed saveScreenshotRecord function - logic moved to ScreenshotProcessor


// --- Refactored Helper Functions ---

// 1. Validate Request (Method, Fields, Files)
function validateRequest(
  req: NextApiRequest,
  fields: Fields,
  files: formidable.Files
): { batchName: string; analysisType: string; uploadedFiles: File[]; error?: { status: number; message: string } } {
  if (req.method !== 'POST') {
    return { error: { status: 405, message: 'Method not allowed' }, batchName: '', analysisType: '', uploadedFiles: [] };
  }

  const batchName = fields.batchName?.[0];
  const analysisType = fields.analysisType?.[0];

  if (!batchName || !analysisType) {
    return { error: { status: 400, message: 'Missing required fields: batchName and analysisType' }, batchName: '', analysisType: '', uploadedFiles: [] };
  }

  const uploadedFiles = (Array.isArray(files.file) ? files.file : [files.file]).filter(Boolean) as File[];

  if (!uploadedFiles.length) {
    return { error: { status: 400, message: 'No files provided' }, batchName, analysisType, uploadedFiles: [] };
  }

  return { batchName, analysisType, uploadedFiles };
}

// 2. Create Batch Record in DB
async function createBatchRecord(
  batchName: string,
  analysisType: string,
  supabaseClient: SupabaseClient
): Promise<{ batch_id: number }> {
  const { data: batchData, error: batchError } = await supabaseClient
    .from('batch')
    .insert({
      batch_name: batchName,
      batch_status: 'uploading',
      batch_analysis_type: analysisType
    })
    .select('batch_id') // Only select the ID
    .single();

  if (batchError || !batchData) {
    console.error('Supabase batch insert error:', batchError);
    throw new Error('Failed to create batch record');
  }
  return batchData;
}

// Removed processAndSaveImages function - logic replaced by direct calls to ScreenshotProcessor
// Removed updateBatchStatus function - BatchProcessingService handles status updates post-upload


// 5. Handle Errors
function handleUploadError(error: unknown, res: NextApiResponse): void {
  console.error('Upload handler error:', error);
  const errorMessage = error instanceof Error ? error.message : 'An unexpected error occurred during upload.';
  // Specific checks for user input errors (file size, type) vs internal server errors
  const isUserInputError = error instanceof Error &&
                           (error.message.includes('limit') ||
                            error.message.includes('Only image files') ||
                            error.message.includes('Missing required fields') ||
                            error.message.includes('No files provided'));

  const statusCode = isUserInputError ? 400 : 500;
  
  // Also check for specific errors thrown by ScreenshotProcessor if needed
  if (error instanceof Error && (error.message.includes('Failed to upload') || error.message.includes('Failed to save screenshot record'))) {
    // Could potentially return a more specific status code like 502 Bad Gateway if storage/DB fails
    // For simplicity, sticking to 500 for internal server errors
  }

  res.status(statusCode).json({ error: errorMessage });
}


// --- Main Handler ---
export default async function handler(
  req: NextApiRequest,
  res: NextApiResponse
) {
  // Instantiate services
  const screenshotProcessor = new ScreenshotProcessor(supabase); // Use shared client
  const batchProcessingService = new BatchProcessingService(supabase); // Use shared client

  try {
    // 1. Parse Form Data
    const { fields, files } = await parseFormData(req);

    // 2. Validate Request
    const validationResult = validateRequest(req, fields, files);
    if (validationResult.error) {
      return res.status(validationResult.error.status).json({ error: validationResult.error.message });
    }
    const { batchName, analysisType, uploadedFiles } = validationResult;

    // 3. Create Batch Record
    const { batch_id: batchId } = await createBatchRecord(batchName, analysisType, supabase);

    // 4. Process and Save each image using ScreenshotProcessor
    const processingPromises = uploadedFiles.map(file => 
        screenshotProcessor.processAndSave(file, batchId)
    );
    const results = await Promise.allSettled(processingPromises);

    const successfulUploads: Array<{ name: string; url: string }> = [];
    const failedUploads: Array<{ file: string | null | undefined; reason: any }> = [];

    results.forEach((result, index) => {
      if (result.status === 'fulfilled') {
        successfulUploads.push(result.value);
      } else {
        // Log detailed error from ScreenshotProcessor
        console.error(`Failed to process file ${uploadedFiles[index]?.originalFilename}:`, result.reason);
        failedUploads.push({ 
            file: uploadedFiles[index]?.originalFilename,
            reason: result.reason instanceof Error ? result.reason.message : result.reason 
        });
      }
    });

    // 5. Handle partial failures - Update batch status if ALL files failed
    if (failedUploads.length === uploadedFiles.length && uploadedFiles.length > 0) {
        console.warn(`[Batch ${batchId}] All uploads failed. Setting status to 'upload_failed'.`);
        // Use BatchProcessingService's utility or direct Supabase call for this specific status
        try {
            await supabase.from('batch').update({ batch_status: 'upload_failed' }).eq('batch_id', batchId);
        } catch (statusError) {
            console.error(`[Batch ${batchId}] Failed to update status to 'upload_failed':`, statusError);
        }
        // Return an error response indicating complete failure
        return res.status(500).json({
            success: false,
            batchId: batchId,
            message: 'All file uploads failed. Batch marked as failed.',
            errors: failedUploads
        });
    }

    // 6. Log summary
    console.log(`[Batch ${batchId}] Upload complete. Successful: ${successfulUploads.length}, Failed: ${failedUploads.length}`);

    // 7. Kick off Batch Processing Asynchronously (DO NOT await this)
    if (successfulUploads.length > 0) {
        // Use setImmediate or a similar non-blocking mechanism if available in the environment
        // For Node.js environments (like Next.js API routes):
        setImmediate(() => {
            batchProcessingService.start(batchId).catch(err => {
                // Log error from the async process start, status handling is inside start()
                console.error(`[Batch ${batchId}] Error starting background processing:`, err);
            });
        });
        console.log(`[Batch ${batchId}] Background processing task scheduled.`);
    } else {
        // This case is handled by step 5 (all uploads failed)
        console.warn(`[Batch ${batchId}] No successful uploads, background processing not started.`);
    }

    // 8. Return Success Response (even if some files failed, as long as not all failed)
    // The response indicates the immediate outcome of the upload request.
    // The background processing handles the next stages.
    return res.status(200).json({
      success: true,
      batchId: batchId,
      message: failedUploads.length === 0
        ? 'Upload successful. Batch processing started.'
        : `Upload partially successful (${successfulUploads.length}/${uploadedFiles.length}). Batch processing started for successful uploads.`,
      files: successfulUploads, // Only return successfully processed files
      errors: failedUploads.length > 0 ? failedUploads : undefined // Optionally include errors
    });

  } catch (error) {
    // Use the existing error handler
    handleUploadError(error, res);
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/services/upload-service.ts
================
import { API_ENDPOINTS } from '@/lib/constants';

export const uploadFiles = async (
  files: File[],
  batchName: string,
  analysisType: string
): Promise<{ success: boolean; error?: string }> => {
  try {
    const formData = new FormData();
    files.forEach(file => formData.append('file', file));
    formData.append('batchName', batchName);
    formData.append('analysisType', analysisType);

    const response = await fetch(API_ENDPOINTS.UPLOAD, {
      method: 'POST',
      body: formData,
    });

    if (!response.ok) {
      const errorData = await response.json();
      throw new Error(errorData.error || 'Upload failed');
    }
    return { success: true };
  } catch (error) {
    console.error('Upload error:', error);
    return { 
      success: false, 
      error: error instanceof Error ? error.message : 'Unknown error occurred' 
    };
  }
};

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/types/BatchProcessingScreenshot.ts
================
import { Buffer } from 'buffer';

export interface BatchProcessingScreenshot {
    screenshot_id: number;
    batch_id: number;
    screenshot_file_name: string;
    screenshot_file_url: string; // URL like https://<...>/public/<bucket>/<path>
    screenshot_processing_status: string;
    screenshot_processing_time: string;
    screenshot_created_at: string;
    screenshot_signed_url?: string | null;
    screenshot_bucket_path?: string | null;
    // screenshot_image_blob?: Blob | null;
    // screenshot_image_base64?: string | null; // Base64 encoded image with data URI prefix
    screenshot_image_buffer?: Buffer | null; // Raw buffer data for image processing
  }

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/types/Component.ts
================
export interface Component {
  component_id: number;
  screenshot_id: number;
  component_name: string;
  component_description?: string | null;
  component_cta_type?: 'primary' | 'secondary' | 'informative' | null;
  component_reusable: boolean;
  component_x_min?: number | null; // NUMERIC
  component_y_min?: number | null; // NUMERIC
  component_x_max?: number | null; // NUMERIC
  component_y_max?: number | null; // NUMERIC
  component_extraction_model?: string | null;
  component_extraction_time?: number | null; // NUMERIC
  component_extraction_input_tokens?: number | null; // INTEGER
  component_extraction_output_tokens?: number | null; // INTEGER
  component_extraction_cost?: number | null; // NUMERIC
  component_status: 'pending' | 'extracted' | 'error';
  component_created_at: string; // TIMESTAMPTZ
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/types/DetectionResult.ts
================
interface ElementDetectionItem {
  label: string;
  description: string;
  bounding_box: {
    x_min: number;
    y_min: number;
    x_max: number;
    y_max: number;
  };
  status: 'Detected' | 'Not Detected' | 'Error' | 'Overwrite';
  // vlm_model: string; // Track which model provided the detection
  element_inference_time?: number; // Time taken for this specific element
  accuracy_score?: number; // Optional: To be added later
  suggested_coordinates?: { x_min: number; y_min: number; x_max: number; y_max: number };
  hidden?: boolean;
  explanation?: string;
  element_metadata_extraction?: string; // Optional: To be added later
}

interface ComponentDetectionResult {
  screenshot_id: number;
  component_name: string; // Top-level category/component name
  annotated_image_object: Buffer; // The rendered image buffer for this component
  original_image_object?: Buffer; // The original image buffer before any annotations
  annotated_image_url?: string; // To be populated after upload
  screenshot_url?: string; // URL of the original screenshot for debugging/audit
  component_description: string; // Maybe derived from element descriptions or passed in
  detection_status: 'success' | 'partial' | 'failed'; // Overall status for this component
  inference_time: number; // Total time for this component's elements
  elements: ElementDetectionItem[];
  component_ai_description?: string;
  component_metadata_extraction?: string;
}

// Export the interfaces
export type { ComponentDetectionResult, ElementDetectionItem };
