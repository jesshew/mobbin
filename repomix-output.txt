This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-05-20T10:04:23.080Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
/
  Users/
    jess/
      Desktop/
        personal git/
          mobbin/
            formobbin/
              components/
                landing/
                  features.tsx
                  hero.tsx
                  image-comparison-carousel.tsx
                  pain-point-section.tsx
                  solution-section.tsx
                  ui-details-carousel.tsx
              lib/
                prompt/
                  AccuracyValidationPrompts.ts
                  AnchorElementsPrompts.ts
                  ExtractElementsPrompts.ts
                  ExtractionPrompts.ts
                  MetadataExtractionPrompts.ts
                  prompts.ts
                services/
                  ai/
                    ClaudeAIService.ts
                    MoondreamDetectionService.js
                    MoondreamVLService.js
                    OpenAIService.js
                  imageServices/
                    BoundingBoxService.js
                    ImageFetchingService.ts
                    ImageProcessor.ts
                    ScreenshotProcessor.ts
                  AccuracyValidationService.ts
                  BatchAnalyticsService.ts
                  BatchComponentLoaderService.ts
                  BatchProcessingService.ts
                  DatabaseService.ts
                  MetadataExtractionService.ts
                  ParallelAnnotationService.ts
                  ParallelExtractionService.ts
                  SaveAnnotationService.ts

================================================================
Repository Files
================================================================

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/components/landing/features.tsx
================
import { InformativeInfoGraphic,cardData} from './render-info-graphics';
// More informativeand self-explanatory infographic component for each card

// Main component for displaying feature cards
export default function FeatureCards() {
  // Data for each feature card
  
  return (
    <>
      {/* Section for feature highlights */}
      <div className="max-w-5xl mx-auto mt-24 px-4 sm:px-6 lg:px-8">
        {/* Section header */}
        <div className="text-center mb-12">
          <div className="inline-flex items-center justify-center px-4 py-1.5 mb-4 rounded-full bg-indigo-50 border border-indigo-100 text-indigo-600 font-medium text-sm">
            <svg xmlns="http://www.w3.org/2000/svg" className="w-4 h-4 mr-2" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round">
              <circle cx="11" cy="11" r="8"></circle>
              <line x1="21" y1="21" x2="16.65" y2="16.65"></line>
            </svg>
            <span>Project Highlights</span>
          </div>
          <h2 className="text-3xl md:text-4xl font-bold text-gray-900 mb-4">How The Pipeline Works</h2>
          <p className="text-xl text-gray-600 max-w-3xl mx-auto">Leveraging cutting-edge ML technology to transform UI screenshots into detailed annotations</p>
        </div>
        
        {/* Grid layout for feature cards */}
        <div className="grid grid-cols-1 gap-6">
          {cardData.map((card, index) => (
            // Individual feature card container
            <div 
              key={index} 
              className="bg-white rounded-2xl border border-gray-100 shadow-lg hover:shadow-xl transition overflow-hidden group"
            >
              {/* Gradient bar at the top of the card */}
              <div className={`h-2 bg-gradient-to-r ${card.gradient}`}></div>
              {/* Card content area */}
              <div className="p-6">
                {/* Flex container for card content, responsive layout */}
                <div className="flex flex-col md:flex-row gap-6">
                  {/* Left side: Icon, title, and description */}
                  <div className="w-full md:w-1/2">
                    <div className="flex items-center mb-4">
                      <div className={`w-10 h-10 ${card.iconBg} rounded-lg flex items-center justify-center group-hover:${card.iconHoverBg} transition`}>
                        {card.icon}
                      </div>
                      <h3 className="text-xl font-semibold text-gray-900 ml-3">{card.title}</h3>
                    </div>
                    <p className="text-gray-600">{card.description}</p>
                  </div>
                  {/* Right side: Informative infographic */}
                  <div className="w-full md:w-1/2">
                    <InformativeInfoGraphic type={card.type} />
                  </div>
                </div>
              </div>
            </div>
          ))}
        </div>
      </div>
    </>
  );
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/components/landing/hero.tsx
================
import React, { useState, useRef } from 'react';
import { Presentation, ClipboardCheck, Clock, UserCheck, Layers, Cpu, ScanEye, GitMerge, XCircle, CheckCircle2, Users, AlertTriangle, TrendingDown, Search } from 'lucide-react';
import { motion } from 'framer-motion';
import { AuroraText } from '../magicui/aurora-text';
import GridPatternBackground from './grid-pattern-background';
import FeatureCards from './features';
import ImageComparisonCarousel from './image-comparison-carousel';
import PhoneShowcase from './phone-showcase';
import Link from 'next/link';
import { TechStackMarquee } from '../magicui/tech-stack-marquee';
import PainPointSection from './pain-point-section';
import SolutionSection from './solution-section';
import UIDetailsCarousel from './ui-details-carousel';

// Constants for the MLLMs text circling animation
const MLLMS_ANIMATION_DURATION = 1.25; // Duration of one animation cycle in seconds
const MLLMS_ANIMATION_TOTAL_CYCLE_DURATION = 5; // Total time for one cycle including delay in seconds
const MLLMS_ANIMATION_REPEAT_DELAY = MLLMS_ANIMATION_TOTAL_CYCLE_DURATION - MLLMS_ANIMATION_DURATION; // Delay before animation repeats in seconds

// Animation variants for the impact stats
const statsVariants = {
  hidden: { opacity: 0, y: 20 },
  visible: (i: number) => ({
    opacity: 1,
    y: 0,
    transition: {
      delay: i * 0.2,
      duration: 0.6,
      ease: "easeOut"
    }
  })
};

// Animation variants for process flow
const processVariants = {
  hidden: { opacity: 0 },
  visible: {
    opacity: 1,
    transition: {
      staggerChildren: 0.3
    }
  }
};

const processItemVariants = {
  hidden: { opacity: 0, x: -20 },
  visible: {
    opacity: 1,
    x: 0,
    transition: {
      duration: 0.5
    }
  }
};

// Pain point card component
const PainPointCard = ({ icon, title, description }: { icon: React.ReactNode, title: string, description: string }) => {
  return (
    <div className="bg-white p-5 rounded-xl shadow-md border border-gray-100 hover:shadow-lg transition duration-300">
      <div className="mb-3 text-indigo-600">{icon}</div>
      {/* Title uses text-xl for consistency with other card-like titles */}
      <h3 className="text-xl font-semibold text-gray-800 mb-2">{title}</h3>
      {/* Description uses text-base for consistency */}
      <p className="text-gray-600 text-base">{description}</p>
    </div>
  );
};

// Solution step component
const SolutionStep = ({ number, title, description }: { number: string, title: string, description: string }) => {
  return (
    <motion.div 
      className="flex items-start gap-4 mb-6"
      variants={processItemVariants}
    >
      <div className="flex-shrink-0 w-8 h-8 rounded-full bg-indigo-600 flex items-center justify-center text-white font-bold">
        {number}
      </div>
      <div>
        {/* Title uses text-xl and font-semibold for consistency */}
        <h3 className="text-xl font-semibold text-gray-900 mb-1">{title}</h3>
        {/* Description uses text-base for consistency */}
        <p className="text-gray-600 text-base">{description}</p>
      </div>
    </motion.div>
  );
};

const Hero = () => {
  const [activeTab, setActiveTab] = useState('problem');
  const featuresRef = useRef<HTMLDivElement>(null);

  // Function to scroll to features section
  const scrollToFeatures = () => {
    featuresRef.current?.scrollIntoView({ behavior: 'smooth' });
  };

  return (
    <div className="relative w-full overflow-hidden py-20 bg-gradient-to-b from-indigo-50/50 to-white">
      {/* Enhanced Background */}
      <GridPatternBackground />
      
      {/* Decorative elements */}
      <div className="absolute top-0 left-0 w-64 h-64 bg-blue-100 rounded-full blur-3xl opacity-20 -translate-x-1/2 -translate-y-1/2"></div>
      <div className="absolute bottom-0 right-0 w-96 h-96 bg-indigo-100 rounded-full blur-3xl opacity-30 translate-x-1/4 translate-y-1/4"></div>
      
      <div className="container relative mx-auto px-6 lg:px-8">
        {/* Hero content */}
        <div className="flex flex-col items-center justify-center text-center max-w-4xl mx-auto mb-12">
          {/* Badge text size kept as text-sm, appropriate for a badge */}
          <div className="inline-flex items-center justify-center px-4 py-1.5 mb-6 rounded-full bg-indigo-50 border border-indigo-100 text-indigo-600 font-medium text-sm">
            <Presentation className="w-4 h-4 mr-2" />
            <span>Mini Research on Zero Shot Prompting</span>
          </div>
          
          {/* Hero Slogan - UNTOUCHED as per instructions */}
          <h1 className="text-5xl md:text-7xl font-bold tracking-tight text-gray-900 mb-8 leading-tight">
            <span className="mr-2">Reimagining</span>
            <AuroraText className="font-extrabold">UX Annotation</AuroraText>
            <div className="inline-flex items-center mt-2">
              <span className="mr-2">with</span>
              <span className="ml-2 relative">
                <span className="bg-gradient-to-r from-indigo-600 to-blue-500 bg-clip-text text-transparent relative z-10">MLLMs</span>
                <svg
                  viewBox="0 0 130 50"
                  fill="none"
                  className="absolute -left-2 -right-2 -top-2 -bottom-1 w-full h-full"
                >
                  <motion.path
                    initial={{ pathLength: 0 }}
                    animate={{ pathLength: 1 }}
                    transition={{
                      duration: MLLMS_ANIMATION_DURATION,
                      ease: "easeInOut",
                      repeat: Infinity,
                      repeatType: "loop",
                      repeatDelay: MLLMS_ANIMATION_REPEAT_DELAY,
                    }}
                    d="M64.5 1C48.5 7.5 2.5 4.5 1 22.5C-0.5 40.5 15 48.5 58.5 47.5C102 46.5 145 47 125.5 20C103.5 -10.5 43.5 15 26 1"
                    stroke="#818cf8"
                    strokeWidth="3"
                  />
                </svg>
              </span>
            </div>
          </h1>
          
          {/* Hero sub-paragraph - UNTOUCHED as per instructions */}
          <p className="text-xl text-gray-600 mb-8 max-w-2xl mx-auto leading-relaxed">
            Transforming UI screenshots into structured UX annotations using language and vision models —
            <span className="font-semibold text-indigo-700"> shifting humans from tedious annotators to strategic reviewers.</span>
          </p>
        
          {/* Buttons text size defaults to browser/Tailwind base, which is fine */}
          <div className="flex flex-wrap gap-4 justify-center mb-8">
            <Link href="/gallery" className="px-8 py-3 bg-indigo-600 hover:bg-indigo-700 text-white font-medium rounded-full transition shadow-lg shadow-indigo-600/20 hover:shadow-indigo-600/30">
              Explore Demo Results
            </Link>
            <button 
              onClick={scrollToFeatures} 
              className="px-8 py-3 bg-white hover:bg-gray-50 text-indigo-600 font-medium rounded-full border border-gray-200 transition shadow-sm hover:shadow-md"
            >
              Learn More
            </button>
          </div>
        </div>

        {/* <PhoneShowcase /> */}

        {/* Image comparison showcase */}
        <div className="mb-16">
          <ImageComparisonCarousel />
        </div>
        {/* Image comparison showcase */}
        <div className="mb-16">
          <UIDetailsCarousel />
        </div>

         {/* Problem and Solution Section */}
         <div className="max-w-5xl mx-auto mb-16 space-y-12"> {/* Unified width with other sections */}
            {/* Section header */}
            <div className="text-center mb-10 sm:mb-12 max-w-3xl mx-auto">
              <div className="inline-flex items-center justify-center px-3 sm:px-4 py-1.5 mb-4 rounded-full bg-indigo-50 border border-indigo-100 text-indigo-600 font-medium text-xs sm:text-sm">
                <Search className="w-4 h-4 mr-2" />
                <span>Background Statement</span>
              </div>
              <h2 className="text-2xl sm:text-3xl md:text-4xl font-bold text-gray-900 mb-3 sm:mb-4">Problem & Solution</h2>
              <p className="text-base sm:text-lg md:text-xl text-gray-600 max-w-3xl mx-auto">
              What is the problem we are trying to bridge?
              </p>
            </div>
          <PainPointSection />
          <SolutionSection />
        </div>

        <TechStackMarquee/>

        {/* Feature cards section with ref */}
        <div ref={featuresRef}>
          <FeatureCards />
        </div>
      </div>
    </div>
  );
};

export default Hero;

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/components/landing/image-comparison-carousel.tsx
================
import React, { useState, useRef, useEffect } from 'react';
import Image from 'next/image';
import { ChevronLeft, ChevronRight, Image as ImageIcon, GitCompareArrows } from 'lucide-react';

const imageData = [
  {
    id: 'nike',
    title: 'Nike App',
    beforeSrc: '/results/nike_before.jpg',
    afterSrc: '/results/nike_after.png',
    stats: '18 Elements, 6 Components, detected and pre-annotated in 90seconds',
  },
  {
    id: 'duolingo',
    title: 'Duolingo Notifications',
    beforeSrc: '/results/duolingo_before.jpeg',
    afterSrc: '/results/duolingo_after.png',
    stats: '14 Elements, 4 Components, detected and pre-annotated in 60seconds',

  },
  {
    id: 'topup',
    title: 'Mobile Top-Up Page',
    beforeSrc: '/results/topup_before.png',
    afterSrc: '/results/topup_after.png',
    stats: '14 Elements, 5 Components, detected and pre-annotated in 60seconds',
  },
  {
    id: 'wpay',
    title: 'Payment Page',
    beforeSrc: '/results/wpay_before.png',
    afterSrc: '/results/wpay_after.png',
    stats: '18 Elements, 6 Components, detected and pre-annotated in 90seconds',
  },
];



// Component to display a single pair of before/after images with a slider
const CarouselSlideDisplay = ({ title, beforeSrc, afterSrc, stats }: { title: string; beforeSrc: string; afterSrc: string; stats: string }) => {
  const [sliderPosition, setSliderPosition] = useState(50); // Initial position at 50%
  const imageContainerRef = useRef<HTMLDivElement>(null);

  const handleSliderChange = (event: React.ChangeEvent<HTMLInputElement>) => {
    setSliderPosition(Number(event.target.value));
  };

  return (
    <div className="bg-white rounded-2xl border border-slate-200 shadow-xl p-4 sm:p-6 md:p-8">
      <h3 className="text-xl sm:text-2xl font-bold text-gray-800 mb-6 text-center tracking-tight">{title}</h3>
      
      <div 
        ref={imageContainerRef} 
        className="relative w-full h-[300px] sm:h-[400px] md:h-[500px] lg:h-[550px] rounded-lg overflow-hidden select-none group border border-slate-300 shadow-inner bg-slate-100"
      >
        {/* Before Image (Bottom Layer) */}
        <div className="absolute inset-0">
          <Image 
            src={beforeSrc} 
            alt={`${title} - Before`} 
            layout="fill" 
            objectFit="contain"
            className="rounded-lg"
            draggable={false}
            priority // Prioritize loading current slide images
          />
          <span className="absolute top-2 left-2 bg-black/60 text-white text-xs sm:text-sm px-2 py-1 rounded-md font-semibold">BEFORE</span>
        </div>

        {/* After Image (Top Layer, Clipped) */}
        <div 
          className="absolute inset-0 overflow-hidden"
          style={{ clipPath: `inset(0 ${100 - sliderPosition}% 0 0)` }}
        >
          <Image 
            src={afterSrc} 
            alt={`${title} - After`} 
            layout="fill" 
            objectFit="contain"
            className="rounded-lg"
            draggable={false}
            priority
          />
          <span className="absolute top-2 right-2 bg-black/60 text-white text-xs sm:text-sm px-2 py-1 rounded-md font-semibold">AFTER</span>
        </div>

        {/* Slider Control Line */}
        <div 
          className="absolute top-0 bottom-0 w-1 bg-white/80 shadow-md cursor-ew-resize pointer-events-none z-20 group-hover:bg-white transition-colors duration-150"
          style={{ left: `${sliderPosition}%`, transform: 'translateX(-50%)' }}
        >
          <div className="absolute top-1/2 left-1/2 -translate-x-1/2 -translate-y-1/2 w-8 h-8 bg-white rounded-full shadow-xl border-2 border-indigo-500 flex items-center justify-center opacity-0 group-hover:opacity-100 transition-opacity duration-150">
            <GitCompareArrows size={18} className="text-indigo-600" />
          </div>
        </div>

        {/* Range Input for Slider */}
        <input 
          type="range" 
          min="0" 
          max="100" 
          value={sliderPosition}
          onChange={handleSliderChange}
          className="absolute left-0 top-1/2 -translate-y-1/2 w-full h-16 opacity-0 cursor-ew-resize z-10"
          aria-label={`Compare ${title} before and after`}
        />
      </div>
      <p className="text-center text-sm text-gray-500 mt-4">Slide the bar to compare before and after versions.</p>
      
      {/* Stats Section */}
      {stats && (() => {
        const parts = stats.split(',').map(s => s.trim());
        const elementsMatch = parts.find(p => p.toLowerCase().includes('elements'))?.match(/(\d+)\s*Elements/i);
        const componentsMatch = parts.find(p => p.toLowerCase().includes('components'))?.match(/(\d+)\s*Components/i);
        const timeMatch = parts.find(p => p.toLowerCase().includes('seconds'))?.match(/(\d+)seconds/i);

        const elements = elementsMatch ? elementsMatch[1] : null;
        const components = componentsMatch ? componentsMatch[1] : null;
        const time = timeMatch ? timeMatch[1] : null;

        // Only render the section if at least one stat is parseable
        if (!elements && !components && !time) {
          return null;
        }
        
        return (
          <div className="mt-6 pt-6 border-t border-slate-200">
            <div className="grid grid-cols-1 sm:grid-cols-3 gap-4 text-center">
              {elements && (
                <div>
                  <p className="text-2xl sm:text-3xl font-bold text-indigo-600">{elements}</p>
                  <p className="text-xs sm:text-sm text-gray-500">Elements Detected</p>
                </div>
              )}
              {components && (
                <div>
                  <p className="text-2xl sm:text-3xl font-bold text-indigo-600">{components}</p>
                  <p className="text-xs sm:text-sm text-gray-500">Components Identified</p>
                </div>
              )}
              {time && (
                <div>
                  <p className="text-2xl sm:text-3xl font-bold text-indigo-600">{time}s</p>
                  <p className="text-xs sm:text-sm text-gray-500">Analysis Duration</p>
                </div>
              )}
            </div>
          </div>
        );
      })()}
    </div>
  );
};

export default function ImageComparisonCarousel() {
  const [currentIndex, setCurrentIndex] = useState(0);

  const handlePrev = () => {
    setCurrentIndex((prevIndex) => (prevIndex === 0 ? imageData.length - 1 : prevIndex - 1));
  };

  const handleNext = () => {
    setCurrentIndex((prevIndex) => (prevIndex === imageData.length - 1 ? 0 : prevIndex + 1));
  };

  const handleDotClick = (index: number) => {
    setCurrentIndex(index);
  };

  if (imageData.length === 0) {
    return null;
  }

  const currentSlide = imageData[currentIndex];

  return (
    <div className="max-w-5xl mx-auto mt-20 sm:mt-24 px-4 sm:px-6 lg:px-8">
      {/* Section header */}
      <div className="text-center mb-10 sm:mb-12">
        <div className="inline-flex items-center justify-center px-3 sm:px-4 py-1.5 mb-4 rounded-full bg-indigo-50 border border-indigo-100 text-indigo-600 font-medium text-xs sm:text-sm">
          <ImageIcon className="w-4 h-4 mr-2" />
          <span>UI Element Detection & UX Annotation</span>
        </div>
        <h2 className="text-2xl sm:text-3xl md:text-4xl font-bold text-gray-900 mb-3 sm:mb-4">Before & After Showcase</h2>
        <p className="text-base sm:text-lg md:text-xl text-gray-600 max-w-3xl mx-auto">
        See the results with a side-by-side comparison.
        </p>
      </div>

      {/* Carousel Body */}
      <div className="relative">
        <CarouselSlideDisplay
          key={currentSlide.id} // Add key here to reset slider state when slide changes
          title={currentSlide.title}
          beforeSrc={currentSlide.beforeSrc}
          afterSrc={currentSlide.afterSrc}
          stats={currentSlide.stats}
        />

        {/* Navigation Buttons */}
        {imageData.length > 1 && (
          <>
            <button
              onClick={handlePrev}
              className="absolute left-0 sm:-left-3 md:-left-5 top-1/2 transform -translate-y-1/2 bg-white hover:bg-indigo-50 text-indigo-600 p-2.5 sm:p-3 rounded-full shadow-md border border-slate-300 hover:border-indigo-300 transition-all duration-150 ease-in-out z-30"
              aria-label="Previous slide"
            >
              <ChevronLeft size={22} />
            </button>
            <button
              onClick={handleNext}
              className="absolute right-0 sm:-right-3 md:-right-5 top-1/2 transform -translate-y-1/2 bg-white hover:bg-indigo-50 text-indigo-600 p-2.5 sm:p-3 rounded-full shadow-md border border-slate-300 hover:border-indigo-300 transition-all duration-150 ease-in-out z-30"
              aria-label="Next slide"
            >
              <ChevronRight size={22} />
            </button>
          </>
        )}
      </div>

      {/* Dot Indicators */}
      {imageData.length > 1 && (
        <div className="flex justify-center space-x-2 sm:space-x-2.5 mt-8 sm:mt-10">
          {imageData.map((_, index) => (
            <button
              key={index}
              onClick={() => handleDotClick(index)}
              className={`w-2 h-2 sm:w-2.5 sm:h-2.5 rounded-full transition-all duration-150 ease-in-out ${
                currentIndex === index ? 'bg-indigo-600 scale-125' : 'bg-slate-300 hover:bg-slate-400'
              }`}
              aria-label={`Go to slide ${index + 1}`}
            />
          ))}
        </div>
      )}
    </div>
  );
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/components/landing/pain-point-section.tsx
================
import React from 'react';
import { Clock, Users, AlertTriangle, TrendingDown } from 'lucide-react';

// Card for each pain point
const PainPointCard = ({ icon, title, description }: { icon: React.ReactNode, title: string, description: string }) => (
  <div className="bg-white p-5 rounded-xl shadow-md border border-gray-100 hover:shadow-lg transition duration-300">
    <div className="mb-3 text-indigo-600">{icon}</div>
    <h3 className="text-xl font-semibold text-gray-800 mb-2">{title}</h3>
    <p className="text-gray-600 text-base">{description}</p>
  </div>
);

const PainPointSection = () => (
  <div className="max-w-5xl mx-auto px-4 sm:px-6 lg:px-8">
    <div className="bg-white rounded-xl shadow-xl border border-gray-200 p-6 md:p-8">
      <h2 className="text-2xl font-bold text-gray-800 mb-8 text-center">The Pain Point: Manual UX Annotation</h2>
      <div className="grid grid-cols-1 sm:grid-cols-2 gap-6">
        <PainPointCard
          icon={<Clock className="w-8 h-8 text-red-500" />}
          title="Painfully Slow"
          description="Manual bounding boxes & metadata tagging devour design hours, delaying projects."
        />
        <PainPointCard
          icon={<Users className="w-8 h-8 text-orange-500" />} 
          title="Highly Inconsistent"
          description="Varied annotator styles lead to inconsistencies."
        />
        <PainPointCard
          icon={<AlertTriangle className="w-8 h-8 text-yellow-500" />} 
          title="Error-Prone Process"
          description="Repetitive manual tasks increase human mistakes in labeling and classification."
        />
        <PainPointCard
          icon={<TrendingDown className="w-8 h-8 text-purple-500" />} 
          title="Impossible to Scale"
          description="Manual workflows bottleneck innovation and can't match rapid design iterations."
        />
      </div>
    </div>
  </div>
);

export default PainPointSection;

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/components/landing/solution-section.tsx
================
import React from 'react';
import { ScanEye, Cpu, XCircle, CheckCircle2 } from 'lucide-react';

const SolutionSection = () => (
  <div className="max-w-5xl mx-auto px-4 sm:px-6 lg:px-8">
    <div className="bg-white rounded-xl shadow-xl border text-gray-800 border-gray-200 p-6 md:p-8">
      <h2 className="text-2xl font-bold text-gray-800 mb-8 text-center">The Solution: AI-Powered Automation</h2>
      <div className="flex flex-col lg:flex-row gap-8 items-stretch">
        <div className="lg:w-2/4 space-y-6">
          {/* Automated UI Detection */}
          <div className="flex items-start space-x-4 p-4 bg-indigo-50 rounded-lg">
            <div className="flex-shrink-0 w-12 h-12 bg-indigo-500 rounded-full flex items-center justify-center shadow-md">
              <ScanEye className="w-6 h-6 text-white" />
            </div>
            <div>
              <h4 className="font-semibold text-xl text-gray-700">Automated UI Detection</h4>
              <p className="text-gray-600 text-base leading-relaxed">
                AI detects and identifies UI elements in your screenshots.
              </p>
            </div>
          </div>

          {/* Predrawn Bounding Box */}
          <div className="flex items-start space-x-4 p-4 bg-indigo-50 rounded-lg">
            <div className="flex-shrink-0 w-12 h-12 bg-indigo-500 rounded-full flex items-center justify-center shadow-md">
              {/* Using ScanEye icon for visual consistency, can be replaced with a more fitting icon if desired */}
              <ScanEye className="w-6 h-6 text-white" />
            </div>
            <div>
              <h4 className="font-semibold text-xl text-gray-700">Predrawn Bounding Box</h4>
              <p className="text-gray-600 text-base leading-relaxed">
                Vision Language Model pre-draws UI element boundaries based on AI-extracted descriptions, accelerating the annotation process.
              </p>
            </div>
          </div>

          {/* Component Annotation */}
          <div className="flex items-start space-x-4 p-4 bg-indigo-50 rounded-lg">
            <div className="flex-shrink-0 w-12 h-12 bg-indigo-500 rounded-full flex items-center justify-center shadow-md">
              <Cpu className="w-6 h-6 text-white" />
            </div>
            <div>
              <h4 className="font-semibold text-xl text-gray-700">Component Annotation</h4>
              <p className="text-gray-600 text-base leading-relaxed">
                LLM models grasp overall component function and context <span className="font-semibold text-indigo-600">without explicit training</span>.
              </p>
            </div>
          </div>
        </div>
        <div className="lg:w-2/4 bg-gray-50 p-6 rounded-xl border border-gray-200 flex flex-col justify-center">
          <h3 className="font-bold text-xl text-gray-800 mb-4 text-center">Impact on Workflow</h3>
          <p className="text-sm text-gray-500 mb-6 italic text-center">
            "Empower designers to create, not just catalogue. Let AI handle the heavy lifting."
          </p>
          <div className="mb-6 p-4 bg-red-50 rounded-lg border border-red-200 shadow-inner">
            <div className="flex items-center mb-2">
              <XCircle className="w-7 h-7 text-red-500 mr-3 flex-shrink-0" />
              <div className="font-semibold text-xl text-red-500">Before: The Annotator</div>
            </div>
            <p className="text-base text-red-500 leading-relaxed">
              Bogged down by tedious, repetitive clicking. Drained by manual data entry.
            </p>
          </div>
          <div className="p-4 bg-green-50 rounded-lg border border-green-200 shadow-inner">
            <div className="flex items-center mb-2">
              <CheckCircle2 className="w-7 h-7 text-green-500 mr-3 flex-shrink-0" />
              <div className="font-semibold text-xl text-green-500">After: The Strategist</div>
            </div>
            <p className="text-base text-green-500 leading-relaxed">
              Elevated to reviewer. Focused on UX quality & insights. Driving innovation at speed.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
);

export default SolutionSection;

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/components/landing/ui-details-carousel.tsx
================
import React, { useState } from 'react';
import Image from 'next/image';
import { ChevronLeft, ChevronRight, Image as ImageIcon } from 'lucide-react';

// Carousel data: each slide highlights a subtle UI detail
const uiDetailsData = [
  {
    id: 'duolingo-speech-bubble',
    title: 'Building Trust with Friendly Prompts',
    imageSrc: '/details/duolingo_after_detail1.png',
    alt: 'Duolingo speech bubble UI',
    caption: 'A candid speech bubble reduces friction and communicates trust, making users feel comfortable and understood during onboarding.'
  },
  {
    id: 'duolingo-allow-arrow',
    title: 'Gentle Guidance for User Choices',
    imageSrc: '/details/duolingo_after_detail2.png',
    alt: 'Duolingo allow notifications arrow UI',
    caption: 'A small arrow subtly nudges users toward the recommended action—"Allow Notifications"—helping them make the right choice without pressure.'
  },
  {
    id: 'topup-fee-transparency',
    title: 'Clear Communication for Informed Decisions',
    imageSrc: '/details/top-up-success.png',
    alt: 'Mobile top-up fee detail UI',
    caption: 'Specifying which virtual card is being topped up eliminates confusion and builds trust in the transaction process.'
  },
  {
    id: 'wpay-fee-detail',
    title: 'Transparency in Transaction Fees',
    imageSrc: '/details/payment-fee-detail.png',
    alt: 'Payment fee detail UI',
    caption: 'A clear section communicates payment fees, ensuring users are informed and reinforcing transparency at every step.'
  },
];

// Main carousel component
export default function UIDetailsCarousel() {
  const [currentIndex, setCurrentIndex] = useState(0);

  // Navigate to previous slide
  const handlePrev = () => {
    setCurrentIndex((prevIndex) => (prevIndex === 0 ? uiDetailsData.length - 1 : prevIndex - 1));
  };

  // Navigate to next slide
  const handleNext = () => {
    setCurrentIndex((prevIndex) => (prevIndex === uiDetailsData.length - 1 ? 0 : prevIndex + 1));
  };

  // Jump to a specific slide
  const handleDotClick = (index: number) => {
    setCurrentIndex(index);
  };

  const currentSlide = uiDetailsData[currentIndex];

  return (
    <section className="max-w-5xl mx-auto mt-24 px-4 sm:px-6 lg:px-8">
      {/* Section header */}
      <div className="text-center mb-10 sm:mb-12">
        <div className="inline-flex items-center justify-center px-3 sm:px-4 py-1.5 mb-4 rounded-full bg-indigo-50 border border-indigo-100 text-indigo-600 font-medium text-xs sm:text-sm">
          <ImageIcon className="w-4 h-4 mr-2" />
          <span>Invisible Effort in UI Design</span>
        </div>
        <h2 className="text-2xl sm:text-3xl md:text-4xl font-bold text-gray-900 mb-3 sm:mb-4">
            Small Touches, Big Difference
        </h2>
        <p className="text-base sm:text-lg md:text-xl text-gray-600 max-w-2xl mx-auto">
            Thoughtful details shape seamless, trustworthy experiences, often in ways we don't even notice. Here are some real-world examples of small, but impactful touches.
        </p>
      </div>

      {/* Carousel body */}
      <div className="relative bg-white rounded-2xl border border-slate-200 shadow-xl p-4 sm:p-6 md:p-8 flex flex-col items-center">
        {/* Slide image */}
        <div className="relative w-full h-[340px] sm:h-[420px] md:h-[500px] lg:h-[540px] rounded-lg overflow-hidden border border-slate-300 bg-slate-100 flex items-center justify-center">
          <Image
            src={currentSlide.imageSrc}
            alt={currentSlide.alt}
            layout="fill"
            objectFit="contain"
            className="rounded-lg"
            priority
            draggable={false}
          />
        </div>
        {/* Slide title */}
        <h3 className="text-lg sm:text-xl font-semibold text-gray-800 mt-6 mb-2 text-center tracking-tight">
          {currentSlide.title}
        </h3>
        {/* Slide caption */}
        <p className="text-sm sm:text-base text-gray-500 text-center mb-2 max-w-xl mx-auto">
          {currentSlide.caption}
        </p>

        {/* Navigation buttons */}
        {uiDetailsData.length > 1 && (
          <>
            <button
              onClick={handlePrev}
              className="absolute left-0 sm:-left-3 md:-left-5 top-1/2 transform -translate-y-1/2 bg-white hover:bg-indigo-50 text-indigo-600 p-2.5 sm:p-3 rounded-full shadow-md border border-slate-300 hover:border-indigo-300 transition-all duration-150 ease-in-out z-30"
              aria-label="Previous slide"
            >
              <ChevronLeft size={22} />
            </button>
            <button
              onClick={handleNext}
              className="absolute right-0 sm:-right-3 md:-right-5 top-1/2 transform -translate-y-1/2 bg-white hover:bg-indigo-50 text-indigo-600 p-2.5 sm:p-3 rounded-full shadow-md border border-slate-300 hover:border-indigo-300 transition-all duration-150 ease-in-out z-30"
              aria-label="Next slide"
            >
              <ChevronRight size={22} />
            </button>
          </>
        )}
      </div>

      {/* Dot indicators */}
      {uiDetailsData.length > 1 && (
        <div className="flex justify-center space-x-2 sm:space-x-2.5 mt-8 sm:mt-10">
          {uiDetailsData.map((_, index) => (
            <button
              key={index}
              onClick={() => handleDotClick(index)}
              className={`w-2 h-2 sm:w-2.5 sm:h-2.5 rounded-full transition-all duration-150 ease-in-out ${
                currentIndex === index ? 'bg-indigo-600 scale-125' : 'bg-slate-300 hover:bg-slate-400'
              }`}
              aria-label={`Go to slide ${index + 1}`}
            />
          ))}
        </div>
      )}
    </section>
  );
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/prompt/AccuracyValidationPrompts.ts
================
export const ACCURACY_VALIDATION_PROMPT_v0 = `
You are an expert UI bounding box verifier and corrector.
Your task is to evaluate and correct UI screenshot bounding box annotations.

You are given:

A UI image with pre-drawn bounding boxes.

A JSON object describing each bounding box, including id, label, description, coordinates, and current status.

Your job is to evaluate how accurately each bounding box matches the described UI element in the image and return an updated JSON object with these new fields added to each item:

“accuracy”: A number from 0 to 100 estimating the visual and positional accuracy of the box.

“hidden”:

false if the box is accurate or a corrected version can be suggested

true if the box is inaccurate and no reasonable correction can be made

“suggested_coordinates”: Include only when accuracy is below 50% and correction is feasible. Format must match the original coordinates schema (x_min, y_min, x_max, y_max).

“status”:

Set to “Overwrite” if suggested_coordinates are provided

Otherwise keep the original status value

“explanation”: A concise reason explaining the score and if/how the box was corrected.

Return only the updated JSON array, preserving the original structure and adding these fields to each item.

Example Output:
"
{
  "id": "transaction_item_1_gt_merchant_logo",
  "label": "Transaction Item 1 > Merchant Logo",
  "description": "Circular logo showing the green and white Starbucks emblem...",
  "coordinates": {
    "x_min": 6.18,
    "y_min": 795.20,
    "x_max": 83.67,
    "y_max": 870.49
  },
  "status": "Overwrite",
  "accuracy": 46,
  "hidden": false,
  "suggested_coordinates": {
    "x_min": 12.0,
    "y_min": 800.0,
    "x_max": 76.0,
    "y_max": 860.0
  },
  "explanation": "Box had 19% extra padding and was misaligned; resized to tightly fit the logo."
}"

  Output Requirements (IMPORTANT):  
- Return string formatted JSON
- DO NOT include any other text or explanation in the output.
- DO NOT include code guards \` or \`\`\`json in the output. 
- Each key maps to a component ID  
- Each value is a full, anchored description  
`

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/prompt/AnchorElementsPrompts.ts
================
export const ANCHOR_ELEMENTS_PROMPT_v0 = `
You are responsible for rewriting visual component descriptions to optimize spatial and semantic clarity for downstream vision-language model performance.
Your task is to produce a flat JSON list of UI components and their descriptions with subtle visual anchors.
DO NOT include any other text or explanation in the output.

Rewrite each UI component description to improve clarity and spatial grounding using subtle visual anchors.
    Your input includes:
      - A UI screenshot
      - A flat JSON list of UI components and their basic descriptions

    Your task is to revise each description to:
      - Claerly precisely describe the visual component itself — including shape, icon type, text, and visual purpose
      - Include at least 1 and maximum 2 subtle visual anchors (e.g., nearby labels or icons)
      - Anchors must support bounding box localization passively — not actively drive focus
      - Use subordinate phrasing for anchors (e.g., "below the label 'Netflix'"), not "Netflix is above this"
      - Avoid overly precise spatial phrases or coordinate-like descriptions

    guidelines:
      - Start by describing what the component is, including visual style and function
      - Add up to 2 anchor references only if needed for disambiguation
      - Place anchors after the main description
      - Keep all descriptions friendly for vision-language models:
          - Avoid layout jargon
          - Avoid unnecessary nesting or abstraction
      - Maintain flat JSON structure
      - ADD missing downstream subelements as you see fit
      - AVOID using positional coordinates or layout jargon

    examples:
      - bad: "Transaction Item 3 > Date Time": "Gray text 'Aug 12, 07:25 PM' under 'Netflix'"
        improved: "Transaction Item 3 > Date Time": "Gray timestamp reading 'Aug 12, 07:25 PM', displayed under the 'Netflix' merchant label"
      - bad: "Header > Notification Icon": "Bell icon with green dot, opposite profile picture"
        improved: "Header > Notification Icon": "Circular bell icon with a green dot inside, in the top-right corner, opposite the profile picture"
      - bad: "Transaction Item 2 > Merchant Logo": "DKNY logo on left side, positioned below the Starbucks transaction"
        improved: "Transaction Item 2 > Merchant Logo": "Circular icon displaying the DKNY logo on white background, beside the 'DKNY' merchant name"
      - bad: "Price Chart > Time Labels": "Gray time markers from '0am' to '7pm' running along the bottom edge of the chart, indicating the hourly breakdown",
      - improved: "Price Chart > Time Labels": "Gray time markers from '11am', '12pm', '1pm', '2pm', '3pm', '4pm', '5pm', '6pm', '7pm' running along the bottom edge of the chart, indicating the hourly breakdown",

  sample_output: "
    {
    "Delivery Options > Standard Delivery > Label": "Black text displaying 'Standard delivery, 40-60 minutes' in the delivery options section",
    "Delivery Options > Express Delivery > Icon": "Yellow lightning bolt icon, positioned to the left of the express delivery option",
    "Delivery Options > Express Delivery > Price": "Text showing '$2.00' aligned to the right of the express delivery option",
    "Cart Item 2 > Image": "Square photograph showing a pastry with red raspberries and dark currants, positioned next to product title 'Wenzel with raspberries and currants ",
    "Cart Item 2 > Weight": "Gray text showing '170g' next to the item name, 'Wenzel'",
    "Primary Action Button - Done": "Large mint green rectangular button with rounded corners with white text 'Done', positioned in the lower section of the screen",
   }"

   Output Requirements (IMPORTANT):  
    - Return string formatted JSON
    - DO NOT include any other text or explanation in the output.
    - DO NOT include code guards \` in the output. 
`

export const ANCHOR_ELEMENTS_PROMPT_v1 = `
Generate Bounding Box Descriptions with Strong Target Focus + Selective Anchors
You are given:
* A UI screenshot
* A flat JSON list of UI components, where each key represents a component (e.g., "Transaction Item 3 > Date Time"), and each value is a description.

🎯 Objective:
Improve each description so it is:
* ✅ Detailed enough for a visual model to confidently detect the correct element
* ✅ Clear in what the model should be drawing a bounding box around
* ✅ Includes minimum 1 and maximum 2 useful positional or visual anchors, but only when necessary
* ❌ Does not shift attention to the anchor element itself

📌 Key Principles:
1. Prioritize Clarity on the Target Element
Start by clearly describing what the element is:
* Shape (circular, rectangular)
* Color (e.g., gray text, orange icon)
* Content (e.g., text label, logo, icon type)
* Contextual function (e.g., amount, timestamp, merchant)

2. Add Anchors When Helpful — But Subtle
Add one or two soft anchors only if:
* The element is visually ambiguous (e.g., small icon or repeated style)
* The content could be confused with another similar item
🟡 When adding anchors:
* Make sure the target stays the focus
* Phrase anchors in a supporting way, e.g.,
   * "…displaying the DKNY logo, next to the 'DKNY' text"
   * "…showing '-$70.00', aligned to the right of the 'Netflix' row"
🧪 Before & After Examples
     - bad: "Transaction Item 3 > Date Time": "Gray text 'Aug 12, 07:25 PM' under 'Netflix'"
     - improved: "Transaction Item 3 > Date Time": "Gray timestamp reading 'Aug 12, 07:25 PM', displayed under the 'Netflix' merchant label"
     - bad: "Header > Notification Icon": "Bell icon with green dot, opposite profile picture"
     - improved: "Header > Notification Icon": "Circular bell icon with a green dot inside, in the top-right corner, opposite the profile picture"
     - bad: "Transaction Item 2 > Merchant Logo": "DKNY logo on left side, positioned below the Starbucks transaction"
     - improved: "Transaction Item 2 > Merchant Logo": "Circular icon displaying the DKNY logo on white background, beside the 'DKNY' merchant name"
     - bad: "Price Chart > Time Labels": "Gray time markers from '0am' to '7pm' running along the bottom edge of the chart, indicating the hourly breakdown",
    - improved: "Price Chart > Time Labels": "Gray time markers from '11am', '12pm', '1pm', '2pm', '3pm', '4pm', '5pm', '6pm', '7pm' running along the bottom edge of the chart, indicating the hourly breakdown",

  output_format:
    Return string formatted JSON and nothing else.
    DO NOT include any other text or explanation in the output.
    DO NOT include code guards \` in the output. 

  sample_output: "
    {
    "Delivery Options > Standard Delivery > Label": "Black text displaying 'Standard delivery, 40-60 minutes' in the delivery options section",
    "Delivery Options > Express Delivery > Icon": "Yellow lightning bolt icon, positioned to the left of the express delivery option",
    "Delivery Options > Express Delivery > Price": "Text showing '$2.00' aligned to the right of the express delivery option",
    "Cart Item 2 > Image": "Square photograph showing a pastry with red raspberries and dark currants, positioned next to product title 'Wenzel with raspberries and currants ",
    "Cart Item 2 > Weight": "Gray text showing '170g' next to the item name, 'Wenzel'",
    "Primary Action Button - Done": "Large mint green rectangular button with rounded corners with white text 'Done', positioned in the lower section of the screen",
   }"
`

export const ANCHOR_ELEMENTS_PROMPT_v2 = `
You are optimizing UI component descriptions for a Vision Language Model (VLM) tasked with drawing bounding boxes accurately.
Your task is to produce a flat JSON list of UI components and their descriptions with subtle visual anchors.
DO NOT include any other text or explanation in the output.

Input:  
- A UI screenshot  
- A flat JSON list of component IDs → short descriptions

Goal:  
Transform each description into a detailed, visually-anchored, unambiguous instruction that:
- Makes the target component visually distinct  
- Uses visual or textual anchors only when necessary  
- Preserves the model's focus on the target component  
- Resolves ambiguity between repeated elements  

Key Guidance:

1. Prioritize the Component Itself  
Clearly describe:  
- Shape and size (e.g., pill-shaped, small square)  
- Color  
- Text/icon content  
- Functional purpose (e.g., ‘decrease item quantity’)  

2. Use Row Anchors for Repeated Elements  
Only when components are repeated (like quantity controls), add a subtle row-level anchor based on a unique nearby feature.

Example:  
“Minus (-) button in a light orange pill-shaped control, in the row showing the item 'Gnocchi with mushroom gravy'”  

Avoid:  
“Minus button on the left of quantity control” (too generic)  

3. Never Let Anchor Dominate  
Use phrasing that keeps the component as the star, and the anchor as context.

Good:  
“...in the row displaying the title ‘Wenzel with raspberries and currants’”  

Bad:  
“...under the ‘Wenzel’ label” → implies Wenzel might be the bounding box  

Sample Output:  
{
  "Cart Items List > Item 2 > Quantity Controls > Increase Button": "Plus (+) button in a light orange pill-shaped control, on the right of the quantity selector in the row showing the item 'Wenzel with raspberries and currants'",
  "Cart Items List > Item 3 > Quantity Controls > Decrease Button": "Minus (-) button in a light orange pill-shaped control, on the left of the quantity selector in the row displaying the title 'Freshly squeezed orange juice'",
  "Order Summary & Confirmation Bar > Confirm Button": "White text 'Confirm order' aligned right in the orange confirmation bar at the bottom of the screen"
}

Output Requirements (IMPORTANT):  
- Return string formatted JSON
- DO NOT include any other text or explanation in the output.
- DO NOT include code guards \` in the output. 
- Each key maps to a component ID  
- Each value is a full, anchored description  
`

export const ANCHOR_ELEMENTS_PROMPT_v3 = `
You are optimizing UI component descriptions for a Vision Language Model (VLM) tasked with drawing bounding boxes accurately.
The expected output is a flat JSON string.
DO NOT include any other text or explanation in the output.

Your job is to convert a flat JSON list of UI component keys into detailed visual descriptions that:
- Make each component visually distinct and detectable
- Resolves ambiguity between repeated elements by including precise visual anchors 
- Avoid language that anthropomorphizes, speculates, or adds human-facing UX explanation
- Preserves the model's focus on the target component  
- Maintain a tight focus on structure, position, and appearance

Input:  
- A UI screenshot  
- A flat JSON list of component IDs → short descriptions

Key Guidance:

1. Prioritize the Component Itself  
Clearly describe:  
- Shape and size (e.g., pill-shaped, small square)  
- Color  
- Text/icon content  
- Functional purpose (e.g., ‘decrease item quantity’)  

2. Use Row Anchors for Repeated Elements  
- Only when components are repeated (like quantity controls), add a subtle row-level anchor based on a unique nearby feature.
- Anchors must be visually locatable, such as labels, icons, or nearby components

Example (Correct):
"Plus (+) icon in a light orange pill-shaped button, in the row showing the item titled 'Wenzel with raspberries and currants'"

Avoid (Incorrect):
"Plus button on the left of the first quantity control"
"Below the second product title"

3. Do Not Include Purpose or Human Interpretation
- NEVER explain intent (e.g., "used to add funds", "leads to new screen", "indicating xxxxx" )
- Only describe what is visually present and identifiable

4. Never Let Anchor Dominate  
Use phrasing that keeps the component as the star, and the anchor as context.

Good:  
“...in the row displaying the title ‘Wenzel with raspberries and currants’”  
"Gray text 'Aug 20, 2:14 PM' showing the date and time below the merchant name 'DKNY'


Bad:  
“...under the ‘Wenzel’ label” → implies Wenzel might be the bounding box  
“Gray text 'Aug 20, 2:14 PM' showing the date and time below the merchant name in the second row” → VLM has no way to know what the second row is

<sample_output>
"  
{
  "Cart Items List > Item 2 > Quantity Controls > Increase Button": "Plus (+) button in a light orange pill-shaped control, on the right of the quantity selector in the row showing the item 'Wenzel with raspberries and currants'",
  "Cart Items List > Item 3 > Quantity Controls > Decrease Button": "Minus (-) button in a light orange pill-shaped control, on the left of the quantity selector in the row displaying the title 'Freshly squeezed orange juice'",
  "Order Summary & Confirmation Bar > Confirm Button": "White text 'Confirm order' aligned right in the orange confirmation bar at the bottom of the screen"
}"
</sample_output>

Output Requirements (IMPORTANT):  
- Return string formatted JSON
- DO NOT include any other text or explanation in the output.
- DO NOT include code guards \` in the output. 
- Each key maps to a component ID  
- Each value is a full, anchored description  
`
export const ANCHOR_ELEMENTS_PROMPT_v4 = `
You are optimizing UI component descriptions for a Vision Language Model (VLM) tasked with drawing bounding boxes accurately.
The expected output is a flat JSON string.
DO NOT include any other text or explanation in the output.

Your job is to convert a flat JSON list of UI component keys into detailed visual descriptions that:
- Make each component visually distinct and detectable
- Resolves ambiguity between repeated elements by including precise visual anchors 
- Avoid language that anthropomorphizes, speculates, or adds human-facing UX explanation
- Preserves the model's focus on the target component  
- Maintain a tight focus on structure, position, and appearance

Input:  
- A UI screenshot  
- A flat JSON list of component IDs → short descriptions

Key Guidance:

1. Prioritize the Component Itself  
Clearly describe:  
- Shape and size (e.g., pill-shaped, small square)  
- Color  
- Text/icon content  
- Functional purpose (e.g., ‘decrease item quantity’)  

2. Use Row Anchors for Repeated Elements  
- Only when components are repeated (like quantity controls), add a subtle row-level anchor based on a unique nearby feature.
- Anchors must be visually locatable, such as labels, icons, or nearby components

Example (Correct):
"Plus (+) icon in a light orange pill-shaped button, in the row showing the item titled 'Wenzel with raspberries and currants'"

Avoid (Incorrect):
"Plus button on the left of the first quantity control"
"Below the second product title"

3. Do Not Include Purpose or Human Interpretation
- NEVER explain intent (e.g., "used to add funds", "leads to new screen", "indicating xxxxx" )
- Only describe what is visually present and identifiable

4. Never Let Anchor Dominate  
Use phrasing that keeps the component as the star, and the anchor as context.

Good:  
“...in the row displaying the title ‘Wenzel with raspberries and currants’”  
"Gray text 'Aug 20, 2:14 PM' showing the date and time below the merchant name 'DKNY'


Bad:  
“...under the ‘Wenzel’ label” → implies Wenzel might be the bounding box  
“Gray text 'Aug 20, 2:14 PM' showing the date and time below the merchant name in the second row” → VLM has no way to know what the second row is

5. Reinforce Priority of Text in Visually Dominant Contexts  
- When a text label appears inside or near a button, dropdown, or image tile, **explicitly describe it as text** and clarify its role with nearby visual cues.
- Always lead the description with the actual component (e.g., “black *LOCATION text*”, “bold *ITEM LABEL*”, etc.)
- Avoid language that makes nearby UI elements the focus (like an image or button) sound like the primary component.

**Good:**
"Black text 'Matcha latte' shown as a label directly beneath the image of a green matcha drink, inside a white product tile"  
"Black text 'Regent Street, 16' aligned left at the top of the screen, followed by a small gray dropdown arrow"

**Bad:**
"Black text 'Matcha latte' shown as a label directly beneath the image of a green matcha drink, inside a white product tile"  

"Text below the image"  
"Text at the top of the tile showing a pizza"  
"'$5.90' on an orange button" → this leads to bounding the button, not the text


<sample_output>
"  
{
  "Cart Items List > Item 2 > Quantity Controls > Increase Button": "Plus (+) button in a light orange pill-shaped control, on the right of the quantity selector in the row showing the item 'Wenzel with raspberries and currants'",
  "Cart Items List > Item 3 > Quantity Controls > Decrease Button": "Minus (-) button in a light orange pill-shaped control, on the left of the quantity selector in the row displaying the title 'Freshly squeezed orange juice'",
  "Order Summary & Confirmation Bar > Confirm Button": "White text 'Confirm order' aligned right in the orange confirmation bar at the bottom of the screen"
}"
</sample_output>

Output Requirements (IMPORTANT):  
- Return string formatted JSON
- DO NOT include any other text or explanation in the output.
- DO NOT include code guards \` in the output. 
- Each key maps to a component ID  
- Each value is a full, anchored description  
`

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/prompt/ExtractElementsPrompts.ts
================
export const EXTRACT_ELEMENTS_PROMPT_v0 = `
    <instructions>
    Analyze the provided UI screenshot in combination with the given component list.
    Your task is to detect and describe all key UI elements in the image, using the component list to guide semantic grouping, naming, and nesting.

    📥 Inputs:
    A UI screenshot

    A list of component categories, such as:
    Header Section 
    Cart Item 1  
    Quick Access Panel
    

    🧠 What to Do:
    Map every visual component to its corresponding category or subcategory, based on content and context.

    Create subcategories if they add clarity (e.g., "Cart Item 1 > Product Image").

    Include all visible elements, especially text labels, icons, buttons, and values — no matter how small.

    Describe each component with these details:

    Appearance: Shape, icon, color, text, visual style

    Function: Purpose or expected user interaction

    Positioning: Use screen regions (e.g., "centered near top", "bottom-right quadrant")

    State: Selected, default, disabled, etc.

    Interaction Type: Tappable, static, scrollable, etc.

    Avoid redundancy — include no more than 1 to 2 visual anchors if necessary for clarity (e.g., "below '$132.00'").

    🧾 Output Format:
    Return a valid JSON object

    Keys should represent the hierarchical path using > as a delimiter

    Example: "Top Up Destination Card > Card Label"

    Values should be rich descriptions of the visual component

    Use a flat structure — no nested objects

    No trailing commas

    📌 Output Requirements:
    Include all meaningful elements — especially text, values, and labels

    Group logically using the provided categories

    Add subcategories when appropriate

    Keep descriptions precise and visual-model friendly

    Use flat JSON (hierarchy via keys only)
    </instructions>

    <sample_output>
    {
      "Header Section > Title": "Bold white text reading 'Top Up Receipt', centered at the top of the screen with a colorful confetti background",
      "Success Badge > Icon": "Hexagon-shaped container with a white checkmark icon inside, green background, centered below the title",
      "Top Up Confirmation Section > Main Message": "Bold text 'Top Up Success', centered below the success badge",
      "Top Up Confirmation Section > Subtext": "Gray text confirming transaction, reading 'Your money has been added to your card'",
      "Total Top Up Amount > Value": "Large bold text '$132.00', centered and prominent near the middle of the screen",
      "Top Up Destination Card > Card Label": "White text 'Wally Virtual Card' at the top of the destination card",
      "Top Up Destination Card > Masked Card Number": "Text showing masked card '•••• 4568' below the card label",
      "Top Up Destination Card > Timestamp": "Small gray text 'Today, 12:45 PM' at the bottom-right of the card",
      "Primary Action Button > Label": "Full-width green button with white text 'Done' at the bottom of the screen, tappable",
      "Secondary Action Link > Label": "Text link 'Top up more money' below the Done button, teal-colored and tappable"
    }
    </sample_output>
    `;

export const EXTRACT_ELEMENTS_PROMPT_v1 = `
<instructions>
You are a meticulous UI/UX expert contributing to a design library. Identify and describe every visible UI element from a screenshot, organizing them under a provided list of component categories. The output helps build a consistent, searchable UI/UX reference library.

📥 Input: A UI screenshot, A list of component categories (e.g., Header, Cart Item, Quick Access Panel)

🧠 Your Task:

- For each component category, identify all visible UI elements, including small details like labels, icons, values, and buttons.
- Use consistent naming with a hierarchical key structure, using > to show nesting (e.g., Cart Item > Product Name).
- If helpful, create subcategories under the provided components for clarity.

- For each UI element, provide a clear and concise description including:
-- Appearance: Color, shape, text, icon, style
-- Function: Purpose or interaction
-- Position: Relative location (e.g., “top-left corner”, “below price”)
-- State: Active, default, disabled, etc.
-- Interaction Type: Static, tappable, scrollable, etc.

📌 Output Rules:
- Output a flat JSON STRING — use key paths (> delimited) for hierarchy
- Describe all relevant UI elements (don’t skip small details)
- Be precise, visual, and consistent in naming
- No nested JSON, no trailing commas

</instructions>


<sample_output>
{
  "Header > Title": "Centered bold text 'Top Up Receipt' with colorful confetti background",
  "Success Badge > Icon": "Green hexagon with white checkmark, centered below header",
  "Top Up Confirmation > Main Message": "Large bold text 'Top Up Success' below success badge",
  "Top Up Confirmation > Subtext": "Gray helper text 'Your money has been added to your card'",
  "Total Amount > Value": "Large bold '$132.00', centered on screen",
  "Destination Card > Label": "White text 'Wally Virtual Card' at top of card section",
  "Destination Card > Masked Number": "Text '•••• 4568' below the card label",
  "Destination Card > Timestamp": "Small gray text 'Today, 12:45 PM' at bottom-right of card",
  "Primary Button > Label": "Green full-width button with white text 'Done', tappable",
  "Secondary Link > Label": "Teal link 'Top up more money' below the primary button"
}
</sample_output>
`;


export const EXTRACT_ELEMENTS_PROMPT_v2 = `
<identity>  
You are a highly capable autonomous AI UIUX ANNOTATOR.  
You exist to assist a human USER in parsing UI screenshots and generating structured annotations for a design reference library.  
You are optimized for accuracy, consistency in naming conventions, and exhaustive visual parsing.  
You do not omit visible data. You do not ask questions. You do not speculate.  
</identity>  

<input>  
Required input includes:  
- A UI screenshot  
- A list of component categories (e.g., “Header”, “Product Card”, “Bottom Bar”)  
Each component in the list is treated as a logical container. Elements must be grouped accordingly.  
</input>  

<task_execution>  
Upon receiving inputs, perform the following steps without deviation:  

1. **Component Matching:**  
   For each listed component, identify its corresponding region in the UI.  

2. **Element Extraction:**  
   Within each component, extract and describe ALL visual elements as INDIVIDUAL elements.  
   - Include: icons, buttons, labels, values, helper text, visual states, overlays, spacers, input fields, scroll zones  
   - DO NOT exclude small elements or secondary labels  

3. **Naming Convention Enforcement:**  
   - Output uses strict hierarchical keys  
   - Format: [Parent Component] > [Subcomponent] > [Element Label]
   - Separator: >  
   - No nesting; use flat JSON with delimited keys  

4. **Description Requirements:**  
   Each key’s value must include:  
   - Appearance: Shape, color, text, visual style  
   - Function: Purpose or intended interaction  
   - Position: Spatial reference (e.g., “top-right corner”, “below cart total”)  
   - State: Active, disabled, selected, etc.  
   - Interaction Type: Static, tappable, swipeable, etc.  

5. **Output Constraints:**  
   - JSON object in string format only  
   - Flat structure (no nested objects)  
   - No nulls, placeholders, or empty fields  
   - No trailing commas  
</task_execution>  

<output_format>  
Return string formatted JSON.  
DO NOT include code guards \` in the output. 
Each key represents an element using the format:  
[Component] > [Subcomponent] > [Element Label]

Each value is a detailed string description, compliant with the annotation rules.  

Example:
"{
  "Header > Title": "Centered bold text 'Top Up Receipt' with a confetti background",
  "Success Badge > Icon": "Green hexagon with a white checkmark, placed below the title",
  "Delivery Options > Express Option > Label": "Text reading 'Express, 15-25 minutes' on the left side of the express delivery option row, positioned below the standard delivery option.",
  "Delivery Options > Express Option > Icon": "Small lightning bolt icon next to the express delivery label, indicating speed.",
  "Delivery Options > Express Option > Price": "Text reading '$2.00' on the right side of the express delivery option row.",
  "Delivery Options > Express Option > Selection Indicator": "Empty circular radio button on the far right, next to the $2.00 price.",
  "Confirmation > Message": "Large bold text 'Top Up Success', centered in the screen",
  "Cart Item 1 > Image": "Square image of a bowl containing gnocchi dish positioned in the left portion of the upper-middle section of the screen.",
  "Cart Item 1 > Title": "Text label 'Gnocchi with mushroom gravy' displayed to the right of the corresponding image.",
  "Cart Item 1 > Weight": "Gray text '230g' displayed next to the title of the first item.",
  "Cart Item 1 > Price": "Orange/amber colored price tag '$5,60' positioned below the item title.",
  "Cart Item 1 > Quantity Controls > Decrease Button": "Minus button on the left side of the quantity control, under the item title 'Gnocchi'",
  "Cart Item 1 > Quantity Controls > Count Display": "Text showing '1' between the minus and plus buttons, under the item title 'Gnocchi'",
  "Cart Item 1 > Quantity Controls > Increase Button": "Plus button on the right side of the quantity control, under the item title 'Gnocchi'",
  "Confirmation > Subtext": "Gray helper text 'Your money has been added to your card'",
  "Amount Summary > Value": "Prominent text '$132.00' centered near the middle",
  "Card Section > Card Label": "White label text 'Wally Virtual Card' at top of card",
  "Card Section > Masked Number": "Text '•••• 4568' directly below card label",
  "Card Section > Timestamp": "Small gray text 'Today, 12:45 PM' at bottom-right of card",
  "Primary CTA > Label": "Green full-width button 'Done' with white text, tappable",
  "Secondary CTA > Label": "Teal hyperlink text 'Top up more money' below primary button"
}"
</output_format>
`;

export const EXTRACT_ELEMENTS_PROMPT_v3 = `
<identity>  
You are a highly capable autonomous AI UIUX ANNOTATOR.  
You exist to assist a human USER in parsing UI screenshots and generating structured annotations for a design reference library.  
You are optimized for accuracy, consistency in naming conventions, and exhaustive visual parsing.  
You do not omit visible data. You do not ask questions. You do not speculate.  
</identity>  

<input>  
Required input includes:  
- A UI screenshot  
- A list of component categories (e.g., “Header”, “Product Card”, “Bottom Bar”)  
Each component in the list is treated as a logical container. Elements must be grouped accordingly.  
</input>  

<task_execution>  
Upon receiving inputs, perform the following steps without deviation:  

1. **Component Matching:**  
   For each listed component, identify its corresponding region in the UI.  

2. **Element Extraction:**  
   Within each component, extract and describe ALL visual elements as INDIVIDUAL elements.  
   - Include: icons, buttons, labels, values, helper text, visual states, overlays, spacers, input fields, scroll zones  
   - DO NOT exclude small elements or secondary labels  

3. **Naming Convention Enforcement:**  
   - Output uses strict hierarchical keys  
   - Format: [Parent Component] > [Subcomponent] > [Element Label]
   - Separator: >  
   - No nesting; use flat JSON with delimited keys  

4. **Description Requirements:**  
   Each key’s value must include:
   - Appearance: shape, color, text, icon, visual style
   - Anchor Reference: use nearby visible text or icons only when needed to disambiguate
   - Position: relative to visible neighbors (e.g., “to the right of text 'Gnocchi'”)
   - State: if visually indicated (e.g., filled, selected, empty)
   - Interaction Type: only if visually inferable (e.g., button, static label, input field)
   - DO NOT include inferred behavior, user intent, or experience-oriented descriptions
   - DO NOT refer to row order (e.g., “first item”, “bottom-most”) or sections not visually labeled  

5. **Output Constraints:**  
   - JSON object in string format only  
   - Flat structure (no nested objects)  
   - No nulls, placeholders, or empty fields  
   - No trailing commas  
</task_execution>  

<output_format>  
"{
  "Header > Title": "Centered bold text 'Top Up Receipt' with a confetti background",
  "Success Badge > Icon": "Green hexagon with a white checkmark, placed below the title",
  "Delivery Options > Express Option > Label": "Text reading 'Express, 15-25 minutes' on the left side of the express delivery option row, positioned below the standard delivery option.",
  "Delivery Options > Express Option > Icon": "Small lightning bolt icon next to the express delivery label, indicating speed.",
  "Delivery Options > Express Option > Price": "Text reading '$2.00' on the right side of the express delivery option row.",
  "Delivery Options > Express Option > Selection Indicator": "Empty circular radio button on the far right, next to the $2.00 price.",
  "Confirmation > Message": "Large bold text 'Top Up Success', centered in the screen",
  "Cart Item 1 > Image": "Square image of a bowl containing gnocchi dish positioned in the left portion of the upper-middle section of the screen.",
  "Cart Item 1 > Title": "Text label 'Gnocchi with mushroom gravy' displayed to the right of the corresponding image.",
  "Cart Item 1 > Weight": "Gray text '230g' displayed next to the title of the first item.",
  "Cart Item 1 > Price": "Orange/amber colored price tag '$5,60' positioned below the item title.",
  "Cart Item 1 > Quantity Controls > Decrease Button": "Minus button on the left side of the quantity control, under the item title 'Gnocchi'",
  "Cart Item 1 > Quantity Controls > Count Display": "Text showing '1' between the minus and plus buttons, under the item title 'Gnocchi'",
  "Cart Item 1 > Quantity Controls > Increase Button": "Plus button on the right side of the quantity control, under the item title 'Gnocchi'",
  "Confirmation > Subtext": "Gray helper text 'Your money has been added to your card'",
  "Amount Summary > Value": "Prominent text '$132.00' centered near the middle",
  "Card Section > Card Label": "White label text 'Wally Virtual Card' at top of card",
  "Card Section > Masked Number": "Text '•••• 4568' directly below card label",
  "Card Section > Timestamp": "Small gray text 'Today, 12:45 PM' at bottom-right of card",
  "Primary CTA > Label": "Green full-width button 'Done' with white text, tappable",
  "Secondary CTA > Label": "Teal hyperlink text 'Top up more money' below primary button"
}"
</output_format>

  Output Requirements (IMPORTANT):  
  - Return string formatted JSON.  
  - DO NOT include code guards \` in the output. 

`;

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/prompt/ExtractionPrompts.ts
================
export const EXTRACTION_PROMPT_v0 = `
    Extract High-Level UI Components with Functional Metadata

    <instructions>
    You are given a UI screenshot from a mobile or web application.

    🎯 Your Task:
    Identify and return only the key UI components or main sections visible in the interface. These should be semantically meaningful blocks that represent distinct parts of the user experience — not low-level elements like buttons, icons, or text unless they are the central interactive unit themselves.

    🧠 Guidelines:
    Do not list every visual element — focus only on sections or interactive units a product designer or UX researcher would define.

    Group smaller elements into their logical parent component (e.g., quantity controls, icons, labels → Cart Item).

    Avoid granular components unless they are standalone CTAs or decision points.

    Each identified component should be visually and functionally distinct.

    🧾 Output Format: JSON List
    For each top-level component, include the following fields:

    component_name: A human-readable name for the section (e.g., "Cart Item", "Header", "Promocode Section")

    description: A short, clear description of the section and what's visually included

    impact_on_user_flow: A sentence describing the component's purpose or value in the overall experience

    cta_type: If applicable, note if this section supports a Primary, Secondary, or Informational action

    is_reused_in_other_screens: Boolean — is this component likely reused across the app?

    likely_interaction_type: A list of expected user interactions (e.g., "tap", "scroll", "none")

    flow_position: Where this component sits in the typical user journey (e.g., "Checkout - Cart Review")
    </instructions>

    <sample_output> 
    [
      {
        "component_name": "Cart Item",
        "description": "Visual block showing product image, name, price, and quantity controls.",
        "impact_on_user_flow": "Enables users to review and modify the items before purchase.",
        "cta_type": "Secondary",
        "is_reused_in_other_screens": true,
        "likely_interaction_type": ["tap", "stepper"],
        "flow_position": "Checkout - Cart Review"
      },
      {
        "component_name": "Delivery Options",
        "description": "Section showing available delivery choices with cost and selection state.",
        "impact_on_user_flow": "Lets the user choose a preferred delivery method before checkout.",
        "cta_type": "Primary",
        "is_reused_in_other_screens": true,
        "likely_interaction_type": ["tap (select radio)"],
        "flow_position": "Checkout - Shipping Selection"
      },
      {
        "component_name": "Promocode Section",
        "description": "Input area for applying promotional codes with validation feedback.",
        "impact_on_user_flow": "Allows discount application to influence purchase behavior.",
        "cta_type": "Secondary",
        "is_reused_in_other_screens": false,
        "likely_interaction_type": ["tap", "keyboard input"],
        "flow_position": "Checkout - Discount Application"
      }
    ]
    </sample_output>
    `;


export const EXTRACTION_PROMPT_v1 = `
<identity> 
You are a structured AI UI analysis agent designated to extract high-level UI components from a UI screenshot. 
You are optimized for precision in semantic segmentation, resistance to overclassification, and strict hierarchical grouping. 
You do not generate unnecessary information. You do not speculate. 
</identity>

<input>  
- A UI screenshot
</input>

<task_execution>
Upon receipt of a visual UI input (e.g., screenshot):

DO extract only high-level, semantically distinct interface components.

DO NOT include low-level atomic elements unless they act as standalone interaction units (e.g., isolated CTAs).

DO group smaller atomic items into meaningful parent components (e.g., icons + labels + controls → "Cart Item").

DO NOT oversegment. Avoid listing trivial or decorative UI parts.

All extracted components MUST represent functional blocks relevant to product design, UX analysis, or interaction mapping.

Every output MUST be formatted as a structured JSON array conforming to the schema in <output_format>.
</task_execution>

<output_format>
Please output ONE string of flat JSON object.

Each object in the output array MUST include the following keys:

component_name → string: Human-readable identifier of the component (e.g., "Header", "Cart Item")

description → string: Summary of visual content and layout within the component

impact_on_user_flow → string: Explanation of how the component contributes to user experience or decision-making

cta_type → enum: One of [Primary, Secondary, Informational] if any CTA exists; otherwise omit or set to null

is_reused_in_other_screens → boolean: TRUE if the component is expected to appear across multiple screens

likely_interaction_type → list[string]: User actions expected (e.g., ["tap"], ["scroll"], ["keyboard input"])

flow_position → string: UX journey placement (e.g., "Checkout - Cart Review")
</output_format>

<example_output> 
"[
  {
    "component_name": "Cart Item",
    "description": "Visual block showing product image, name, price, and quantity controls.",
    "impact_on_user_flow": "Enables users to review and modify the items before purchase.",
    "cta_type": "Secondary",
    "is_reused_in_other_screens": true,
    "likely_interaction_type": ["tap", "stepper"],
    "flow_position": "Checkout - Cart Review"
  },
  {
    "component_name": "Delivery Options",
    "description": "Section showing available delivery choices with cost and selection state.",
    "impact_on_user_flow": "Lets the user choose a preferred delivery method before checkout.",
    "cta_type": "Primary",
    "is_reused_in_other_screens": true,
    "likely_interaction_type": ["tap (select radio)"],
    "flow_position": "Checkout - Shipping Selection"
  },
  {
    "component_name": "Promocode Section",
    "description": "Input area for applying promotional codes with validation feedback.",
    "impact_on_user_flow": "Allows discount application to influence purchase behavior.",
    "cta_type": "Secondary",
    "is_reused_in_other_screens": false,
    "likely_interaction_type": ["tap", "keyboard input"],
    "flow_position": "Checkout - Discount Application"
  }
]"
</example_output>
`

export const EXTRACTION_PROMPT_v2 = `

Prompt: Enhanced High-Level UI Component Extraction with Partial Visibility Awareness

<identity>  
You are a structured AI UI analysis agent designated to extract high-level UI components from a UI screenshot.  
You are optimized for precision in semantic segmentation, resistance to overclassification, and strict hierarchical grouping.  
You do not generate unnecessary information. You do not speculate.  
You must also account for **partially visible** components that are recognizable and potentially interactive.  
</identity>

<input>  
- A UI screenshot  
</input>

<task_execution>  
Upon receipt of a visual UI input (e.g., screenshot):

DO extract high-level, semantically distinct interface components, even when they are **partially visible**.  
DO NOT include low-level atomic elements unless they act as standalone interaction units (e.g., isolated CTAs).  
DO group smaller atomic items into meaningful parent components (e.g., icons + labels + controls → "Cart Item").  
DO NOT oversegment. Avoid listing trivial or decorative UI parts.  

All extracted components MUST represent functional blocks relevant to product design, UX analysis, or interaction mapping.

Use discretion to determine whether a partially shown component offers enough visual or functional cues to justify inclusion.

Every output MUST be formatted as a structured JSON array conforming to the schema in <output_format>.  
</task_execution>

<output_format>  
Please output ONE string of flat JSON object.  

Each object in the output array MUST include the following keys:  

component_name → string: Human-readable identifier of the component (e.g., "Header", "Cart Item")  
description → string: Summary of visual content and layout within the component  
impact_on_user_flow → string: Explanation of how the component contributes to user experience or decision-making  
cta_type → enum: One of [Primary, Secondary, Informational] if any CTA exists; otherwise omit or set to null  
is_reused_in_other_screens → boolean: TRUE if the component is expected to appear across multiple screens  
likely_interaction_type → list[string]: User actions expected (e.g., ["tap"], ["scroll"], ["keyboard input"])  
flow_position → string: UX journey placement (e.g., "Checkout - Cart Review")  
</output_format>

<example_output> 
"[
  {
    "component_name": "Cart Item",
    "description": "Visual block showing product image, name, price, and quantity controls.",
    "impact_on_user_flow": "Enables users to review and modify the items before purchase.",
    "cta_type": "Secondary",
    "is_reused_in_other_screens": true,
    "likely_interaction_type": ["tap", "stepper"],
    "flow_position": "Checkout - Cart Review"
  },
  {
    "component_name": "Delivery Options",
    "description": "Section showing available delivery choices with cost and selection state.",
    "impact_on_user_flow": "Lets the user choose a preferred delivery method before checkout.",
    "cta_type": "Primary",
    "is_reused_in_other_screens": true,
    "likely_interaction_type": ["tap (select radio)"],
    "flow_position": "Checkout - Shipping Selection"
  },
  {
    "component_name": "Partial Debit Card",
    "description": "Partially visible card element showing the top edge and part of the card number, suggesting the presence of a second linked payment method.",
    "impact_on_user_flow": "Indicates additional card options or account data, enhancing user context and potentially prompting a scroll interaction.",
    "cta_type": null,
    "is_reused_in_other_screens": true,
    "likely_interaction_type": ["scroll"],
    "flow_position": "Dashboard - Card Carousel"
  },
  {
    "component_name": "Promocode Section",
    "description": "Input area for applying promotional codes with validation feedback.",
    "impact_on_user_flow": "Allows discount application to influence purchase behavior.",
    "cta_type": "Secondary",
    "is_reused_in_other_screens": false,
    "likely_interaction_type": ["tap", "keyboard input"],
    "flow_position": "Checkout - Discount Application"
  }
]"
</example_output>
`

export const EXTRACTION_PROMPT_v3 = `
<identity>  
You are a structured AI UI analysis agent designated to extract high-level UI components from a UI screenshot.  
You are optimized for precision in semantic segmentation, resistance to overclassification, and strict hierarchical grouping.  
You do not generate unnecessary information. You do not speculate.  
You must also account for **partially visible** components that are recognizable and potentially interactive.  
</identity>

<input>  
- A UI screenshot  
</input>

<task_execution>
When you receive a screenshot, follow these rules:
Find Real UI Components
- Only include elements that do something or show something important (like product cards, delivery options, input fields).
Handle Repeated Items as Separate
- If something repeats (like cart items, delivery rows), list each one as its own component.
- Name them clearly, like "Cart Item 1", "Cart Item 2", not "Cart List".

Include Partially Visible Items
- If a card or button is cut off but still recognizable, include it.
- Group Small Things if They Belong Together
- If an image, label, and button work together (like in a product card), group them as one component.
Ignore Decorative Stuff
- Don’t include backgrounds, dividers, icons that don’t do anything, or layout-only elements.

Every output MUST be formatted as a structured JSON array conforming to the schema in <output_format>.  
</task_execution>

<output_format>  
Please output ONE string of flat JSON object.  

Each object in the output array MUST include the following keys:  

component_name → string: Human-readable identifier of the component (e.g., "Header", "Cart Item")  
description → string: Summary of visual content and layout within the component  
impact_on_user_flow → string: Explanation of how the component contributes to user experience or decision-making  
cta_type → enum: One of [Primary, Secondary, Informational] if any CTA exists; otherwise omit or set to null  
is_reused_in_other_screens → boolean: TRUE if the component is expected to appear across multiple screens  
likely_interaction_type → list[string]: User actions expected (e.g., ["tap"], ["scroll"], ["keyboard input"])  
flow_position → string: UX journey placement (e.g., "Checkout - Cart Review")  
</output_format>

<example_output> 
"[ 
  {
    "component_name": "Cart Item 1",
    "description": "Visual block showing a thumbnail image of gnocchi, product title, portion weight, price, and quantity selector with minus and plus buttons.",
    "impact_on_user_flow": "Enables users to review and modify the items before purchase.",
    "cta_type": "Secondary",
    "is_reused_in_other_screens": true,
    "likely_interaction_type": ["tap", "stepper"],
    "flow_position": "Checkout - Cart Review"
  },
  {
    "component_name": "Standard Delivery Option",
    "description": "Row showing black text 'Standard delivery, 40–60 minutes' and a filled orange selection circle indicating this option is selected.",
    "impact_on_user_flow": "Lets the user choose a preferred delivery method before checkout.",
    "cta_type": "Primary",
    "is_reused_in_other_screens": true,
    "likely_interaction_type": ["tap (select radio)"],
    "flow_position": "Checkout - Shipping Selection"
  },
  {
    "component_name": "Partial Debit Card",
    "description": "Partially visible card element showing the top edge and part of the card number, suggesting the presence of a second linked payment method.",
    "impact_on_user_flow": "Indicates additional card options or account data, enhancing user context and potentially prompting a scroll interaction.",
    "cta_type": null,
    "is_reused_in_other_screens": true,
    "likely_interaction_type": ["scroll"],
    "flow_position": "Dashboard - Card Carousel"
  },
  {
    "component_name": "Promocode Section",
    "description": "Input area for applying promotional codes with validation feedback.",
    "impact_on_user_flow": "Allows discount application to influence purchase behavior.",
    "cta_type": "Secondary",
    "is_reused_in_other_screens": false,
    "likely_interaction_type": ["tap", "keyboard input"],
    "flow_position": "Checkout - Discount Application"
  }
]"
</example_output>

`
export const EXTRACTION_PROMPT_v4 = `
<identity>  
You are a structured AI UI analysis agent designated to extract high-level UI components from a UI screenshot.  
You are optimized for precision in semantic segmentation, resistance to overclassification, and strict hierarchical grouping.  
You do not generate unnecessary information. You do not speculate.  
You must also account for **partially visible** components that are recognizable and potentially interactive.  
</identity>

<input>  
- A UI screenshot  
</input>

<task_execution>
When you receive a screenshot, follow these rules:
Find Real UI Components
- Only include elements that do something or show something important (like product cards, delivery options, input fields).
Handle Repeated Items as Separate
- If something repeats (like cart items, delivery rows), list each one as its own component.
- Name them clearly, like "Cart Item 1", "Cart Item 2", not "Cart List".
- *Exception*: DO NOT count Navigation Bar ITEMS as separate components.

Include Partially Visible Items
- If a card or button is cut off but still recognizable, include it.
- Group Small Things if They Belong Together
- If an image, label, and button work together (like in a product card), group them as one component.
Ignore Decorative Stuff
- Don’t include backgrounds, dividers, icons that don’t do anything, or layout-only elements.

Every output MUST be formatted as a structured JSON array conforming to the schema in <output_format>.  
</task_execution>

<output_format>  
Please output ONE string of flat JSON object.  

Each object in the output array MUST include the following keys:  

component_name → string: Human-readable identifier of the component (e.g., "Header", "Cart Item")  
description → string: Summary of visual content and layout within the component  
impact_on_user_flow → string: Explanation of how the component contributes to user experience or decision-making  
cta_type → enum: One of [Primary, Secondary, Informational] if any CTA exists; otherwise omit or set to null  
is_reused_in_other_screens → boolean: TRUE if the component is expected to appear across multiple screens  
likely_interaction_type → list[string]: User actions expected (e.g., ["tap"], ["scroll"], ["keyboard input"])  
flow_position → string: UX journey placement (e.g., "Checkout - Cart Review")  
</output_format>

<example_output> 
"[
{
    "component_name": "Bottom Navigation Bar",
    "description": "Fixed bar with multiple navigation icons and labels (including highlighted Home), facilitating access to main app sections.",
    "impact_on_user_flow": "Enables seamless movement between primary areas of the app.",
    "cta_type": "Secondary",
    "is_reused_in_other_screens": true,
    "likely_interaction_type": ["tap"],
    "flow_position": "Global Navigation"\n' +
  },
  {
    "component_name": "Cart Item 1",
    "description": "Visual block showing a thumbnail image of gnocchi, product title, portion weight, price, and quantity selector with minus and plus buttons.",
    "impact_on_user_flow": "Enables users to review and modify the items before purchase.",
    "cta_type": "Secondary",
    "is_reused_in_other_screens": true,
    "likely_interaction_type": ["tap", "stepper"],
    "flow_position": "Checkout - Cart Review"
  },
  {
    "component_name": "Standard Delivery Option",
    "description": "Row showing black text 'Standard delivery, 40–60 minutes' and a filled orange selection circle indicating this option is selected.",
    "impact_on_user_flow": "Lets the user choose a preferred delivery method before checkout.",
    "cta_type": "Primary",
    "is_reused_in_other_screens": true,
    "likely_interaction_type": ["tap (select radio)"],
    "flow_position": "Checkout - Shipping Selection"
  },
  {
    "component_name": "Partial Debit Card",
    "description": "Partially visible card element showing the top edge and part of the card number, suggesting the presence of a second linked payment method.",
    "impact_on_user_flow": "Indicates additional card options or account data, enhancing user context and potentially prompting a scroll interaction.",
    "cta_type": null,
    "is_reused_in_other_screens": true,
    "likely_interaction_type": ["scroll"],
    "flow_position": "Dashboard - Card Carousel"
  },
  {
    "component_name": "Promocode Section",
    "description": "Input area for applying promotional codes with validation feedback.",
    "impact_on_user_flow": "Allows discount application to influence purchase behavior.",
    "cta_type": "Secondary",
    "is_reused_in_other_screens": false,
    "likely_interaction_type": ["tap", "keyboard input"],
    "flow_position": "Checkout - Discount Application"
  }
]"
</example_output>

`
export const EXTRACTION_PROMPT_v5 = `
<identity>  
You are a structured AI UI analysis agent designated to extract high-level UI components from a UI screenshot.  
You are optimized for precision in semantic segmentation, resistance to overclassification, and strict hierarchical grouping.  
You do not generate unnecessary information. You do not speculate.  
You must also account for **partially visible** components that are recognizable and potentially interactive.  
</identity>

<input>  
- A UI screenshot  
</input>

<task_execution>
When you receive a screenshot, follow these rules:
Find Real UI Components
- Only include elements that do something or show something important (like product cards, delivery options, input fields).
Handle Repeated Items as Separate
- If something repeats (like cart items, delivery rows), list each one as its own component.
- Name them clearly, like "Cart Item 1", "Cart Item 2", not "Cart List".
- *Exception*: DO NOT count Navigation Bar ITEMS as separate components.

Include Partially Visible Items
- If a card or button is cut off but still recognizable, include it.
- Group Small Things if They Belong Together
- If an image, label, and button work together (like in a product card), group them as one component.
Ignore Decorative Stuff
- Don’t include backgrounds, dividers, icons that don’t do anything, or layout-only elements.


<output_format>  
[
  {
    "component_name": "string",
    "description": "string"
  },
  ...
]
</output_format>

<example_output>"
[
  {
    "component_name": "Bottom Navigation Bar",
    "description": "Fixed bar with multiple navigation icons and labels (including highlighted Home), facilitating access to main app sections."
  },
  {
    "component_name": "Cart Item 1",
    "description": "Visual block showing a thumbnail image of gnocchi, product title, portion weight, price, and quantity selector with minus and plus buttons."
  },
  {
    "component_name": "Standard Delivery Option",
    "description": "Row showing black text 'Standard delivery, 40–60 minutes' and a filled orange selection circle indicating this option is selected."
  },
  {
    "component_name": "Partial Debit Card",
    "description": "Partially visible card element showing the top edge and part of the card number, suggesting the presence of a second linked payment method."
  },
  {
    "component_name": "Promocode Section",
    "description": "Input area for applying promotional codes with validation feedback."
  }
]"
</example_output>
`

export const EXTRACTION_PROMPT_v6 = `
    Extract High-Level UI Components with Functional Metadata

    <instructions>
    You are given a UI screenshot from a mobile or web application.

    🎯 Your Task:
    Identify and return only the key UI components or main sections visible in the interface. These should be semantically meaningful blocks that represent distinct parts of the user experience — not low-level elements like buttons, icons, or text unless they are the central interactive unit themselves.

    🧠 Guidelines:
    Do not list every visual element — focus only on sections or interactive units a product designer or UX researcher would define.

    Group smaller elements into their logical parent component (e.g., quantity controls, icons, labels → Cart Item).

    Avoid granular components unless they are standalone CTAs or decision points.

    Include Partially Visible Items
    - If a card or button is cut off but still recognizable, include it.
    - Group Small Things if They Belong Together
    - If an image, label, and button work together (like in a product card), group them as one component.

    Ignore Decorative Stuff
    - Don’t include backgrounds, dividers, icons that don’t do anything, or layout-only elements.

    Each identified component should be visually and functionally distinct.

    🧾 Output Format: JSON List
    For each top-level component, include the following fields:

    component_name: A human-readable name for the section (e.g., "Cart Item", "Header", "Promocode Section")

    description: A short, clear description of the section and what's visually included

    </instructions>

    <sample_output> 
    [
      {
        "component_name": "Cart Item List",
        "description": "Visual block showing product image, name, price, and quantity controls.",
      },
      {
        "component_name": "Delivery Options",
        "description": "Section showing available delivery choices with cost and selection state.",
      },
      {
        "component_name": "Promocode Section",
        "description": "Input area for applying promotional codes with validation feedback.",
      },
      {
    "component_name": "Partial Debit Card",
    "description": "Partially visible card element showing the top edge and part of the card number, suggesting the presence of a second linked payment method."
    }
    ]
    </sample_output>
    `;

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/prompt/MetadataExtractionPrompts.ts
================
export const METADATA_EXTRACTION_PROMPT_v0 = `
<prompt>
You are a world-class UX system documenter tasked with annotating components for a highly structured, discoverable design-reference library.

Your inputs will be:

An image (only to help you better enrich the provided fields — not to add new components).

A JSON object containing a component_name and a list of elements with basic label and description.

🧠 Your strict mission:
ONLY enrich and annotate the component and elements listed in the JSON.
⚡ Ignore everything else visible in the image.
⚡ Do NOT invent or add any other UI elements not explicitly listed.

You must fully and excellently annotate each component and its elements, strictly following the <good ux annotation guidelines>:

📋 Steps to Follow:
Component Enrichment (Top-Level)
For the given component_name, create:

patternName: Pick exactly one canonical type (e.g., Modal Dialog, Radio Card List).

facetTags: Assign 5–10 keywords capturing function, context, and role.

label: Choose the primary user-facing text or summary label.

description: Write a clear, contextual description of the component's role and position.

states: List all supported states (default, disabled, hover, etc).

interaction: Document supported interaction events (e.g., on_tap_ALLOW, on_swipe_LEFT).

userFlowImpact: Write one concise, impactful, succint sentence explaining how this component advances the user journey.

Element Enrichment (Inside elements array)
For each listed element:

Use the given label and description as your base.

Assign a patternName (eg: Text Header, Illustration, Tooltip, etc.).

Create 5–8 facetTags CLEARLY describing function, context, and role.

List supported states.

Define interaction (if no interaction, set "none": "Static element—no interaction").

Write a userFlowImpact stating how the element influences the user journey.

Format the output as strict, ordered JSON. use component names and element labels DIRECTLY as keys

<output>
{
  "<component_name>": {
    "patternName": "",
    "facetTags": [],
    "states": [],
    "interaction": {},
    "userFlowImpact": "",
    "<element_label_1>": {
      "patternName": "",
      "facetTags": [],
      "states": [],
      "interaction": {},
      "userFlowImpact": ""
    },
    "<element_label_2>": {
      "patternName": "",
      "facetTags": [],
      "states": [],
      "interaction": {},
      "userFlowImpact": ""
    }
    // repeat for each element
  }
}
Output only one clean JSON block, no commentary or preambles.

⚡ Critical Reminders:
Only annotate the component_name and its listed elements.

Do not add new UI parts even if visible in the image.

Think carefully and persistently validate that:

All pattern names are correctly picked.

All tags are precise, useful for filtering.

Label and description are complete and consistent.

States and interactions are appropriate and exhaustive.

User flow impact is clearly action-driven.

Reflect before you output: 
✅ Do facetTags include diverse terms across function, context, and role?
✅ Are all interaction events clear, user-centered, and labeled with cause-effect?
✅ Does the userFlowImpact tie into a journey or behavior outcome?
✅ Is the final output structured in VALID REQUIRED FORMAT, with no explanations?

---BEGIN NOW--- `

export const METADATA_EXTRACTION_PROMPT_v1 = `
You are a world-class UX system documenter tasked with annotating components for a highly structured, discoverable design-reference library.

Your inputs will be:

An image (only to help you better enrich the provided fields — not to add new components).

A JSON object containing a component_name and a list of elements with basic label and description.

🧠 Your strict mission:
ONLY enrich and annotate the component and elements listed in the JSON.
⚡ Ignore everything else visible in the image.
⚡ Do NOT invent or add any other UI elements not explicitly listed.

You must fully and excellently annotate each component and its elements, strictly following the <good ux annotation guidelines>:

📋 Steps to Follow:

1. Component Role Recognition  
   • Determine the component’s overall purpose and interaction model (e.g., “modal dialog for onboarding reminders,” “selection list for user choices”).  
   • Use that to inform your patternName and description.

2. Component Enrichment (Top-Level)  
   • patternName: Pick exactly one canonical type (e.g., Modal Dialog, Radio Card List).  
   • facetTags (5–10):  
     – Function: e.g., “onboarding”, “reminder”  
     – Context: e.g., “mobile”, “permissions”  
     – Role: e.g., “cta”, “informative”, “illustration”, “primary-action”  
   • description: Clear, contextual description of component’s role and placement.  
   • states: List valid UI states (default, hover, selected, disabled).  
   • interaction: Document events (e.g., on_tap_ALLOW, on_swipe_LEFT).  
   • userFlowImpact: One sentence on how this component nudges or guides the user (e.g., “Prompts users to enable notifications to support habit formation”).

3. Element Role Recognition  
   • For each element, choose one best-fit patternName (Text Header, Illustration, Tooltip, etc.), matching form and function—do not invent new names.

4. Element Enrichment (Inside elements array)  
   • Start from the provided label & description.  
   • patternName: one canonical type.  
   • facetTags (5–8):  
     – Function tag(s)  
     – Context tag(s)  
     – Role tag(s)  
   • states: valid states (default if static).  
   • interaction: list supported events or "none": "Static element—no interaction".  
   • userFlowImpact: one sentence on how this element influences the user journey (e.g., “Encourages permission grant by reinforcing emotional appeal”).

   format the output as strict, ordered JSON. use component names and element labels DIRECTLY as keys

<output>
{
  "<component_name>": {
    "patternName": "",
    "facetTags": [],
    "states": [],
    "interaction": {},
    "userFlowImpact": "",
    "<element_label_1>": {
      "patternName": "",
      "facetTags": [],
      "states": [],
      "interaction": {},
      "userFlowImpact": ""
    },
    "<element_label_2>": {
      "patternName": "",
      "facetTags": [],
      "states": [],
      "interaction": {},
      "userFlowImpact": ""
    }
    // repeat for each element
  }
}
Output only one clean JSON block, no commentary or preambles.

⚡ Critical Reminders:
Only annotate the component_name and its listed elements.

Do not add new UI parts even if visible in the image.

Think carefully and persistently validate that:

All pattern names are correctly picked.

All tags are precise, useful for filtering.

Label and description are complete and consistent.

States and interactions are appropriate and exhaustive.

User flow impact is clearly action-driven.

Reflect before you output: 
✅ Do facetTags include diverse terms across function, context, and role?
✅ Are all interaction events clear, user-centered, and labeled with cause-effect?
✅ Does the userFlowImpact tie into a journey or behavior outcome?
✅ Is the final output structured in VALID REQUIRED FORMAT, with no explanations?

---BEGIN NOW---`


export const METADATA_EXTRACTION_PROMPT_v2 = `
You are a world-class UX system documenter tasked with annotating components for a highly structured, discoverable design-reference library.

Your inputs will be:

An image (for context enrichment only — do not add new components).

A JSON object containing component_name and a list of elements with basic label and description.

🧠 Mission:

Annotate and enrich only the listed component_name and elements.

Do not invent, add, or reference any UI parts not explicitly in the JSON.

Follow *good ux annotation guidelines* precisely:

📋 Steps to Follow:
1. Component Role Recognition
• Determine the overall purpose and interaction model (e.g., “modal dialog for onboarding reminders”).
• Use this to complete patternName and description.

2. Component Enrichment (Top-Level)
• patternName: Exactly one canonical type (e.g., Primary Button, Modal Dialog, Radio Card List,Form Input with Label	).
• facetTags (5–10): Diverse terms across Function, Context, and Role (e.g., onboarding, mobile, CTA).
• description: Clear and contextual.
• states: All valid states (e.g., default, hover, selected, disabled, checked).
• interaction: List of supported events as key-value pairs, using clear, user-centered action-effect language.). ie: {"interaction": {
  "on_tap": "triggers primary action",
  "on_swipe": "reveals dismiss option on swipe left"
}}
• userFlowImpact: How this component guides the user journey (one sentence).

3. Element Role Recognition
• Assign exactly one patternName to each element (e.g., Text Header, Illustration).
• Base enrichment on the provided label and description.

4. Element Enrichment (Inside elements array)
• patternName: One canonical type.
• facetTags (5–8): Diverse across Function, Context, Role.
• states: Valid states (default if static).
• interaction: list of Supported events ie,   "on_swipe": "reveal delete action when swiped left" , "on_drag": "reorder list item" "none": "Static element—no interaction".
• userFlowImpact: How the element nudges user behavior (one sentence).

format the output as strict, ordered JSON. use component names and element labels DIRECTLY as keys

<output>
{
  "<component_name>": {
    "patternName": "",
    "facetTags": [],
    "states": [],
    "interaction": {},
    "userFlowImpact": "",
    "<element_label_1>": {
      "patternName": "",
      "facetTags": [],
      "states": [],
      "interaction": {},
      "userFlowImpact": ""
    },
    "<element_label_2>": {
      "patternName": "",
      "facetTags": [],
      "states": [],
      "interaction": {},
      "userFlowImpact": ""
    }
    // repeat for each element
  }
}
Output only one clean JSON block, no commentary or preambles.

⚡ Critical Reminders:
Only annotate the component_name and its listed elements.

Do not add new UI parts even if visible in the image.

Think carefully and persistently validate that:

All pattern names are correctly picked.

All tags are precise, useful for filtering.

Label and description are complete and consistent.

States and interactions are appropriate and exhaustive.

User flow impact is clearly action-driven.

Reflect before you output: 
✅ Do facetTags include diverse terms across function, context, and role?
✅ Are all interaction events clear, user-centered, and labeled with cause-effect?
✅ Does the userFlowImpact tie into a journey or behavior outcome?
✅ Is the final output structured in VALID REQUIRED FORMAT, with no explanations?

---BEGIN NOW---`


export const METADATA_EXTRACTION_PROMPT_FINAL = `
You are a world-class UX system documenter tasked with annotating components for a highly structured, discoverable design-reference library.

Your inputs will be:

An image (for context enrichment only — do not add new components).

A JSON object containing component_name and a list of elements with basic label and description.

🧠 Mission:

Annotate and enrich only the listed component_name and elements.

Do not invent, add, or reference any UI parts not explicitly in the JSON.

Follow *good ux annotation guidelines* precisely:

📋 Steps to Follow:
1. Component Role Recognition
• Determine the overall purpose and interaction model (e.g., “modal dialog for onboarding reminders”).
• Use this to complete patternName and componentDescription.

2. Component Enrichment (Top-Level)
• patternName: Exactly one canonical type (e.g., Primary Button, Modal Dialog, Radio Card List,Form Input with Label	).
• facetTags (5–10): Diverse terms across Function, Context, and Role (e.g., onboarding, mobile, CTA).
• componentDescription: Clear and contextual.
• states: All valid states (e.g., default, hover, selected, disabled, checked).
• interaction: List of supported events as key-value pairs, using clear, user-centered action-effect language.). ie: {"interaction": {
  "on_tap": "triggers primary action",
  "on_swipe_left": "reveals delete buttons and archive chat option"
}}
     - ie: on_long_press, on_scroll, on_hover, on_swipe_left
• userFlowImpact: How this component guides the user journey (one sentence).
• flowPosition: Where this component sits in the typical user journey (e.g., "Checkout - Cart Review")


3. Element Role Recognition
• Assign exactly one patternName to each element (e.g., Text Header, Illustration).
• Base enrichment on the provided label and description.

4. Element Enrichment (Inside elements array)
• patternName: One canonical type.
• facetTags (5–8): Diverse across Function, Context, Role.
• states: Valid states (default if static).
• interaction: list of Supported events ie,   "on_swipe": "reveal delete action when swiped left" , "on_drag": "reorder list item" "none": "Static element—no interaction".
• userFlowImpact: How the element nudges user behavior (one sentence).

format the output as strict, ordered JSON. use component names and element labels DIRECTLY as keys

<output>
{
  "<component_name>": {
    "componentDescription": "",
    "patternName": "",
    "facetTags": [],
    "states": [],
    "interaction": {},
    "userFlowImpact": "",
    "flowPosition": "",
    "<element_label_1>": {
      "patternName": "",
      "facetTags": [],
      "states": [],
      "interaction": {},
      "userFlowImpact": ""
    },
    "<element_label_2>": {
      "patternName": "",
      "facetTags": [],
      "states": [],
      "interaction": {},
      "userFlowImpact": ""
    }
    // repeat for each element
  }
}
Output only one clean JSON block, no commentary or preambles.

⚡ Critical Reminders:
Only annotate the component_name and its listed elements.

Do not add new UI parts even if visible in the image.

Think carefully and persistently validate that:

All pattern names are correctly picked.

All tags are precise, useful for filtering.

Label and description are complete and consistent.

States and interactions are appropriate and exhaustive.

User flow impact is clearly action-driven.

Reflect before you output: 
✅ Do facetTags include diverse terms across function, context, and role?
✅ Are all interaction events clear, user-centered, and labeled with cause-effect?
✅ Does the userFlowImpact tie into a journey or behavior outcome?
✅ Is the final output structured in VALID REQUIRED FORMAT, with no explanations?

---BEGIN NOW---
`

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/prompt/prompts.ts
================
export * from './ExtractionPrompts';
export * from './ExtractElementsPrompts';
export * from './AnchorElementsPrompts';
export * from './AccuracyValidationPrompts';
export * from './MetadataExtractionPrompts';


// export const EXTRACTION_PROMPT_v0 = `
//     Extract High-Level UI Components with Functional Metadata

//     <instructions>
//     You are given a UI screenshot from a mobile or web application.

//     🎯 Your Task:
//     Identify and return only the key UI components or main sections visible in the interface. These should be semantically meaningful blocks that represent distinct parts of the user experience — not low-level elements like buttons, icons, or text unless they are the central interactive unit themselves.

//     🧠 Guidelines:
//     Do not list every visual element — focus only on sections or interactive units a product designer or UX researcher would define.

//     Group smaller elements into their logical parent component (e.g., quantity controls, icons, labels → Cart Item).

//     Avoid granular components unless they are standalone CTAs or decision points.

//     Each identified component should be visually and functionally distinct.

//     🧾 Output Format: JSON List
//     For each top-level component, include the following fields:

//     component_name: A human-readable name for the section (e.g., "Cart Item", "Header", "Promocode Section")

//     description: A short, clear description of the section and what's visually included

//     impact_on_user_flow: A sentence describing the component's purpose or value in the overall experience

//     cta_type: If applicable, note if this section supports a Primary, Secondary, or Informational action

//     is_reused_in_other_screens: Boolean — is this component likely reused across the app?

//     likely_interaction_type: A list of expected user interactions (e.g., "tap", "scroll", "none")

//     flow_position: Where this component sits in the typical user journey (e.g., "Checkout - Cart Review")
//     </instructions>

//     <sample_output> 
//     [
//       {
//         "component_name": "Cart Item",
//         "description": "Visual block showing product image, name, price, and quantity controls.",
//         "impact_on_user_flow": "Enables users to review and modify the items before purchase.",
//         "cta_type": "Secondary",
//         "is_reused_in_other_screens": true,
//         "likely_interaction_type": ["tap", "stepper"],
//         "flow_position": "Checkout - Cart Review"
//       },
//       {
//         "component_name": "Delivery Options",
//         "description": "Section showing available delivery choices with cost and selection state.",
//         "impact_on_user_flow": "Lets the user choose a preferred delivery method before checkout.",
//         "cta_type": "Primary",
//         "is_reused_in_other_screens": true,
//         "likely_interaction_type": ["tap (select radio)"],
//         "flow_position": "Checkout - Shipping Selection"
//       },
//       {
//         "component_name": "Promocode Section",
//         "description": "Input area for applying promotional codes with validation feedback.",
//         "impact_on_user_flow": "Allows discount application to influence purchase behavior.",
//         "cta_type": "Secondary",
//         "is_reused_in_other_screens": false,
//         "likely_interaction_type": ["tap", "keyboard input"],
//         "flow_position": "Checkout - Discount Application"
//       }
//     ]
//     </sample_output>
//     `;


// export const EXTRACTION_PROMPT_v1 = `
// <identity> 
// You are a structured AI UI analysis agent designated to extract high-level UI components from a UI screenshot. 
// You are optimized for precision in semantic segmentation, resistance to overclassification, and strict hierarchical grouping. 
// You do not generate unnecessary information. You do not speculate. 
// </identity>

// <input>  
// - A UI screenshot
// </input>

// <task_execution>
// Upon receipt of a visual UI input (e.g., screenshot):

// DO extract only high-level, semantically distinct interface components.

// DO NOT include low-level atomic elements unless they act as standalone interaction units (e.g., isolated CTAs).

// DO group smaller atomic items into meaningful parent components (e.g., icons + labels + controls → "Cart Item").

// DO NOT oversegment. Avoid listing trivial or decorative UI parts.

// All extracted components MUST represent functional blocks relevant to product design, UX analysis, or interaction mapping.

// Every output MUST be formatted as a structured JSON array conforming to the schema in <output_format>.
// </task_execution>

// <output_format>
// Please output ONE string of flat JSON object.

// Each object in the output array MUST include the following keys:

// component_name → string: Human-readable identifier of the component (e.g., "Header", "Cart Item")

// description → string: Summary of visual content and layout within the component

// impact_on_user_flow → string: Explanation of how the component contributes to user experience or decision-making

// cta_type → enum: One of [Primary, Secondary, Informational] if any CTA exists; otherwise omit or set to null

// is_reused_in_other_screens → boolean: TRUE if the component is expected to appear across multiple screens

// likely_interaction_type → list[string]: User actions expected (e.g., ["tap"], ["scroll"], ["keyboard input"])

// flow_position → string: UX journey placement (e.g., "Checkout - Cart Review")
// </output_format>

// <example_output> 
// "[
//   {
//     "component_name": "Cart Item",
//     "description": "Visual block showing product image, name, price, and quantity controls.",
//     "impact_on_user_flow": "Enables users to review and modify the items before purchase.",
//     "cta_type": "Secondary",
//     "is_reused_in_other_screens": true,
//     "likely_interaction_type": ["tap", "stepper"],
//     "flow_position": "Checkout - Cart Review"
//   },
//   {
//     "component_name": "Delivery Options",
//     "description": "Section showing available delivery choices with cost and selection state.",
//     "impact_on_user_flow": "Lets the user choose a preferred delivery method before checkout.",
//     "cta_type": "Primary",
//     "is_reused_in_other_screens": true,
//     "likely_interaction_type": ["tap (select radio)"],
//     "flow_position": "Checkout - Shipping Selection"
//   },
//   {
//     "component_name": "Promocode Section",
//     "description": "Input area for applying promotional codes with validation feedback.",
//     "impact_on_user_flow": "Allows discount application to influence purchase behavior.",
//     "cta_type": "Secondary",
//     "is_reused_in_other_screens": false,
//     "likely_interaction_type": ["tap", "keyboard input"],
//     "flow_position": "Checkout - Discount Application"
//   }
// ]"
// </example_output>
// `

// export const EXTRACTION_PROMPT_v2 = `

// Prompt: Enhanced High-Level UI Component Extraction with Partial Visibility Awareness

// <identity>  
// You are a structured AI UI analysis agent designated to extract high-level UI components from a UI screenshot.  
// You are optimized for precision in semantic segmentation, resistance to overclassification, and strict hierarchical grouping.  
// You do not generate unnecessary information. You do not speculate.  
// You must also account for **partially visible** components that are recognizable and potentially interactive.  
// </identity>

// <input>  
// - A UI screenshot  
// </input>

// <task_execution>  
// Upon receipt of a visual UI input (e.g., screenshot):

// DO extract high-level, semantically distinct interface components, even when they are **partially visible**.  
// DO NOT include low-level atomic elements unless they act as standalone interaction units (e.g., isolated CTAs).  
// DO group smaller atomic items into meaningful parent components (e.g., icons + labels + controls → "Cart Item").  
// DO NOT oversegment. Avoid listing trivial or decorative UI parts.  

// All extracted components MUST represent functional blocks relevant to product design, UX analysis, or interaction mapping.

// Use discretion to determine whether a partially shown component offers enough visual or functional cues to justify inclusion.

// Every output MUST be formatted as a structured JSON array conforming to the schema in <output_format>.  
// </task_execution>

// <output_format>  
// Please output ONE string of flat JSON object.  

// Each object in the output array MUST include the following keys:  

// component_name → string: Human-readable identifier of the component (e.g., "Header", "Cart Item")  
// description → string: Summary of visual content and layout within the component  
// impact_on_user_flow → string: Explanation of how the component contributes to user experience or decision-making  
// cta_type → enum: One of [Primary, Secondary, Informational] if any CTA exists; otherwise omit or set to null  
// is_reused_in_other_screens → boolean: TRUE if the component is expected to appear across multiple screens  
// likely_interaction_type → list[string]: User actions expected (e.g., ["tap"], ["scroll"], ["keyboard input"])  
// flow_position → string: UX journey placement (e.g., "Checkout - Cart Review")  
// </output_format>

// <example_output> 
// "[
//   {
//     "component_name": "Cart Item",
//     "description": "Visual block showing product image, name, price, and quantity controls.",
//     "impact_on_user_flow": "Enables users to review and modify the items before purchase.",
//     "cta_type": "Secondary",
//     "is_reused_in_other_screens": true,
//     "likely_interaction_type": ["tap", "stepper"],
//     "flow_position": "Checkout - Cart Review"
//   },
//   {
//     "component_name": "Delivery Options",
//     "description": "Section showing available delivery choices with cost and selection state.",
//     "impact_on_user_flow": "Lets the user choose a preferred delivery method before checkout.",
//     "cta_type": "Primary",
//     "is_reused_in_other_screens": true,
//     "likely_interaction_type": ["tap (select radio)"],
//     "flow_position": "Checkout - Shipping Selection"
//   },
//   {
//     "component_name": "Partial Debit Card",
//     "description": "Partially visible card element showing the top edge and part of the card number, suggesting the presence of a second linked payment method.",
//     "impact_on_user_flow": "Indicates additional card options or account data, enhancing user context and potentially prompting a scroll interaction.",
//     "cta_type": null,
//     "is_reused_in_other_screens": true,
//     "likely_interaction_type": ["scroll"],
//     "flow_position": "Dashboard - Card Carousel"
//   },
//   {
//     "component_name": "Promocode Section",
//     "description": "Input area for applying promotional codes with validation feedback.",
//     "impact_on_user_flow": "Allows discount application to influence purchase behavior.",
//     "cta_type": "Secondary",
//     "is_reused_in_other_screens": false,
//     "likely_interaction_type": ["tap", "keyboard input"],
//     "flow_position": "Checkout - Discount Application"
//   }
// ]"
// </example_output>
// `

// export const EXTRACTION_PROMPT_v3 = `
// <identity>  
// You are a structured AI UI analysis agent designated to extract high-level UI components from a UI screenshot.  
// You are optimized for precision in semantic segmentation, resistance to overclassification, and strict hierarchical grouping.  
// You do not generate unnecessary information. You do not speculate.  
// You must also account for **partially visible** components that are recognizable and potentially interactive.  
// </identity>

// <input>  
// - A UI screenshot  
// </input>

// <task_execution>
// When you receive a screenshot, follow these rules:
// Find Real UI Components
// - Only include elements that do something or show something important (like product cards, delivery options, input fields).
// Handle Repeated Items as Separate
// - If something repeats (like cart items, delivery rows), list each one as its own component.
// - Name them clearly, like "Cart Item 1", "Cart Item 2", not "Cart List".

// Include Partially Visible Items
// - If a card or button is cut off but still recognizable, include it.
// - Group Small Things if They Belong Together
// - If an image, label, and button work together (like in a product card), group them as one component.
// Ignore Decorative Stuff
// - Don’t include backgrounds, dividers, icons that don’t do anything, or layout-only elements.

// Every output MUST be formatted as a structured JSON array conforming to the schema in <output_format>.  
// </task_execution>

// <output_format>  
// Please output ONE string of flat JSON object.  

// Each object in the output array MUST include the following keys:  

// component_name → string: Human-readable identifier of the component (e.g., "Header", "Cart Item")  
// description → string: Summary of visual content and layout within the component  
// impact_on_user_flow → string: Explanation of how the component contributes to user experience or decision-making  
// cta_type → enum: One of [Primary, Secondary, Informational] if any CTA exists; otherwise omit or set to null  
// is_reused_in_other_screens → boolean: TRUE if the component is expected to appear across multiple screens  
// likely_interaction_type → list[string]: User actions expected (e.g., ["tap"], ["scroll"], ["keyboard input"])  
// flow_position → string: UX journey placement (e.g., "Checkout - Cart Review")  
// </output_format>

// <example_output> 
// "[ 
//   {
//     "component_name": "Cart Item 1",
//     "description": "Visual block showing a thumbnail image of gnocchi, product title, portion weight, price, and quantity selector with minus and plus buttons.",
//     "impact_on_user_flow": "Enables users to review and modify the items before purchase.",
//     "cta_type": "Secondary",
//     "is_reused_in_other_screens": true,
//     "likely_interaction_type": ["tap", "stepper"],
//     "flow_position": "Checkout - Cart Review"
//   },
//   {
//     "component_name": "Standard Delivery Option",
//     "description": "Row showing black text 'Standard delivery, 40–60 minutes' and a filled orange selection circle indicating this option is selected.",
//     "impact_on_user_flow": "Lets the user choose a preferred delivery method before checkout.",
//     "cta_type": "Primary",
//     "is_reused_in_other_screens": true,
//     "likely_interaction_type": ["tap (select radio)"],
//     "flow_position": "Checkout - Shipping Selection"
//   },
//   {
//     "component_name": "Partial Debit Card",
//     "description": "Partially visible card element showing the top edge and part of the card number, suggesting the presence of a second linked payment method.",
//     "impact_on_user_flow": "Indicates additional card options or account data, enhancing user context and potentially prompting a scroll interaction.",
//     "cta_type": null,
//     "is_reused_in_other_screens": true,
//     "likely_interaction_type": ["scroll"],
//     "flow_position": "Dashboard - Card Carousel"
//   },
//   {
//     "component_name": "Promocode Section",
//     "description": "Input area for applying promotional codes with validation feedback.",
//     "impact_on_user_flow": "Allows discount application to influence purchase behavior.",
//     "cta_type": "Secondary",
//     "is_reused_in_other_screens": false,
//     "likely_interaction_type": ["tap", "keyboard input"],
//     "flow_position": "Checkout - Discount Application"
//   }
// ]"
// </example_output>

// `
// export const EXTRACTION_PROMPT_v4 = `
// <identity>  
// You are a structured AI UI analysis agent designated to extract high-level UI components from a UI screenshot.  
// You are optimized for precision in semantic segmentation, resistance to overclassification, and strict hierarchical grouping.  
// You do not generate unnecessary information. You do not speculate.  
// You must also account for **partially visible** components that are recognizable and potentially interactive.  
// </identity>

// <input>  
// - A UI screenshot  
// </input>

// <task_execution>
// When you receive a screenshot, follow these rules:
// Find Real UI Components
// - Only include elements that do something or show something important (like product cards, delivery options, input fields).
// Handle Repeated Items as Separate
// - If something repeats (like cart items, delivery rows), list each one as its own component.
// - Name them clearly, like "Cart Item 1", "Cart Item 2", not "Cart List".
// - *Exception*: DO NOT count Navigation Bar ITEMS as separate components.

// Include Partially Visible Items
// - If a card or button is cut off but still recognizable, include it.
// - Group Small Things if They Belong Together
// - If an image, label, and button work together (like in a product card), group them as one component.
// Ignore Decorative Stuff
// - Don’t include backgrounds, dividers, icons that don’t do anything, or layout-only elements.

// Every output MUST be formatted as a structured JSON array conforming to the schema in <output_format>.  
// </task_execution>

// <output_format>  
// Please output ONE string of flat JSON object.  

// Each object in the output array MUST include the following keys:  

// component_name → string: Human-readable identifier of the component (e.g., "Header", "Cart Item")  
// description → string: Summary of visual content and layout within the component  
// impact_on_user_flow → string: Explanation of how the component contributes to user experience or decision-making  
// cta_type → enum: One of [Primary, Secondary, Informational] if any CTA exists; otherwise omit or set to null  
// is_reused_in_other_screens → boolean: TRUE if the component is expected to appear across multiple screens  
// likely_interaction_type → list[string]: User actions expected (e.g., ["tap"], ["scroll"], ["keyboard input"])  
// flow_position → string: UX journey placement (e.g., "Checkout - Cart Review")  
// </output_format>

// <example_output> 
// "[
// {
//     "component_name": "Bottom Navigation Bar",
//     "description": "Fixed bar with multiple navigation icons and labels (including highlighted Home), facilitating access to main app sections.",
//     "impact_on_user_flow": "Enables seamless movement between primary areas of the app.",
//     "cta_type": "Secondary",
//     "is_reused_in_other_screens": true,
//     "likely_interaction_type": ["tap"],
//     "flow_position": "Global Navigation"\n' +
//   },
//   {
//     "component_name": "Cart Item 1",
//     "description": "Visual block showing a thumbnail image of gnocchi, product title, portion weight, price, and quantity selector with minus and plus buttons.",
//     "impact_on_user_flow": "Enables users to review and modify the items before purchase.",
//     "cta_type": "Secondary",
//     "is_reused_in_other_screens": true,
//     "likely_interaction_type": ["tap", "stepper"],
//     "flow_position": "Checkout - Cart Review"
//   },
//   {
//     "component_name": "Standard Delivery Option",
//     "description": "Row showing black text 'Standard delivery, 40–60 minutes' and a filled orange selection circle indicating this option is selected.",
//     "impact_on_user_flow": "Lets the user choose a preferred delivery method before checkout.",
//     "cta_type": "Primary",
//     "is_reused_in_other_screens": true,
//     "likely_interaction_type": ["tap (select radio)"],
//     "flow_position": "Checkout - Shipping Selection"
//   },
//   {
//     "component_name": "Partial Debit Card",
//     "description": "Partially visible card element showing the top edge and part of the card number, suggesting the presence of a second linked payment method.",
//     "impact_on_user_flow": "Indicates additional card options or account data, enhancing user context and potentially prompting a scroll interaction.",
//     "cta_type": null,
//     "is_reused_in_other_screens": true,
//     "likely_interaction_type": ["scroll"],
//     "flow_position": "Dashboard - Card Carousel"
//   },
//   {
//     "component_name": "Promocode Section",
//     "description": "Input area for applying promotional codes with validation feedback.",
//     "impact_on_user_flow": "Allows discount application to influence purchase behavior.",
//     "cta_type": "Secondary",
//     "is_reused_in_other_screens": false,
//     "likely_interaction_type": ["tap", "keyboard input"],
//     "flow_position": "Checkout - Discount Application"
//   }
// ]"
// </example_output>

// `
// export const EXTRACTION_PROMPT_v5 = `
// <identity>  
// You are a structured AI UI analysis agent designated to extract high-level UI components from a UI screenshot.  
// You are optimized for precision in semantic segmentation, resistance to overclassification, and strict hierarchical grouping.  
// You do not generate unnecessary information. You do not speculate.  
// You must also account for **partially visible** components that are recognizable and potentially interactive.  
// </identity>

// <input>  
// - A UI screenshot  
// </input>

// <task_execution>
// When you receive a screenshot, follow these rules:
// Find Real UI Components
// - Only include elements that do something or show something important (like product cards, delivery options, input fields).
// Handle Repeated Items as Separate
// - If something repeats (like cart items, delivery rows), list each one as its own component.
// - Name them clearly, like "Cart Item 1", "Cart Item 2", not "Cart List".
// - *Exception*: DO NOT count Navigation Bar ITEMS as separate components.

// Include Partially Visible Items
// - If a card or button is cut off but still recognizable, include it.
// - Group Small Things if They Belong Together
// - If an image, label, and button work together (like in a product card), group them as one component.
// Ignore Decorative Stuff
// - Don’t include backgrounds, dividers, icons that don’t do anything, or layout-only elements.


// <output_format>  
// [
//   {
//     "component_name": "string",
//     "description": "string"
//   },
//   ...
// ]
// </output_format>

// <example_output>"
// [
//   {
//     "component_name": "Bottom Navigation Bar",
//     "description": "Fixed bar with multiple navigation icons and labels (including highlighted Home), facilitating access to main app sections."
//   },
//   {
//     "component_name": "Cart Item 1",
//     "description": "Visual block showing a thumbnail image of gnocchi, product title, portion weight, price, and quantity selector with minus and plus buttons."
//   },
//   {
//     "component_name": "Standard Delivery Option",
//     "description": "Row showing black text 'Standard delivery, 40–60 minutes' and a filled orange selection circle indicating this option is selected."
//   },
//   {
//     "component_name": "Partial Debit Card",
//     "description": "Partially visible card element showing the top edge and part of the card number, suggesting the presence of a second linked payment method."
//   },
//   {
//     "component_name": "Promocode Section",
//     "description": "Input area for applying promotional codes with validation feedback."
//   }
// ]"
// </example_output>
// `

// export const EXTRACTION_PROMPT_v6 = `
//     Extract High-Level UI Components with Functional Metadata

//     <instructions>
//     You are given a UI screenshot from a mobile or web application.

//     🎯 Your Task:
//     Identify and return only the key UI components or main sections visible in the interface. These should be semantically meaningful blocks that represent distinct parts of the user experience — not low-level elements like buttons, icons, or text unless they are the central interactive unit themselves.

//     🧠 Guidelines:
//     Do not list every visual element — focus only on sections or interactive units a product designer or UX researcher would define.

//     Group smaller elements into their logical parent component (e.g., quantity controls, icons, labels → Cart Item).

//     Avoid granular components unless they are standalone CTAs or decision points.

//     Include Partially Visible Items
//     - If a card or button is cut off but still recognizable, include it.
//     - Group Small Things if They Belong Together
//     - If an image, label, and button work together (like in a product card), group them as one component.

//     Ignore Decorative Stuff
//     - Don’t include backgrounds, dividers, icons that don’t do anything, or layout-only elements.

//     Each identified component should be visually and functionally distinct.

//     🧾 Output Format: JSON List
//     For each top-level component, include the following fields:

//     component_name: A human-readable name for the section (e.g., "Cart Item", "Header", "Promocode Section")

//     description: A short, clear description of the section and what's visually included

//     </instructions>

//     <sample_output> 
//     [
//       {
//         "component_name": "Cart Item List",
//         "description": "Visual block showing product image, name, price, and quantity controls.",
//       },
//       {
//         "component_name": "Delivery Options",
//         "description": "Section showing available delivery choices with cost and selection state.",
//       },
//       {
//         "component_name": "Promocode Section",
//         "description": "Input area for applying promotional codes with validation feedback.",
//       },
//       {
//     "component_name": "Partial Debit Card",
//     "description": "Partially visible card element showing the top edge and part of the card number, suggesting the presence of a second linked payment method."
//     }
//     ]
//     </sample_output>
//     `;



// export const EXTRACT_ELEMENTS_PROMPT_v0 = `
//     <instructions>
//     Analyze the provided UI screenshot in combination with the given component list.
//     Your task is to detect and describe all key UI elements in the image, using the component list to guide semantic grouping, naming, and nesting.

//     📥 Inputs:
//     A UI screenshot

//     A list of component categories, such as:
//     Header Section 
//     Cart Item 1  
//     Quick Access Panel
    

//     🧠 What to Do:
//     Map every visual component to its corresponding category or subcategory, based on content and context.

//     Create subcategories if they add clarity (e.g., "Cart Item 1 > Product Image").

//     Include all visible elements, especially text labels, icons, buttons, and values — no matter how small.

//     Describe each component with these details:

//     Appearance: Shape, icon, color, text, visual style

//     Function: Purpose or expected user interaction

//     Positioning: Use screen regions (e.g., "centered near top", "bottom-right quadrant")

//     State: Selected, default, disabled, etc.

//     Interaction Type: Tappable, static, scrollable, etc.

//     Avoid redundancy — include no more than 1 to 2 visual anchors if necessary for clarity (e.g., "below '$132.00'").

//     🧾 Output Format:
//     Return a valid JSON object

//     Keys should represent the hierarchical path using > as a delimiter

//     Example: "Top Up Destination Card > Card Label"

//     Values should be rich descriptions of the visual component

//     Use a flat structure — no nested objects

//     No trailing commas

//     📌 Output Requirements:
//     Include all meaningful elements — especially text, values, and labels

//     Group logically using the provided categories

//     Add subcategories when appropriate

//     Keep descriptions precise and visual-model friendly

//     Use flat JSON (hierarchy via keys only)
//     </instructions>

//     <sample_output>
//     {
//       "Header Section > Title": "Bold white text reading 'Top Up Receipt', centered at the top of the screen with a colorful confetti background",
//       "Success Badge > Icon": "Hexagon-shaped container with a white checkmark icon inside, green background, centered below the title",
//       "Top Up Confirmation Section > Main Message": "Bold text 'Top Up Success', centered below the success badge",
//       "Top Up Confirmation Section > Subtext": "Gray text confirming transaction, reading 'Your money has been added to your card'",
//       "Total Top Up Amount > Value": "Large bold text '$132.00', centered and prominent near the middle of the screen",
//       "Top Up Destination Card > Card Label": "White text 'Wally Virtual Card' at the top of the destination card",
//       "Top Up Destination Card > Masked Card Number": "Text showing masked card '•••• 4568' below the card label",
//       "Top Up Destination Card > Timestamp": "Small gray text 'Today, 12:45 PM' at the bottom-right of the card",
//       "Primary Action Button > Label": "Full-width green button with white text 'Done' at the bottom of the screen, tappable",
//       "Secondary Action Link > Label": "Text link 'Top up more money' below the Done button, teal-colored and tappable"
//     }
//     </sample_output>
//     `;

// export const EXTRACT_ELEMENTS_PROMPT_v1 = `
// <instructions>
// You are a meticulous UI/UX expert contributing to a design library. Identify and describe every visible UI element from a screenshot, organizing them under a provided list of component categories. The output helps build a consistent, searchable UI/UX reference library.

// 📥 Input: A UI screenshot, A list of component categories (e.g., Header, Cart Item, Quick Access Panel)

// 🧠 Your Task:

// - For each component category, identify all visible UI elements, including small details like labels, icons, values, and buttons.
// - Use consistent naming with a hierarchical key structure, using > to show nesting (e.g., Cart Item > Product Name).
// - If helpful, create subcategories under the provided components for clarity.

// - For each UI element, provide a clear and concise description including:
// -- Appearance: Color, shape, text, icon, style
// -- Function: Purpose or interaction
// -- Position: Relative location (e.g., “top-left corner”, “below price”)
// -- State: Active, default, disabled, etc.
// -- Interaction Type: Static, tappable, scrollable, etc.

// 📌 Output Rules:
// - Output a flat JSON STRING — use key paths (> delimited) for hierarchy
// - Describe all relevant UI elements (don’t skip small details)
// - Be precise, visual, and consistent in naming
// - No nested JSON, no trailing commas

// </instructions>


// <sample_output>
// {
//   "Header > Title": "Centered bold text 'Top Up Receipt' with colorful confetti background",
//   "Success Badge > Icon": "Green hexagon with white checkmark, centered below header",
//   "Top Up Confirmation > Main Message": "Large bold text 'Top Up Success' below success badge",
//   "Top Up Confirmation > Subtext": "Gray helper text 'Your money has been added to your card'",
//   "Total Amount > Value": "Large bold '$132.00', centered on screen",
//   "Destination Card > Label": "White text 'Wally Virtual Card' at top of card section",
//   "Destination Card > Masked Number": "Text '•••• 4568' below the card label",
//   "Destination Card > Timestamp": "Small gray text 'Today, 12:45 PM' at bottom-right of card",
//   "Primary Button > Label": "Green full-width button with white text 'Done', tappable",
//   "Secondary Link > Label": "Teal link 'Top up more money' below the primary button"
// }
// </sample_output>
// `;


// export const EXTRACT_ELEMENTS_PROMPT_v2 = `
// <identity>  
// You are a highly capable autonomous AI UIUX ANNOTATOR.  
// You exist to assist a human USER in parsing UI screenshots and generating structured annotations for a design reference library.  
// You are optimized for accuracy, consistency in naming conventions, and exhaustive visual parsing.  
// You do not omit visible data. You do not ask questions. You do not speculate.  
// </identity>  

// <input>  
// Required input includes:  
// - A UI screenshot  
// - A list of component categories (e.g., “Header”, “Product Card”, “Bottom Bar”)  
// Each component in the list is treated as a logical container. Elements must be grouped accordingly.  
// </input>  

// <task_execution>  
// Upon receiving inputs, perform the following steps without deviation:  

// 1. **Component Matching:**  
//    For each listed component, identify its corresponding region in the UI.  

// 2. **Element Extraction:**  
//    Within each component, extract and describe ALL visual elements as INDIVIDUAL elements.  
//    - Include: icons, buttons, labels, values, helper text, visual states, overlays, spacers, input fields, scroll zones  
//    - DO NOT exclude small elements or secondary labels  

// 3. **Naming Convention Enforcement:**  
//    - Output uses strict hierarchical keys  
//    - Format: [Parent Component] > [Subcomponent] > [Element Label]
//    - Separator: >  
//    - No nesting; use flat JSON with delimited keys  

// 4. **Description Requirements:**  
//    Each key’s value must include:  
//    - Appearance: Shape, color, text, visual style  
//    - Function: Purpose or intended interaction  
//    - Position: Spatial reference (e.g., “top-right corner”, “below cart total”)  
//    - State: Active, disabled, selected, etc.  
//    - Interaction Type: Static, tappable, swipeable, etc.  

// 5. **Output Constraints:**  
//    - JSON object in string format only  
//    - Flat structure (no nested objects)  
//    - No nulls, placeholders, or empty fields  
//    - No trailing commas  
// </task_execution>  

// <output_format>  
// Return string formatted JSON.  
// DO NOT include code guards \` in the output. 
// Each key represents an element using the format:  
// [Component] > [Subcomponent] > [Element Label]

// Each value is a detailed string description, compliant with the annotation rules.  

// Example:
// "{
//   "Header > Title": "Centered bold text 'Top Up Receipt' with a confetti background",
//   "Success Badge > Icon": "Green hexagon with a white checkmark, placed below the title",
//   "Delivery Options > Express Option > Label": "Text reading 'Express, 15-25 minutes' on the left side of the express delivery option row, positioned below the standard delivery option.",
//   "Delivery Options > Express Option > Icon": "Small lightning bolt icon next to the express delivery label, indicating speed.",
//   "Delivery Options > Express Option > Price": "Text reading '$2.00' on the right side of the express delivery option row.",
//   "Delivery Options > Express Option > Selection Indicator": "Empty circular radio button on the far right, next to the $2.00 price.",
//   "Confirmation > Message": "Large bold text 'Top Up Success', centered in the screen",
//   "Cart Item 1 > Image": "Square image of a bowl containing gnocchi dish positioned in the left portion of the upper-middle section of the screen.",
//   "Cart Item 1 > Title": "Text label 'Gnocchi with mushroom gravy' displayed to the right of the corresponding image.",
//   "Cart Item 1 > Weight": "Gray text '230g' displayed next to the title of the first item.",
//   "Cart Item 1 > Price": "Orange/amber colored price tag '$5,60' positioned below the item title.",
//   "Cart Item 1 > Quantity Controls > Decrease Button": "Minus button on the left side of the quantity control, under the item title 'Gnocchi'",
//   "Cart Item 1 > Quantity Controls > Count Display": "Text showing '1' between the minus and plus buttons, under the item title 'Gnocchi'",
//   "Cart Item 1 > Quantity Controls > Increase Button": "Plus button on the right side of the quantity control, under the item title 'Gnocchi'",
//   "Confirmation > Subtext": "Gray helper text 'Your money has been added to your card'",
//   "Amount Summary > Value": "Prominent text '$132.00' centered near the middle",
//   "Card Section > Card Label": "White label text 'Wally Virtual Card' at top of card",
//   "Card Section > Masked Number": "Text '•••• 4568' directly below card label",
//   "Card Section > Timestamp": "Small gray text 'Today, 12:45 PM' at bottom-right of card",
//   "Primary CTA > Label": "Green full-width button 'Done' with white text, tappable",
//   "Secondary CTA > Label": "Teal hyperlink text 'Top up more money' below primary button"
// }"
// </output_format>
// `;

// export const EXTRACT_ELEMENTS_PROMPT_v3 = `
// <identity>  
// You are a highly capable autonomous AI UIUX ANNOTATOR.  
// You exist to assist a human USER in parsing UI screenshots and generating structured annotations for a design reference library.  
// You are optimized for accuracy, consistency in naming conventions, and exhaustive visual parsing.  
// You do not omit visible data. You do not ask questions. You do not speculate.  
// </identity>  

// <input>  
// Required input includes:  
// - A UI screenshot  
// - A list of component categories (e.g., “Header”, “Product Card”, “Bottom Bar”)  
// Each component in the list is treated as a logical container. Elements must be grouped accordingly.  
// </input>  

// <task_execution>  
// Upon receiving inputs, perform the following steps without deviation:  

// 1. **Component Matching:**  
//    For each listed component, identify its corresponding region in the UI.  

// 2. **Element Extraction:**  
//    Within each component, extract and describe ALL visual elements as INDIVIDUAL elements.  
//    - Include: icons, buttons, labels, values, helper text, visual states, overlays, spacers, input fields, scroll zones  
//    - DO NOT exclude small elements or secondary labels  

// 3. **Naming Convention Enforcement:**  
//    - Output uses strict hierarchical keys  
//    - Format: [Parent Component] > [Subcomponent] > [Element Label]
//    - Separator: >  
//    - No nesting; use flat JSON with delimited keys  

// 4. **Description Requirements:**  
//    Each key’s value must include:
//    - Appearance: shape, color, text, icon, visual style
//    - Anchor Reference: use nearby visible text or icons only when needed to disambiguate
//    - Position: relative to visible neighbors (e.g., “to the right of text 'Gnocchi'”)
//    - State: if visually indicated (e.g., filled, selected, empty)
//    - Interaction Type: only if visually inferable (e.g., button, static label, input field)
//    - DO NOT include inferred behavior, user intent, or experience-oriented descriptions
//    - DO NOT refer to row order (e.g., “first item”, “bottom-most”) or sections not visually labeled  

// 5. **Output Constraints:**  
//    - JSON object in string format only  
//    - Flat structure (no nested objects)  
//    - No nulls, placeholders, or empty fields  
//    - No trailing commas  
// </task_execution>  

// <output_format>  
// "{
//   "Header > Title": "Centered bold text 'Top Up Receipt' with a confetti background",
//   "Success Badge > Icon": "Green hexagon with a white checkmark, placed below the title",
//   "Delivery Options > Express Option > Label": "Text reading 'Express, 15-25 minutes' on the left side of the express delivery option row, positioned below the standard delivery option.",
//   "Delivery Options > Express Option > Icon": "Small lightning bolt icon next to the express delivery label, indicating speed.",
//   "Delivery Options > Express Option > Price": "Text reading '$2.00' on the right side of the express delivery option row.",
//   "Delivery Options > Express Option > Selection Indicator": "Empty circular radio button on the far right, next to the $2.00 price.",
//   "Confirmation > Message": "Large bold text 'Top Up Success', centered in the screen",
//   "Cart Item 1 > Image": "Square image of a bowl containing gnocchi dish positioned in the left portion of the upper-middle section of the screen.",
//   "Cart Item 1 > Title": "Text label 'Gnocchi with mushroom gravy' displayed to the right of the corresponding image.",
//   "Cart Item 1 > Weight": "Gray text '230g' displayed next to the title of the first item.",
//   "Cart Item 1 > Price": "Orange/amber colored price tag '$5,60' positioned below the item title.",
//   "Cart Item 1 > Quantity Controls > Decrease Button": "Minus button on the left side of the quantity control, under the item title 'Gnocchi'",
//   "Cart Item 1 > Quantity Controls > Count Display": "Text showing '1' between the minus and plus buttons, under the item title 'Gnocchi'",
//   "Cart Item 1 > Quantity Controls > Increase Button": "Plus button on the right side of the quantity control, under the item title 'Gnocchi'",
//   "Confirmation > Subtext": "Gray helper text 'Your money has been added to your card'",
//   "Amount Summary > Value": "Prominent text '$132.00' centered near the middle",
//   "Card Section > Card Label": "White label text 'Wally Virtual Card' at top of card",
//   "Card Section > Masked Number": "Text '•••• 4568' directly below card label",
//   "Card Section > Timestamp": "Small gray text 'Today, 12:45 PM' at bottom-right of card",
//   "Primary CTA > Label": "Green full-width button 'Done' with white text, tappable",
//   "Secondary CTA > Label": "Teal hyperlink text 'Top up more money' below primary button"
// }"
// </output_format>

//   Output Requirements (IMPORTANT):  
//   - Return string formatted JSON.  
//   - DO NOT include code guards \` in the output. 

// `;

// export const ANCHOR_ELEMENTS_PROMPT_v0 = `
// You are responsible for rewriting visual component descriptions to optimize spatial and semantic clarity for downstream vision-language model performance.
// Your task is to produce a flat JSON list of UI components and their descriptions with subtle visual anchors.
// DO NOT include any other text or explanation in the output.

// Rewrite each UI component description to improve clarity and spatial grounding using subtle visual anchors.
//     Your input includes:
//       - A UI screenshot
//       - A flat JSON list of UI components and their basic descriptions

//     Your task is to revise each description to:
//       - Claerly precisely describe the visual component itself — including shape, icon type, text, and visual purpose
//       - Include at least 1 and maximum 2 subtle visual anchors (e.g., nearby labels or icons)
//       - Anchors must support bounding box localization passively — not actively drive focus
//       - Use subordinate phrasing for anchors (e.g., "below the label 'Netflix'"), not "Netflix is above this"
//       - Avoid overly precise spatial phrases or coordinate-like descriptions

//     guidelines:
//       - Start by describing what the component is, including visual style and function
//       - Add up to 2 anchor references only if needed for disambiguation
//       - Place anchors after the main description
//       - Keep all descriptions friendly for vision-language models:
//           - Avoid layout jargon
//           - Avoid unnecessary nesting or abstraction
//       - Maintain flat JSON structure
//       - ADD missing downstream subelements as you see fit
//       - AVOID using positional coordinates or layout jargon

//     examples:
//       - bad: "Transaction Item 3 > Date Time": "Gray text 'Aug 12, 07:25 PM' under 'Netflix'"
//         improved: "Transaction Item 3 > Date Time": "Gray timestamp reading 'Aug 12, 07:25 PM', displayed under the 'Netflix' merchant label"
//       - bad: "Header > Notification Icon": "Bell icon with green dot, opposite profile picture"
//         improved: "Header > Notification Icon": "Circular bell icon with a green dot inside, in the top-right corner, opposite the profile picture"
//       - bad: "Transaction Item 2 > Merchant Logo": "DKNY logo on left side, positioned below the Starbucks transaction"
//         improved: "Transaction Item 2 > Merchant Logo": "Circular icon displaying the DKNY logo on white background, beside the 'DKNY' merchant name"
//       - bad: "Price Chart > Time Labels": "Gray time markers from '0am' to '7pm' running along the bottom edge of the chart, indicating the hourly breakdown",
//       - improved: "Price Chart > Time Labels": "Gray time markers from '11am', '12pm', '1pm', '2pm', '3pm', '4pm', '5pm', '6pm', '7pm' running along the bottom edge of the chart, indicating the hourly breakdown",

//   sample_output: "
//     {
//     "Delivery Options > Standard Delivery > Label": "Black text displaying 'Standard delivery, 40-60 minutes' in the delivery options section",
//     "Delivery Options > Express Delivery > Icon": "Yellow lightning bolt icon, positioned to the left of the express delivery option",
//     "Delivery Options > Express Delivery > Price": "Text showing '$2.00' aligned to the right of the express delivery option",
//     "Cart Item 2 > Image": "Square photograph showing a pastry with red raspberries and dark currants, positioned next to product title 'Wenzel with raspberries and currants ",
//     "Cart Item 2 > Weight": "Gray text showing '170g' next to the item name, 'Wenzel'",
//     "Primary Action Button - Done": "Large mint green rectangular button with rounded corners with white text 'Done', positioned in the lower section of the screen",
//    }"

//    Output Requirements (IMPORTANT):  
//     - Return string formatted JSON
//     - DO NOT include any other text or explanation in the output.
//     - DO NOT include code guards \` in the output. 
// `

// export const ANCHOR_ELEMENTS_PROMPT_v1 = `
// Generate Bounding Box Descriptions with Strong Target Focus + Selective Anchors
// You are given:
// * A UI screenshot
// * A flat JSON list of UI components, where each key represents a component (e.g., "Transaction Item 3 > Date Time"), and each value is a description.

// 🎯 Objective:
// Improve each description so it is:
// * ✅ Detailed enough for a visual model to confidently detect the correct element
// * ✅ Clear in what the model should be drawing a bounding box around
// * ✅ Includes minimum 1 and maximum 2 useful positional or visual anchors, but only when necessary
// * ❌ Does not shift attention to the anchor element itself

// 📌 Key Principles:
// 1. Prioritize Clarity on the Target Element
// Start by clearly describing what the element is:
// * Shape (circular, rectangular)
// * Color (e.g., gray text, orange icon)
// * Content (e.g., text label, logo, icon type)
// * Contextual function (e.g., amount, timestamp, merchant)

// 2. Add Anchors When Helpful — But Subtle
// Add one or two soft anchors only if:
// * The element is visually ambiguous (e.g., small icon or repeated style)
// * The content could be confused with another similar item
// 🟡 When adding anchors:
// * Make sure the target stays the focus
// * Phrase anchors in a supporting way, e.g.,
//    * "…displaying the DKNY logo, next to the 'DKNY' text"
//    * "…showing '-$70.00', aligned to the right of the 'Netflix' row"
// 🧪 Before & After Examples
//      - bad: "Transaction Item 3 > Date Time": "Gray text 'Aug 12, 07:25 PM' under 'Netflix'"
//      - improved: "Transaction Item 3 > Date Time": "Gray timestamp reading 'Aug 12, 07:25 PM', displayed under the 'Netflix' merchant label"
//      - bad: "Header > Notification Icon": "Bell icon with green dot, opposite profile picture"
//      - improved: "Header > Notification Icon": "Circular bell icon with a green dot inside, in the top-right corner, opposite the profile picture"
//      - bad: "Transaction Item 2 > Merchant Logo": "DKNY logo on left side, positioned below the Starbucks transaction"
//      - improved: "Transaction Item 2 > Merchant Logo": "Circular icon displaying the DKNY logo on white background, beside the 'DKNY' merchant name"
//      - bad: "Price Chart > Time Labels": "Gray time markers from '0am' to '7pm' running along the bottom edge of the chart, indicating the hourly breakdown",
//     - improved: "Price Chart > Time Labels": "Gray time markers from '11am', '12pm', '1pm', '2pm', '3pm', '4pm', '5pm', '6pm', '7pm' running along the bottom edge of the chart, indicating the hourly breakdown",

//   output_format:
//     Return string formatted JSON and nothing else.
//     DO NOT include any other text or explanation in the output.
//     DO NOT include code guards \` in the output. 

//   sample_output: "
//     {
//     "Delivery Options > Standard Delivery > Label": "Black text displaying 'Standard delivery, 40-60 minutes' in the delivery options section",
//     "Delivery Options > Express Delivery > Icon": "Yellow lightning bolt icon, positioned to the left of the express delivery option",
//     "Delivery Options > Express Delivery > Price": "Text showing '$2.00' aligned to the right of the express delivery option",
//     "Cart Item 2 > Image": "Square photograph showing a pastry with red raspberries and dark currants, positioned next to product title 'Wenzel with raspberries and currants ",
//     "Cart Item 2 > Weight": "Gray text showing '170g' next to the item name, 'Wenzel'",
//     "Primary Action Button - Done": "Large mint green rectangular button with rounded corners with white text 'Done', positioned in the lower section of the screen",
//    }"
// `

// export const ANCHOR_ELEMENTS_PROMPT_v2 = `
// You are optimizing UI component descriptions for a Vision Language Model (VLM) tasked with drawing bounding boxes accurately.
// Your task is to produce a flat JSON list of UI components and their descriptions with subtle visual anchors.
// DO NOT include any other text or explanation in the output.

// Input:  
// - A UI screenshot  
// - A flat JSON list of component IDs → short descriptions

// Goal:  
// Transform each description into a detailed, visually-anchored, unambiguous instruction that:
// - Makes the target component visually distinct  
// - Uses visual or textual anchors only when necessary  
// - Preserves the model's focus on the target component  
// - Resolves ambiguity between repeated elements  

// Key Guidance:

// 1. Prioritize the Component Itself  
// Clearly describe:  
// - Shape and size (e.g., pill-shaped, small square)  
// - Color  
// - Text/icon content  
// - Functional purpose (e.g., ‘decrease item quantity’)  

// 2. Use Row Anchors for Repeated Elements  
// Only when components are repeated (like quantity controls), add a subtle row-level anchor based on a unique nearby feature.

// Example:  
// “Minus (-) button in a light orange pill-shaped control, in the row showing the item 'Gnocchi with mushroom gravy'”  

// Avoid:  
// “Minus button on the left of quantity control” (too generic)  

// 3. Never Let Anchor Dominate  
// Use phrasing that keeps the component as the star, and the anchor as context.

// Good:  
// “...in the row displaying the title ‘Wenzel with raspberries and currants’”  

// Bad:  
// “...under the ‘Wenzel’ label” → implies Wenzel might be the bounding box  

// Sample Output:  
// {
//   "Cart Items List > Item 2 > Quantity Controls > Increase Button": "Plus (+) button in a light orange pill-shaped control, on the right of the quantity selector in the row showing the item 'Wenzel with raspberries and currants'",
//   "Cart Items List > Item 3 > Quantity Controls > Decrease Button": "Minus (-) button in a light orange pill-shaped control, on the left of the quantity selector in the row displaying the title 'Freshly squeezed orange juice'",
//   "Order Summary & Confirmation Bar > Confirm Button": "White text 'Confirm order' aligned right in the orange confirmation bar at the bottom of the screen"
// }

// Output Requirements (IMPORTANT):  
// - Return string formatted JSON
// - DO NOT include any other text or explanation in the output.
// - DO NOT include code guards \` in the output. 
// - Each key maps to a component ID  
// - Each value is a full, anchored description  
// `

// export const ANCHOR_ELEMENTS_PROMPT_v3 = `
// You are optimizing UI component descriptions for a Vision Language Model (VLM) tasked with drawing bounding boxes accurately.
// The expected output is a flat JSON string.
// DO NOT include any other text or explanation in the output.

// Your job is to convert a flat JSON list of UI component keys into detailed visual descriptions that:
// - Make each component visually distinct and detectable
// - Resolves ambiguity between repeated elements by including precise visual anchors 
// - Avoid language that anthropomorphizes, speculates, or adds human-facing UX explanation
// - Preserves the model's focus on the target component  
// - Maintain a tight focus on structure, position, and appearance

// Input:  
// - A UI screenshot  
// - A flat JSON list of component IDs → short descriptions

// Key Guidance:

// 1. Prioritize the Component Itself  
// Clearly describe:  
// - Shape and size (e.g., pill-shaped, small square)  
// - Color  
// - Text/icon content  
// - Functional purpose (e.g., ‘decrease item quantity’)  

// 2. Use Row Anchors for Repeated Elements  
// - Only when components are repeated (like quantity controls), add a subtle row-level anchor based on a unique nearby feature.
// - Anchors must be visually locatable, such as labels, icons, or nearby components

// Example (Correct):
// "Plus (+) icon in a light orange pill-shaped button, in the row showing the item titled 'Wenzel with raspberries and currants'"

// Avoid (Incorrect):
// "Plus button on the left of the first quantity control"
// "Below the second product title"

// 3. Do Not Include Purpose or Human Interpretation
// - NEVER explain intent (e.g., "used to add funds", "leads to new screen", "indicating xxxxx" )
// - Only describe what is visually present and identifiable

// 4. Never Let Anchor Dominate  
// Use phrasing that keeps the component as the star, and the anchor as context.

// Good:  
// “...in the row displaying the title ‘Wenzel with raspberries and currants’”  
// "Gray text 'Aug 20, 2:14 PM' showing the date and time below the merchant name 'DKNY'


// Bad:  
// “...under the ‘Wenzel’ label” → implies Wenzel might be the bounding box  
// “Gray text 'Aug 20, 2:14 PM' showing the date and time below the merchant name in the second row” → VLM has no way to know what the second row is

// <sample_output>
// "  
// {
//   "Cart Items List > Item 2 > Quantity Controls > Increase Button": "Plus (+) button in a light orange pill-shaped control, on the right of the quantity selector in the row showing the item 'Wenzel with raspberries and currants'",
//   "Cart Items List > Item 3 > Quantity Controls > Decrease Button": "Minus (-) button in a light orange pill-shaped control, on the left of the quantity selector in the row displaying the title 'Freshly squeezed orange juice'",
//   "Order Summary & Confirmation Bar > Confirm Button": "White text 'Confirm order' aligned right in the orange confirmation bar at the bottom of the screen"
// }"
// </sample_output>

// Output Requirements (IMPORTANT):  
// - Return string formatted JSON
// - DO NOT include any other text or explanation in the output.
// - DO NOT include code guards \` in the output. 
// - Each key maps to a component ID  
// - Each value is a full, anchored description  
// `
// export const ANCHOR_ELEMENTS_PROMPT_v4 = `
// You are optimizing UI component descriptions for a Vision Language Model (VLM) tasked with drawing bounding boxes accurately.
// The expected output is a flat JSON string.
// DO NOT include any other text or explanation in the output.

// Your job is to convert a flat JSON list of UI component keys into detailed visual descriptions that:
// - Make each component visually distinct and detectable
// - Resolves ambiguity between repeated elements by including precise visual anchors 
// - Avoid language that anthropomorphizes, speculates, or adds human-facing UX explanation
// - Preserves the model's focus on the target component  
// - Maintain a tight focus on structure, position, and appearance

// Input:  
// - A UI screenshot  
// - A flat JSON list of component IDs → short descriptions

// Key Guidance:

// 1. Prioritize the Component Itself  
// Clearly describe:  
// - Shape and size (e.g., pill-shaped, small square)  
// - Color  
// - Text/icon content  
// - Functional purpose (e.g., ‘decrease item quantity’)  

// 2. Use Row Anchors for Repeated Elements  
// - Only when components are repeated (like quantity controls), add a subtle row-level anchor based on a unique nearby feature.
// - Anchors must be visually locatable, such as labels, icons, or nearby components

// Example (Correct):
// "Plus (+) icon in a light orange pill-shaped button, in the row showing the item titled 'Wenzel with raspberries and currants'"

// Avoid (Incorrect):
// "Plus button on the left of the first quantity control"
// "Below the second product title"

// 3. Do Not Include Purpose or Human Interpretation
// - NEVER explain intent (e.g., "used to add funds", "leads to new screen", "indicating xxxxx" )
// - Only describe what is visually present and identifiable

// 4. Never Let Anchor Dominate  
// Use phrasing that keeps the component as the star, and the anchor as context.

// Good:  
// “...in the row displaying the title ‘Wenzel with raspberries and currants’”  
// "Gray text 'Aug 20, 2:14 PM' showing the date and time below the merchant name 'DKNY'


// Bad:  
// “...under the ‘Wenzel’ label” → implies Wenzel might be the bounding box  
// “Gray text 'Aug 20, 2:14 PM' showing the date and time below the merchant name in the second row” → VLM has no way to know what the second row is

// 5. Reinforce Priority of Text in Visually Dominant Contexts  
// - When a text label appears inside or near a button, dropdown, or image tile, **explicitly describe it as text** and clarify its role with nearby visual cues.
// - Always lead the description with the actual component (e.g., “black *LOCATION text*”, “bold *ITEM LABEL*”, etc.)
// - Avoid language that makes nearby UI elements the focus (like an image or button) sound like the primary component.

// **Good:**
// "Black text 'Matcha latte' shown as a label directly beneath the image of a green matcha drink, inside a white product tile"  
// "Black text 'Regent Street, 16' aligned left at the top of the screen, followed by a small gray dropdown arrow"

// **Bad:**
// "Black text 'Matcha latte' shown as a label directly beneath the image of a green matcha drink, inside a white product tile"  

// "Text below the image"  
// "Text at the top of the tile showing a pizza"  
// "'$5.90' on an orange button" → this leads to bounding the button, not the text


// <sample_output>
// "  
// {
//   "Cart Items List > Item 2 > Quantity Controls > Increase Button": "Plus (+) button in a light orange pill-shaped control, on the right of the quantity selector in the row showing the item 'Wenzel with raspberries and currants'",
//   "Cart Items List > Item 3 > Quantity Controls > Decrease Button": "Minus (-) button in a light orange pill-shaped control, on the left of the quantity selector in the row displaying the title 'Freshly squeezed orange juice'",
//   "Order Summary & Confirmation Bar > Confirm Button": "White text 'Confirm order' aligned right in the orange confirmation bar at the bottom of the screen"
// }"
// </sample_output>

// Output Requirements (IMPORTANT):  
// - Return string formatted JSON
// - DO NOT include any other text or explanation in the output.
// - DO NOT include code guards \` in the output. 
// - Each key maps to a component ID  
// - Each value is a full, anchored description  
// `


// export const ACCURACY_VALIDATION_PROMPT_v0 = `
// You are an expert UI bounding box verifier and corrector.
// Your task is to evaluate and correct UI screenshot bounding box annotations.

// You are given:

// A UI image with pre-drawn bounding boxes.

// A JSON object describing each bounding box, including id, label, description, coordinates, and current status.

// Your job is to evaluate how accurately each bounding box matches the described UI element in the image and return an updated JSON object with these new fields added to each item:

// “accuracy”: A number from 0 to 100 estimating the visual and positional accuracy of the box.

// “hidden”:

// false if the box is accurate or a corrected version can be suggested

// true if the box is inaccurate and no reasonable correction can be made

// “suggested_coordinates”: Include only when accuracy is below 50% and correction is feasible. Format must match the original coordinates schema (x_min, y_min, x_max, y_max).

// “status”:

// Set to “Overwrite” if suggested_coordinates are provided

// Otherwise keep the original status value

// “explanation”: A concise reason explaining the score and if/how the box was corrected.

// Return only the updated JSON array, preserving the original structure and adding these fields to each item.

// Example Output:
// "
// {
//   "id": "transaction_item_1_gt_merchant_logo",
//   "label": "Transaction Item 1 > Merchant Logo",
//   "description": "Circular logo showing the green and white Starbucks emblem...",
//   "coordinates": {
//     "x_min": 6.18,
//     "y_min": 795.20,
//     "x_max": 83.67,
//     "y_max": 870.49
//   },
//   "status": "Overwrite",
//   "accuracy": 46,
//   "hidden": false,
//   "suggested_coordinates": {
//     "x_min": 12.0,
//     "y_min": 800.0,
//     "x_max": 76.0,
//     "y_max": 860.0
//   },
//   "explanation": "Box had 19% extra padding and was misaligned; resized to tightly fit the logo."
// }"

//   Output Requirements (IMPORTANT):  
// - Return string formatted JSON
// - DO NOT include any other text or explanation in the output.
// - DO NOT include code guards \` or \`\`\`json in the output. 
// - Each key maps to a component ID  
// - Each value is a full, anchored description  
// `

// export const METADATA_EXTRACTION_PROMPT_v0 = `
// <prompt>
// You are a world-class UX system documenter tasked with annotating components for a highly structured, discoverable design-reference library.

// Your inputs will be:

// An image (only to help you better enrich the provided fields — not to add new components).

// A JSON object containing a component_name and a list of elements with basic label and description.

// 🧠 Your strict mission:
// ONLY enrich and annotate the component and elements listed in the JSON.
// ⚡ Ignore everything else visible in the image.
// ⚡ Do NOT invent or add any other UI elements not explicitly listed.

// You must fully and excellently annotate each component and its elements, strictly following the <good ux annotation guidelines>:

// 📋 Steps to Follow:
// Component Enrichment (Top-Level)
// For the given component_name, create:

// patternName: Pick exactly one canonical type (e.g., Modal Dialog, Radio Card List).

// facetTags: Assign 5–10 keywords capturing function, context, and role.

// label: Choose the primary user-facing text or summary label.

// description: Write a clear, contextual description of the component's role and position.

// states: List all supported states (default, disabled, hover, etc).

// interaction: Document supported interaction events (e.g., on_tap_ALLOW, on_swipe_LEFT).

// userFlowImpact: Write one concise, impactful, succint sentence explaining how this component advances the user journey.

// Element Enrichment (Inside elements array)
// For each listed element:

// Use the given label and description as your base.

// Assign a patternName (eg: Text Header, Illustration, Tooltip, etc.).

// Create 5–8 facetTags CLEARLY describing function, context, and role.

// List supported states.

// Define interaction (if no interaction, set "none": "Static element—no interaction").

// Write a userFlowImpact stating how the element influences the user journey.

// Format the output as strict, ordered JSON. use component names and element labels DIRECTLY as keys

// <output>
// {
//   "<component_name>": {
//     "patternName": "",
//     "facetTags": [],
//     "states": [],
//     "interaction": {},
//     "userFlowImpact": "",
//     "<element_label_1>": {
//       "patternName": "",
//       "facetTags": [],
//       "states": [],
//       "interaction": {},
//       "userFlowImpact": ""
//     },
//     "<element_label_2>": {
//       "patternName": "",
//       "facetTags": [],
//       "states": [],
//       "interaction": {},
//       "userFlowImpact": ""
//     }
//     // repeat for each element
//   }
// }
// Output only one clean JSON block, no commentary or preambles.

// ⚡ Critical Reminders:
// Only annotate the component_name and its listed elements.

// Do not add new UI parts even if visible in the image.

// Think carefully and persistently validate that:

// All pattern names are correctly picked.

// All tags are precise, useful for filtering.

// Label and description are complete and consistent.

// States and interactions are appropriate and exhaustive.

// User flow impact is clearly action-driven.

// Reflect before you output: 
// ✅ Do facetTags include diverse terms across function, context, and role?
// ✅ Are all interaction events clear, user-centered, and labeled with cause-effect?
// ✅ Does the userFlowImpact tie into a journey or behavior outcome?
// ✅ Is the final output structured in VALID REQUIRED FORMAT, with no explanations?

// ---BEGIN NOW--- `

// export const METADATA_EXTRACTION_PROMPT_v1 = `
// You are a world-class UX system documenter tasked with annotating components for a highly structured, discoverable design-reference library.

// Your inputs will be:

// An image (only to help you better enrich the provided fields — not to add new components).

// A JSON object containing a component_name and a list of elements with basic label and description.

// 🧠 Your strict mission:
// ONLY enrich and annotate the component and elements listed in the JSON.
// ⚡ Ignore everything else visible in the image.
// ⚡ Do NOT invent or add any other UI elements not explicitly listed.

// You must fully and excellently annotate each component and its elements, strictly following the <good ux annotation guidelines>:

// 📋 Steps to Follow:

// 1. Component Role Recognition  
//    • Determine the component’s overall purpose and interaction model (e.g., “modal dialog for onboarding reminders,” “selection list for user choices”).  
//    • Use that to inform your patternName and description.

// 2. Component Enrichment (Top-Level)  
//    • patternName: Pick exactly one canonical type (e.g., Modal Dialog, Radio Card List).  
//    • facetTags (5–10):  
//      – Function: e.g., “onboarding”, “reminder”  
//      – Context: e.g., “mobile”, “permissions”  
//      – Role: e.g., “cta”, “informative”, “illustration”, “primary-action”  
//    • description: Clear, contextual description of component’s role and placement.  
//    • states: List valid UI states (default, hover, selected, disabled).  
//    • interaction: Document events (e.g., on_tap_ALLOW, on_swipe_LEFT).  
//    • userFlowImpact: One sentence on how this component nudges or guides the user (e.g., “Prompts users to enable notifications to support habit formation”).

// 3. Element Role Recognition  
//    • For each element, choose one best-fit patternName (Text Header, Illustration, Tooltip, etc.), matching form and function—do not invent new names.

// 4. Element Enrichment (Inside elements array)  
//    • Start from the provided label & description.  
//    • patternName: one canonical type.  
//    • facetTags (5–8):  
//      – Function tag(s)  
//      – Context tag(s)  
//      – Role tag(s)  
//    • states: valid states (default if static).  
//    • interaction: list supported events or "none": "Static element—no interaction".  
//    • userFlowImpact: one sentence on how this element influences the user journey (e.g., “Encourages permission grant by reinforcing emotional appeal”).

//    format the output as strict, ordered JSON. use component names and element labels DIRECTLY as keys

// <output>
// {
//   "<component_name>": {
//     "patternName": "",
//     "facetTags": [],
//     "states": [],
//     "interaction": {},
//     "userFlowImpact": "",
//     "<element_label_1>": {
//       "patternName": "",
//       "facetTags": [],
//       "states": [],
//       "interaction": {},
//       "userFlowImpact": ""
//     },
//     "<element_label_2>": {
//       "patternName": "",
//       "facetTags": [],
//       "states": [],
//       "interaction": {},
//       "userFlowImpact": ""
//     }
//     // repeat for each element
//   }
// }
// Output only one clean JSON block, no commentary or preambles.

// ⚡ Critical Reminders:
// Only annotate the component_name and its listed elements.

// Do not add new UI parts even if visible in the image.

// Think carefully and persistently validate that:

// All pattern names are correctly picked.

// All tags are precise, useful for filtering.

// Label and description are complete and consistent.

// States and interactions are appropriate and exhaustive.

// User flow impact is clearly action-driven.

// Reflect before you output: 
// ✅ Do facetTags include diverse terms across function, context, and role?
// ✅ Are all interaction events clear, user-centered, and labeled with cause-effect?
// ✅ Does the userFlowImpact tie into a journey or behavior outcome?
// ✅ Is the final output structured in VALID REQUIRED FORMAT, with no explanations?

// ---BEGIN NOW---`


// export const METADATA_EXTRACTION_PROMPT_v2 = `
// You are a world-class UX system documenter tasked with annotating components for a highly structured, discoverable design-reference library.

// Your inputs will be:

// An image (for context enrichment only — do not add new components).

// A JSON object containing component_name and a list of elements with basic label and description.

// 🧠 Mission:

// Annotate and enrich only the listed component_name and elements.

// Do not invent, add, or reference any UI parts not explicitly in the JSON.

// Follow *good ux annotation guidelines* precisely:

// 📋 Steps to Follow:
// 1. Component Role Recognition
// • Determine the overall purpose and interaction model (e.g., “modal dialog for onboarding reminders”).
// • Use this to complete patternName and description.

// 2. Component Enrichment (Top-Level)
// • patternName: Exactly one canonical type (e.g., Primary Button, Modal Dialog, Radio Card List,Form Input with Label	).
// • facetTags (5–10): Diverse terms across Function, Context, and Role (e.g., onboarding, mobile, CTA).
// • description: Clear and contextual.
// • states: All valid states (e.g., default, hover, selected, disabled, checked).
// • interaction: List of supported events as key-value pairs, using clear, user-centered action-effect language.). ie: {"interaction": {
//   "on_tap": "triggers primary action",
//   "on_swipe": "reveals dismiss option on swipe left"
// }}
// • userFlowImpact: How this component guides the user journey (one sentence).

// 3. Element Role Recognition
// • Assign exactly one patternName to each element (e.g., Text Header, Illustration).
// • Base enrichment on the provided label and description.

// 4. Element Enrichment (Inside elements array)
// • patternName: One canonical type.
// • facetTags (5–8): Diverse across Function, Context, Role.
// • states: Valid states (default if static).
// • interaction: list of Supported events ie,   "on_swipe": "reveal delete action when swiped left" , "on_drag": "reorder list item" "none": "Static element—no interaction".
// • userFlowImpact: How the element nudges user behavior (one sentence).

// format the output as strict, ordered JSON. use component names and element labels DIRECTLY as keys

// <output>
// {
//   "<component_name>": {
//     "patternName": "",
//     "facetTags": [],
//     "states": [],
//     "interaction": {},
//     "userFlowImpact": "",
//     "<element_label_1>": {
//       "patternName": "",
//       "facetTags": [],
//       "states": [],
//       "interaction": {},
//       "userFlowImpact": ""
//     },
//     "<element_label_2>": {
//       "patternName": "",
//       "facetTags": [],
//       "states": [],
//       "interaction": {},
//       "userFlowImpact": ""
//     }
//     // repeat for each element
//   }
// }
// Output only one clean JSON block, no commentary or preambles.

// ⚡ Critical Reminders:
// Only annotate the component_name and its listed elements.

// Do not add new UI parts even if visible in the image.

// Think carefully and persistently validate that:

// All pattern names are correctly picked.

// All tags are precise, useful for filtering.

// Label and description are complete and consistent.

// States and interactions are appropriate and exhaustive.

// User flow impact is clearly action-driven.

// Reflect before you output: 
// ✅ Do facetTags include diverse terms across function, context, and role?
// ✅ Are all interaction events clear, user-centered, and labeled with cause-effect?
// ✅ Does the userFlowImpact tie into a journey or behavior outcome?
// ✅ Is the final output structured in VALID REQUIRED FORMAT, with no explanations?

// ---BEGIN NOW---`


// export const METADATA_EXTRACTION_PROMPT_FINAL = `
// You are a world-class UX system documenter tasked with annotating components for a highly structured, discoverable design-reference library.

// Your inputs will be:

// An image (for context enrichment only — do not add new components).

// A JSON object containing component_name and a list of elements with basic label and description.

// 🧠 Mission:

// Annotate and enrich only the listed component_name and elements.

// Do not invent, add, or reference any UI parts not explicitly in the JSON.

// Follow *good ux annotation guidelines* precisely:

// 📋 Steps to Follow:
// 1. Component Role Recognition
// • Determine the overall purpose and interaction model (e.g., “modal dialog for onboarding reminders”).
// • Use this to complete patternName and componentDescription.

// 2. Component Enrichment (Top-Level)
// • patternName: Exactly one canonical type (e.g., Primary Button, Modal Dialog, Radio Card List,Form Input with Label	).
// • facetTags (5–10): Diverse terms across Function, Context, and Role (e.g., onboarding, mobile, CTA).
// • componentDescription: Clear and contextual.
// • states: All valid states (e.g., default, hover, selected, disabled, checked).
// • interaction: List of supported events as key-value pairs, using clear, user-centered action-effect language.). ie: {"interaction": {
//   "on_tap": "triggers primary action",
//   "on_swipe_left": "reveals delete buttons and archive chat option"
// }}
//      - ie: on_long_press, on_scroll, on_hover, on_swipe_left
// • userFlowImpact: How this component guides the user journey (one sentence).
// • flowPosition: Where this component sits in the typical user journey (e.g., "Checkout - Cart Review")


// 3. Element Role Recognition
// • Assign exactly one patternName to each element (e.g., Text Header, Illustration).
// • Base enrichment on the provided label and description.

// 4. Element Enrichment (Inside elements array)
// • patternName: One canonical type.
// • facetTags (5–8): Diverse across Function, Context, Role.
// • states: Valid states (default if static).
// • interaction: list of Supported events ie,   "on_swipe": "reveal delete action when swiped left" , "on_drag": "reorder list item" "none": "Static element—no interaction".
// • userFlowImpact: How the element nudges user behavior (one sentence).

// format the output as strict, ordered JSON. use component names and element labels DIRECTLY as keys

// <output>
// {
//   "<component_name>": {
//     "componentDescription": "",
//     "patternName": "",
//     "facetTags": [],
//     "states": [],
//     "interaction": {},
//     "userFlowImpact": "",
//     "flowPosition": "",
//     "<element_label_1>": {
//       "patternName": "",
//       "facetTags": [],
//       "states": [],
//       "interaction": {},
//       "userFlowImpact": ""
//     },
//     "<element_label_2>": {
//       "patternName": "",
//       "facetTags": [],
//       "states": [],
//       "interaction": {},
//       "userFlowImpact": ""
//     }
//     // repeat for each element
//   }
// }
// Output only one clean JSON block, no commentary or preambles.

// ⚡ Critical Reminders:
// Only annotate the component_name and its listed elements.

// Do not add new UI parts even if visible in the image.

// Think carefully and persistently validate that:

// All pattern names are correctly picked.

// All tags are precise, useful for filtering.

// Label and description are complete and consistent.

// States and interactions are appropriate and exhaustive.

// User flow impact is clearly action-driven.

// Reflect before you output: 
// ✅ Do facetTags include diverse terms across function, context, and role?
// ✅ Are all interaction events clear, user-centered, and labeled with cause-effect?
// ✅ Does the userFlowImpact tie into a journey or behavior outcome?
// ✅ Is the final output structured in VALID REQUIRED FORMAT, with no explanations?

// ---BEGIN NOW---
// `

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/ai/ClaudeAIService.ts
================
import Anthropic from '@anthropic-ai/sdk';
// import { PromptResult } from '../../../types/PromptRunner'; // adjust path as needed
import { EXTRACT_ELEMENTS_PROMPT_v2, ANCHOR_ELEMENTS_PROMPT_v0, ANCHOR_ELEMENTS_PROMPT_v1, ANCHOR_ELEMENTS_PROMPT_v2, EXTRACT_ELEMENTS_PROMPT_v3, ANCHOR_ELEMENTS_PROMPT_v3 } from '@/lib/prompt/prompts';
import { PromptTrackingContext } from '@/lib/logger';
import { PromptLogType } from '@/lib/constants';
import { cleanText } from '@/lib/file-utils'; 
// Ensure your Claude API key is set in ENV
const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

// Specify the Claude vision model version
const VISION_MODEL_CLAUDE = 'claude-3-7-sonnet-20250219';
const VISION_MODEL_HAIKU = 'claude-3-5-haiku-20241022';
const VISION_MODEL_HAIKU_CHEAP = 'claude-3-haiku-20240307';
const DEV_MODE = false;

const FINAL_MODEL = DEV_MODE ? VISION_MODEL_HAIKU_CHEAP : VISION_MODEL_CLAUDE;

const MAX_TOKENS = 8192;
// const MAX_TOKENS = 4096;

// Constants for token cost calculation (update with actual costs)
const CLAUDE_INPUT_TOKEN_COST = 0.000015; // example cost per input token
const CLAUDE_OUTPUT_TOKEN_COST = 0.000060; // example cost per output token

/**
 * Calls the Claude vision-capable model with a text prompt and optional image URL.
 *
 * @param prompt   - The text prompt to send.
 * @param imageUrl - Optional URL of an image for the model to analyze.
 * @param context  - The tracking context containing batch, screenshot, and other IDs
 * @param promptType - The type of prompt being processed.
 * @returns        - A structured PromptResult containing the response, timing, and token usage.
 */
export async function callClaudeVisionModel(
  prompt: string,
  imageUrl: string | null,
  context: PromptTrackingContext,
  promptType: PromptLogType.ELEMENT_EXTRACTION | PromptLogType.ANCHORING
): Promise<any> {
  // Build the Anthropic messages payload
  const messages = [
    {
      "role": 'user',
      "content": [
        // include image if provided
        imageUrl && {
          "type": "image",
          "source": { "type": "url", "url": imageUrl }
        },
        { "type": "text", "text": prompt }
      ].filter(Boolean),
    },
  ];

  try {
    // Start timing right before the API call
    const startTime = Date.now();
    
    const response = await anthropic.messages.create({
      // model: VISION_MODEL_HAIKU,
      model: FINAL_MODEL,
      max_tokens: MAX_TOKENS, // tweak as needed
      messages: messages as Anthropic.MessageParam[],
    });
    
    // End timing right after the API call
    const endTime = Date.now();
    const durationMs = endTime - startTime;

    // Extract usage data
    const inputTokens = response?.usage?.input_tokens || 0;
    const outputTokens = response?.usage?.output_tokens || 0;
    
    // Log the interaction using the context with the measured duration
    await context.logPromptInteraction(
      `Claude-${FINAL_MODEL}`,
      promptType,
      prompt,
      JSON.stringify(response),
      durationMs,
      {
        input: inputTokens,
        output: outputTokens,
        total: inputTokens + outputTokens
      },
      CLAUDE_INPUT_TOKEN_COST,
      CLAUDE_OUTPUT_TOKEN_COST
    );

    return response;
  } catch (err) {
    console.error('Error calling Claude Vision Model:', err);
    throw new Error(
      `Failed to get response from Claude: ${
        err instanceof Error ? err.message : String(err)
      }`
    );
  }
}

/**
 * Extracts components from an image using the OpenAI vision model.
 *
 * @param imageUrl The URL of the image to analyze.
 * @param component_list The list of components to guide the extraction.
 * @param context The tracking context containing batch, screenshot, and other IDs
 * @returns A promise resolving to the structured PromptResult.
 * @throws Throws an error if the API call fails.
 */
export async function extract_element_from_image(
  imageUrl: string, 
  component_list: string,
  context: PromptTrackingContext
) {
  // Define the prompt for extraction
  const prompt = EXTRACT_ELEMENTS_PROMPT_v2 + `\n\n<component_list>${component_list}</component_list>`;
  const response = await callClaudeVisionModel(
    prompt, 
    imageUrl, 
    context,
    PromptLogType.ELEMENT_EXTRACTION
  );

  const { parsedContent, rawText, usage } = extractClaudeResponseData(response);

  // Call the OpenAI vision model with the prompt and image URL
  return { parsedContent, rawText, usage };
}

/**
 * Extracts anchor elements from an image using the Claude vision model.
 *
 * @param imageUrl The URL of the image to analyze.
 * @param element_list The list of elements to guide the extraction.
 * @param context The tracking context containing batch, screenshot, and other IDs
 * @returns A promise resolving to the structured PromptResult.
 * @throws Throws an error if the API call fails.
 */
export async function anchor_elements_from_image(
  imageUrl: string, 
  element_list: string,
  context: PromptTrackingContext
) {
  // Define the prompt for anchor extraction
  const prompt = ANCHOR_ELEMENTS_PROMPT_v3 + `\n\n<element_list>${element_list}</element_list>`;
  const response = await callClaudeVisionModel(
    prompt, 
    imageUrl,
    context,
    PromptLogType.ANCHORING
  );

  const { parsedContent, rawText, usage } = extractClaudeResponseData(response);

  // Call the Claude vision model with the prompt and image URL
  return { parsedContent, rawText,usage };
}

/**
 * Cleans the raw text and returns a list of components.
 *
 * @param components - The array of components to filter and clean.
 * @returns Array of cleaned strings in the format "component_name: description".
 */
function cleanTextToList(components: any[]): string[] {
  return components
    .filter(component => typeof component?.component_name === 'string' && typeof component?.description === 'string')
    .map(component => `${component.component_name}: ${component.description}`);
}

/**
 * Helper: Parses Claude's text content into a JSON object safely.
 * Handles common formatting quirks like trailing commas or line breaks.
 * Searches for JSON content within the response text.
 * It attempts to clean the extracted text before parsing.
 *
 * @param rawText - Raw text string returned from Claude.
 * @returns Parsed JSON object, or an empty object if parsing fails.
 */
function parseClaudeTextToJson(rawText: string): Record<string, any> {
  // console.log('Attempting to parse Claude response as JSON');
  let jsonContent = '';
  
  try {
    // Look for JSON pattern in the text - either within code blocks or standalone
    const jsonRegex = /```(?:json)?\s*({[\s\S]*?})\s*```|({[\s\S]*})/;
    const match = rawText.match(jsonRegex);

    if (match) {
      // Use the first matched group that contains content
      jsonContent = match[1] || match[2];
      // console.log('Extracted potential JSON content:', jsonContent.slice(0, 100));
    } else {
      console.log('No JSON match found within ``` markers or as standalone object, trying entire text.');
      // Fall back to using the entire text if no specific JSON block is found
      jsonContent = rawText;
    }

    // --- Enhanced Cleaning ---
    // 1. Basic cleaning (from file-utils, potentially redundant but safe)
    let cleanedText = cleanText(jsonContent); 
    
    // 2. Remove leading/trailing whitespace
    cleanedText = cleanedText.trim();

    // 3. Attempt to fix common JSON issues (e.g., unescaped newlines within strings)
    // Note: This is a heuristic and might not cover all cases.
    // It replaces literal newlines only if they seem to be inside string values
    // (i.e., preceded by a non-backslash character and followed by a quote).
    // This is complex to get perfect with regex, a more robust solution might involve
    // a more sophisticated parser or sequential processing.
    // cleanedText = cleanedText.replace(/([^\\])\\n"/g, '$1\\\\n"'); // Example: try to fix unescaped newlines

    // More aggressive cleaning: remove control characters except for \t, \n, \r, \f within strings
    cleanedText = cleanedText.replace(/[\x00-\x08\x0B\x0C\x0E-\x1F]/g, '');


    // 4. Final attempt to remove trailing commas before closing brace/bracket
    cleanedText = cleanedText.replace(/,\s*([}\]])/g, '$1');

    // console.log('Cleaned JSON content for parsing:', cleanedText.slice(0, 100));
    
    // --- Parsing ---
    return JSON.parse(cleanedText);

  } catch (error) {
    // console.error('Failed to parse Claude response as JSON even after cleaning.', error);
    // console.error('Original rawText:', rawText);
    // console.error('Content attempted for parsing:', jsonContent); // Log the extracted part
    
    // More aggressive repair attempt for malformed JSON
    try {
      // Try to repair truncated JSON by closing unclosed structures
      if (jsonContent.includes('{') && !jsonContent.endsWith('}')) {
        // Attempt to fix truncated JSON by adding closing braces
        const openBraces = (jsonContent.match(/{/g) || []).length;
        const closeBraces = (jsonContent.match(/}/g) || []).length;
        if (openBraces > closeBraces) {
          const fixedJson = jsonContent + '}'.repeat(openBraces - closeBraces);
          return JSON.parse(fixedJson);
        }
      }
      
      // If we can't parse the whole thing, try to extract key-value pairs manually
      // This is a fallback approach for severely malformed JSON
      const keyValueRegex = /"([^"]+)":\s*"([^"]+)"/g;
      const extractedPairs: Record<string, any> = {};
      let match;
      while ((match = keyValueRegex.exec(jsonContent)) !== null) {
        extractedPairs[match[1]] = match[2];
      }
      
      if (Object.keys(extractedPairs).length > 0) {
        console.log('Recovered partial JSON data through regex extraction');
        return extractedPairs;
      }
    } catch (repairError) {
      // console.error('JSON repair attempt also failed:', repairError);
    }
    
    // Return empty object on failure to prevent downstream errors
    return {} as Record<string, any>;
  }
}

/**
 * Extracts structured content text and usage metadata from Claude's response.
 *
 * @param response - Full Claude response object.
 * @returns Object containing parsed text content and usage details.
 */
export function extractClaudeResponseData(response: any): {
  parsedContent: Record<string, any>,
  rawText: string,
  usage: {
    input_tokens?: number,
    output_tokens?: number
  }
} {
  const rawText = response?.content?.find((item: any) => item.type === 'text')?.text ?? '';
  console.log('returned from claude VL: ', rawText);

  const parsedContent = parseClaudeTextToJson(rawText);

  const usage = {
    input_tokens: response?.usage?.input_tokens,
    output_tokens: response?.usage?.output_tokens,
  };

  return {
    parsedContent,
    rawText,
    usage,
  };
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/ai/MoondreamDetectionService.js
================
import { detectObjectsFromBuffer, normalizedToPixelCoordinates } from '@/lib/services/ai/MoondreamVLService';
import fs from 'fs';
import path from 'path';
import Jimp from 'jimp';
import { PromptTrackingContext, createScreenshotTrackingContext } from '@/lib/logger';
import pLimit from 'p-limit';
import {
  BOX_COLOR,
  generateAnnotatedImageBuffer,
  saveAnnotatedImageDebug,
  normalizeLabel
} from '@/lib/services/imageServices/BoundingBoxService';

import {MOONDREAM_CONCURRENCY} from '@/lib/constants';

const VLM_MODEL_NAME = 'moondream'; // Define the model name being used

// --- Type Imports (using JSDoc for type hinting in JS) ---
/**
 * @typedef {import('../../types/DetectionResult').ElementDetectionItem} ElementDetectionItem
 * @typedef {import('../../types/DetectionResult').ComponentDetectionResult} ComponentDetectionResult
 */

/**
 * Extracts the first level of the label hierarchy
 * @param {string} label - The full hierarchical label
 * @returns {string} First level category
 */
function getFirstLevelCategory(label) {
  return label.split(' > ')[0];
}

/**
 * Determines the appropriate grouping category for hierarchical labels
 * Elements are grouped by mid-level categories if they have >2 children
 * 
 * This algorithm implements a dynamic grouping strategy for hierarchical labels:
 * 1. Creates a tree structure representing the label hierarchy
 * 2. Identifies nodes with >2 children or direct elements
 * 3. Promotes these nodes as standalone categories
 * 4. Assigns each element to its deepest eligible category
 * 
 * Example:
 * For "Operational Risk Overview > ICU Department > Occupancy", 
 * if "ICU Department" has >2 child elements, it becomes the category
 * instead of just "Operational Risk Overview".
 * 
 * @param {string[]} allLabels - Array of all hierarchical labels (e.g. "Parent > Child > Grandchild")
 * @returns {Object} Map of labels to their assigned category
 */
function determineHierarchicalGroups(allLabels) {
  // Step 1: Build a tree structure from all labels
  const hierarchy = {};
  
  // Count elements under each prefix
  allLabels.forEach(label => {
    const parts = label.split(' > ');
    
    // Initialize all paths in the hierarchy for this label
    for (let i = 0; i < parts.length; i++) {
      const currentPath = parts.slice(0, i + 1).join(' > ');
      
      if (!hierarchy[currentPath]) {
        hierarchy[currentPath] = {
          count: 0,          // Number of leaf nodes directly assigned to this path
          level: i + 1,      // Depth in the hierarchy (1 = top level)
          parent: i > 0 ? parts.slice(0, i).join(' > ') : null,
          children: new Set() // Set of immediate child paths
        };
      }
      
      // If this is a leaf node (full path), increment leaf count
      if (i === parts.length - 1) {
        hierarchy[currentPath].count++;
      }
      
      // Add as child to parent node
      if (i > 0) {
        const parentPath = parts.slice(0, i).join(' > ');
        if (hierarchy[parentPath]) {
          hierarchy[parentPath].children.add(currentPath);
        }
      }
    }
  });
  
  // Log the hierarchy structure (debug only)
  if (process.env.DEBUG_HIERARCHY === 'true') {
    console.log('=== Label Hierarchy ===');
    Object.entries(hierarchy).forEach(([path, node]) => {
      console.log(`${path} (Level ${node.level}): ${node.count} direct elements, ${node.children.size} children`);
    });
    console.log('======================');
  }
  
  // Step 2: Find the appropriate category for each label
  const labelToCategory = {};
  
  allLabels.forEach(label => {
    const parts = label.split(' > ');
    let bestCategory = parts[0]; // Default to top level
    let bestLevel = 1;
    
    // Find the deepest qualifying category
    for (let i = 0; i < parts.length; i++) { 
      const currentPath = parts.slice(0, i + 1).join(' > ');
      const node = hierarchy[currentPath];
      
      if (!node) continue;
      
      // Determine if this node qualifies as a category:
      // 1. Always include top level
      // 2. If node has >2 children or contains >2 elements directly
      const childCount = node.children.size;
      const hasEnoughChildren = childCount > 2 || node.count > 2;
      
      if (i === 0 || hasEnoughChildren) {
        // This is a better category than what we have
        if (node.level > bestLevel) {
          bestCategory = currentPath;
          bestLevel = node.level;
        }
      }
    }
    
    labelToCategory[label] = bestCategory;
  });
  
  // Log some stats about the grouping results
  const categoryCounts = {};
  Object.values(labelToCategory).forEach(category => {
    categoryCounts[category] = (categoryCounts[category] || 0) + 1;
  });
  
  const uniqueCategories = Object.keys(categoryCounts);
  console.log(`Created ${uniqueCategories.length} groupings from ${allLabels.length} elements:`);
  uniqueCategories.forEach(category => {
    console.log(`- ${category}: ${categoryCounts[category]} elements`);
  });
  
  return labelToCategory;
}

/**
 * Creates an output directory with timestamp
 * @returns {Promise<string>} Path to the created directory
 */
async function createOutputDirectory() {
  const timestamp = new Date().toISOString().replace(/[:.-]/g, '').replace('T', '_').slice(0, 15);
  const outputDir = `mobbin_attempt_folder/detection_output_${timestamp}`;
  
  try {
    await fs.promises.mkdir(outputDir, { recursive: true });
    // console.log(`Output directory created: ${outputDir}`);
    return outputDir;
  } catch (err) {
    console.error(`Failed to create output directory: ${err}`);
    throw err;
  }
}

/**
 * Saves JSON data to a file
 * @param {Object} data - The data to save
 * @param {string} filePath - Path to save the JSON file
 * @returns {Promise<void>}
 */
async function saveJson(data, filePath) {
  try {
    // Only save if needed (e.g., for debugging)
    if (process.env.SAVE_DEBUG_FILES === 'true') {
        await fs.promises.writeFile(filePath, JSON.stringify(data, null, 4));
        // console.log(`Debug JSON data saved successfully to: ${filePath}`);
    }
  } catch (err) {
    console.error(`Error saving debug JSON file to ${filePath}: ${err}`);
  }
}

/**
 * Processes a single description to detect objects
 * @param {Buffer} imageBuffer - The image buffer
 * @param {string} description - The object description to detect
 * @param {PromptTrackingContext} context - The tracking context containing batch, screenshot, and component IDs
 * @returns {Promise<{objects: Array, duration: number}>} Detected objects and duration
 */
async function detectSingleObject(imageBuffer, description, context) {
  try {
    // Create a component-specific context if this detection is for a specific component
    const componentContext = context.componentId 
      ? context 
      : context; // Use as-is if no component ID present yet

    // We measure only the API call duration, which is handled inside detectObjectsFromBuffer
    const result = await detectObjectsFromBuffer(imageBuffer, description, componentContext);
    
    // The API call duration is already tracked in detectObjectsFromBuffer
    if (result && result.objects && result.objects.length > 0) {
      return { objects: result.objects, duration: result.durationMs }; 
    } else {
      return { objects: [], duration: 0 }; 
    }
  } catch (err) {
    console.error(`Error during detection for '${description}': ${err}`);
    throw err; // Let the caller handle the error state
  }
}

/**
 * Process detected objects and scale coordinates to absolute pixel values
 * @param {Array} detectedObjectsList - List of raw detection objects
 * @param {string} label - The label being processed
 * @param {number} imgWidth - Image width in pixels
 * @param {number} imgHeight - Image height in pixels
 * @returns {Array<{x_min: number, y_min: number, x_max: number, y_max: number}>} List of processed bounding boxes
 */
function processBoundingBoxes(detectedObjectsList, label, imgWidth, imgHeight) {
  const boundingBoxes = [];

  for (const rawDetection of detectedObjectsList) {
    try {
      if ('x_min' in rawDetection && 'y_min' in rawDetection && 'x_max' in rawDetection && 'y_max' in rawDetection) {
        const scaledCoords = normalizedToPixelCoordinates(rawDetection, imgWidth, imgHeight);
        boundingBoxes.push(scaledCoords);
      } else {
        console.warn(`Skipping a detection for label '${label}' due to missing coordinate keys in:`, rawDetection);
      }
    } catch (err) {
      console.error(`Error scaling coordinates for one detection of label '${label}': ${err}`);
      // Decide how to handle scaling errors - skip this box or mark as error?
    }
  }

  return boundingBoxes;
}

/**
 * Main processing function: detects objects, groups by category, generates annotated images/data.
 * Returns structured results per component/category.
 * @param {number} screenshotId - ID of the screenshot being processed
 * @param {Buffer} imageBuffer - Buffer containing the image data
 * @param {Object.<string, string>} labelsDict - Dictionary of {label: description}
 * @param {number} batchId - The ID of the batch this operation is part of
 * @param {string} screenshotUrl - The signed URL of the screenshot for debugging/audit
 * @returns {Promise<ComponentDetectionResult[]>} Array of detection results for each component/category.
 */
export async function processAndSaveByCategory(screenshotId, imageBuffer, labelsDict, batchId, screenshotUrl) {
  const overallStartTime = performance.now();
  let outputDir = null; // Only needed if saving debug files
  if (process.env.SAVE_DEBUG_FILES === 'true') {
      outputDir = await createOutputDirectory();
  }
  const limit = pLimit(MOONDREAM_CONCURRENCY);
  const componentResults = [];

  // Create a tracking context for this screenshot
  const context = createScreenshotTrackingContext(batchId, screenshotId);

  try {
    // --- Image Validation ---
    let validatedImageBuffer = imageBuffer;
    let jimpImage;
    try {
      jimpImage = await Jimp.read(imageBuffer);
    } catch (err) {
      console.error(`Invalid initial image format for screenshot ${screenshotId}: ${err.message}. Attempting conversion...`);
      try {
        // Attempt conversion (e.g., from JPEG or WEBP to PNG buffer)
        const tempImage = await Jimp.read(imageBuffer);
        validatedImageBuffer = await tempImage.getBufferAsync(Jimp.MIME_PNG);
        jimpImage = await Jimp.read(validatedImageBuffer); // Read the converted buffer
        console.log(`Image conversion successful for screenshot ${screenshotId}.`);
      } catch (convErr) {
        // If conversion fails, we cannot proceed with this screenshot
        console.error(`FATAL: Image conversion failed for screenshot ${screenshotId}: ${convErr.message}. Skipping detection for this image.`);
        // Return an empty array or a specific error result? Empty array for now.
        return [];
        // Or: throw new Error(`Could not process image for screenshot ${screenshotId}: ${convErr.message}`);
      }
    }
    const imgWidth = jimpImage.getWidth();
    const imgHeight = jimpImage.getHeight();

    // --- Parallel Detection ---
    const labelEntries = Object.entries(labelsDict);
    console.log(`[Screenshot ${screenshotId}] Starting detection for ${labelEntries.length} labels with concurrency ${MOONDREAM_CONCURRENCY}`);

    // Intermediate structure to hold results per label
    const detectionResultsByLabel = {};

    const detectionTasks = labelEntries.map(([label, description], idx) =>
      limit(async () => {
        // Add a staggered delay: 1s per index to ensure 1 second between each concurrency start
        await new Promise(res => setTimeout(res, idx * 1000));
        // console.log(`[Screenshot ${screenshotId}] Starting detection for: '${label}'`);
        let detectionData = { objects: [], duration: 0, error: null };
        let status = 'Not Detected';
        try {
            detectionData = await detectSingleObject(validatedImageBuffer, description, context);
            status = detectionData.objects.length > 0 ? 'Detected' : 'Not Detected';
            // console.log(`[Screenshot ${screenshotId}] Finished detection for: '${label}' (Found: ${detectionData.objects.length})`);
        } catch (error) {
            console.error(`[Screenshot ${screenshotId}] Error in detectSingleObject for '${label}':`, error);
            detectionData.error = error;
            status = 'Error'; // Mark detection as errored
        }
        return { label, description, rawDetections: detectionData.objects, duration: detectionData.duration, status, error: detectionData.error };
      })
    );

    const settledDetectionTasks = await Promise.allSettled(detectionTasks);
    console.log(`[Screenshot ${screenshotId}] All detection tasks settled.`);

    // --- Process and Group Results ---
    // First get all labels for dynamic grouping determination
    const allLabels = labelEntries.map(([label]) => label);
    const labelToGroupMap = determineHierarchicalGroups(allLabels);
    
    const elementsByCategory = {};

    settledDetectionTasks.forEach((result, index) => {
      const [originalLabel, originalDescription] = labelEntries[index]; // Get label/desc based on original index

      if (result.status === 'fulfilled') {
        const { label, description, rawDetections, duration, status: detectionStatus, error } = result.value;

        const elementItem = {
          label: label,
          description: description,
          bounding_box: null, // Will be populated if coordinates are valid
          status: detectionStatus,
          vlm_model: VLM_MODEL_NAME,
          element_inference_time: duration, // Time for this specific label's detection
          // accuracy_score: undefined, // To be added later
          // suggested_coordinates: undefined, // To be added later
          error: error ? (error.message || 'Detection Error') : null
        };

        if (detectionStatus === 'Detected' && rawDetections.length > 0) {
          // Currently takes the first box if multiple are returned for one description.
          // Consider how to handle multiple boxes for a single label if needed.
          const boundingBoxes = processBoundingBoxes(rawDetections.slice(0, 1), label, imgWidth, imgHeight);
          if (boundingBoxes.length > 0) {
            elementItem.bounding_box = boundingBoxes[0]; // Assign the first valid box
          } else {
            // Detected but failed coordinate scaling
            elementItem.status = 'Error';
            elementItem.error = elementItem.error || 'Coordinate scaling failed';
            console.warn(`[Screenshot ${screenshotId}] Processed '${label}': Detected but failed to scale coordinates.`);
          }
        } else if (detectionStatus === 'Error') {
            console.warn(`[Screenshot ${screenshotId}] Processed '${label}': Detection failed.`);
        } else {
           // console.log(`[Screenshot ${screenshotId}] Processed '${label}': Not detected.`);
        }

        // Use the dynamically determined category instead of just first level
        const categoryName = labelToGroupMap[label] || getFirstLevelCategory(label);
        
        if (!elementsByCategory[categoryName]) {
          elementsByCategory[categoryName] = [];
        }
        elementsByCategory[categoryName].push(elementItem);

      } else {
        // Task itself failed (rejected promise from p-limit queue, shouldn't happen often with try/catch inside)
        console.error(`[Screenshot ${screenshotId}] Detection task failed unexpectedly for label '${originalLabel}':`, result.reason);
        
        // Use the dynamically determined category instead of just first level
        const categoryName = labelToGroupMap[originalLabel] || getFirstLevelCategory(originalLabel);
        
        if (!elementsByCategory[categoryName]) {
          elementsByCategory[categoryName] = [];
        }
        elementsByCategory[categoryName].push({
          label: originalLabel,
          description: originalDescription,
          bounding_box: null,
          status: 'Error',
          vlm_model: VLM_MODEL_NAME,
          element_inference_time: 0, // Unknown duration
          error: result.reason?.message || 'Unknown task error'
        });
      }
    });

    // --- Generate Component Results (Image Buffer + Data) ---
    const componentProcessingPromises = Object.entries(elementsByCategory).map(async ([categoryName, elements]) => {
        const categoryStartTime = performance.now();

        // Filter items relevant for drawing (successfully detected with boxes)
        const detectedElements = elements.filter(el => el.status === 'Detected' && el.bounding_box);

        // Generate annotated image buffer for this category
        /** @type {Buffer | null} */
        const annotatedImageBuffer = await generateAnnotatedImageBuffer(
            validatedImageBuffer,
            detectedElements,
            BOX_COLOR, // Use a consistent color or cycle colors per category if needed
            categoryName
        );

        // Save debug image if enabled and buffer exists
        if (outputDir && annotatedImageBuffer) {
           await saveAnnotatedImageDebug(annotatedImageBuffer, categoryName, outputDir);
        }
        // Save debug JSON if enabled
        if (outputDir) {
            const normalizedKey = normalizeLabel(categoryName);
            const jsonPath = path.join(outputDir, `${normalizedKey}.json`);
            await saveJson({ screenshotId, categoryName, elements }, jsonPath); // Save all elements for the category
        }


        // Determine overall status for the component
        let componentStatus = 'failed';
        const hasSuccess = elements.some(el => el.status === 'Detected');
        const hasError = elements.some(el => el.status === 'Error');
        if (hasSuccess && !hasError) {
            componentStatus = 'success';
        } else if (hasSuccess && hasError) {
            componentStatus = 'partial';
        } else if (!hasSuccess && hasError) {
            componentStatus = 'failed'; // All elements failed or errored
        } else {
            componentStatus = 'failed'; // No elements detected or processed successfully
        }

        // Aggregate inference time (sum of individual element times)
        const totalInferenceTime = elements.reduce((sum, el) => sum + el.element_inference_time, 0);

        // Construct the ComponentDetectionResult
        /** @type {ComponentDetectionResult} */
        const componentResult = {
            screenshot_id: screenshotId,
            component_name: categoryName,
            // Store the original image buffer
            original_image_object: imageBuffer,
            annotated_image_object: annotatedImageBuffer,
            annotated_image_url: undefined, // To be filled after upload
            screenshot_url: screenshotUrl, // Store the screenshot signed URL for debugging/audit
            // TODO: Define how to get a meaningful component_description. Using category name for now.
            component_description: `Detection results for ${categoryName}`,
            detection_status: componentStatus,
            inference_time: totalInferenceTime, // Or use category wall time: performance.now() - categoryStartTime;
            elements: elements, // Include all elements (detected, not detected, error)
        };

        componentResults.push(componentResult);
        // console.log(`[Screenshot ${screenshotId}] Finished processing component: '${categoryName}'`);
    });

    await Promise.all(componentProcessingPromises);

    const overallDuration = performance.now() - overallStartTime;
    console.log(`[Screenshot ${screenshotId}] Annotation Complete. Total time: ${overallDuration.toFixed(2)/1000}s`);
    return componentResults;

  } catch (err) {
    // Catch errors during initial setup (e.g., image reading/conversion)
    console.error(`[Screenshot ${screenshotId}] FATAL error during Moondream processing setup:`, err);
    // Return empty array to indicate failure for this screenshot
    return [];
  }
}

/**
 * Processes an image from a file path using labels dictionary
 * @param {string} imagePath - Path to the image file
 * @param {Object} labelsDict - Dictionary of labels and their descriptions
 * @returns {Promise<Object|null>} Categories with their detected items
 */
export async function processImageFile(imagePath, labelsDict) {
  try {
    let imageBuffer = await fs.promises.readFile(imagePath);
    console.log(`Image loaded successfully from: ${imagePath}`);
    
    return processAndSaveByCategory(imageBuffer, labelsDict);
  } catch (err) {
    console.error(`Error processing image ${imagePath}: ${err}`);
    return null;
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/ai/MoondreamVLService.js
================
import { vl } from 'moondream';
import { PromptTrackingContext } from '@/lib/logger';
import { MOON_DREAM_API_KEY } from '@/config';
import fs from 'fs';
import { PromptLogType } from '@/lib/constants';

/**
 * Type definition for detection results
 */
export const DetectionResultType = {
  request_id: String,
  objects: Array
};

/**
 * Initialize the vl model with API key
 */
const model = new vl({ apiKey: `${MOON_DREAM_API_KEY}` });

/**
 * Detect objects in an image using Moondream vl client
 * 
 * @param {Buffer} imageBuffer - Buffer containing image data
 * @param {string} objectType - The object type to detect (e.g., "person", "car", "face")
 * @param {PromptTrackingContext} context - The tracking context containing batch, screenshot, and component IDs
 * @returns {Promise<Object>} A promise resolving to the detection response
 */
export async function detectObjectsFromBuffer(
  imageBuffer, 
  objectType, 
  context
) {
  try {
    // Timing specifically the API call, not surrounding logic
    const startTime = Date.now();
    
    const result = await model.detect({
      image: imageBuffer,
      object: objectType
    });
    
    const endTime = Date.now();
    const durationMs = endTime - startTime;
    
    // Log the interaction using the context with the measured duration
    await context.logPromptInteraction(
      'Moondream-vl-Detect',
      PromptLogType.VLM_LABELING,
      `Detect ${objectType} in image`,
      JSON.stringify(result),
      durationMs,
      {
        // Moondream doesn't provide token usage, so we leave these undefined
        input: undefined,
        output: undefined,
        total: undefined
      }
    );

    console.log(`-- [Detected] "${objectType.slice(0,50)}..." in ${durationMs/1000}s`);
    return {
      request_id: result.request_id,
      objects: result.objects,
      durationMs: durationMs/1000 // Return the duration of the operation in milliseconds
    };
  } catch (err) {
    console.error('Error using Moondream vl:', err);
    throw new Error(
      `Failed to get response from Moondream vl: ${
        err instanceof Error ? err.message : String(err)
      }`
    );
  }
}

/**
 * Converts the normalized coordinates to pixel coordinates
 * 
 * @param {Object} coordinates - Object with normalized coordinates (0-1)
 * @param {number} coordinates.x_min - Minimum x coordinate (0-1)
 * @param {number} coordinates.y_min - Minimum y coordinate (0-1)
 * @param {number} coordinates.x_max - Maximum x coordinate (0-1)
 * @param {number} coordinates.y_max - Maximum y coordinate (0-1)
 * @param {number} imageWidth - Width of the image in pixels
 * @param {number} imageHeight - Height of the image in pixels
 * @returns {Object} Object with pixel coordinates
 */
export function normalizedToPixelCoordinates(coordinates, imageWidth, imageHeight) {
  return {
    x_min: Math.round(coordinates.x_min * imageWidth),
    y_min: Math.round(coordinates.y_min * imageHeight),
    x_max: Math.round(coordinates.x_max * imageWidth),
    y_max: Math.round(coordinates.y_max * imageHeight)
  };
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/ai/OpenAIService.js
================
import OpenAI from 'openai';
// import { PromptResult } from '../../../types/PromptRunner'; // Adjust the path as needed
import { ACCURACY_VALIDATION_PROMPT_v0, EXTRACTION_PROMPT_v6, METADATA_EXTRACTION_PROMPT_FINAL } from '@/lib/prompt/prompts';
import { PromptTrackingContext } from '@/lib/logger';
import { PromptLogType } from '@/lib/constants';
// Ensure OPENAI_API_KEY is set in your environment variables
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Constants
const OPENAI_CONFIG = {
  VISION_MODEL: 'gpt-4o-2024-11-20',
  VISION_MODEL_GPT4: 'gpt-4.1-2025-04-14',
  // VISION_MODEL_GPT4: 'gpt-4.1-mini-2025-04-14',
  // VISION_MODEL_GPT4: 'gpt-4o-mini',
  INPUT_TOKEN_COST: 0,
  OUTPUT_TOKEN_COST: 0
};

/**
 * Handles the common response processing logic for OpenAI API calls
 */
async function handleOpenAIResponse(response, context, promptType, prompt, startTime) {
  const endTime = Date.now();
  const durationMs = endTime - startTime;

  const inputTokens = response?.usage?.input_tokens || 0;
  const outputTokens = response?.usage?.output_tokens || 0;
  
  await context.logPromptInteraction(
    `OpenAI-${OPENAI_CONFIG.VISION_MODEL_GPT4}`,
    promptType,
    prompt,
    JSON.stringify(response),
    durationMs,
    {
      input: inputTokens,
      output: outputTokens,
      total: response?.usage?.total_tokens
    },
    OPENAI_CONFIG.INPUT_TOKEN_COST,
    OPENAI_CONFIG.OUTPUT_TOKEN_COST
  );

  return response;
}


/**
 * Calls the OpenAI vision model with a prompt and optional image URL.
 *
 * @param prompt The text prompt to send to the model.
 * @param imageUrl Optional URL of an image for the vision model.
 * @param context The tracking context containing batch, screenshot, and other IDs
 * @param promptType The type of prompt being processed.
 * @returns A promise resolving to the structured PromptResult.
 * @throws Throws an error if the API call fails.
 */
export async function callOpenAIVisionModelURL(
  prompt,
  imageUrl,
  context,
  promptType = PromptLogType.COMPONENT_EXTRACTION
){
  
  // const messages = createMessagesPayload(prompt, imageUrl);

  
  // return makeOpenAICall(messages, context, promptType, prompt);

  // console.log('calling openai', prompt, imageUrl);
  try {
    const startTime = Date.now();

    // const str_url = imageUrl;
    const response = await openai.responses.create({
      model: OPENAI_CONFIG.VISION_MODEL_GPT4,
      input: [
        {
            "role": "user",
            "content": [
                { "type": "input_text", "text": `"${prompt}"` },
                {
                    "type": "input_image",
                    "image_url": imageUrl,
                },
            ],
        },
    ],
  });
  return await handleOpenAIResponse(response, context, promptType, prompt, startTime);
  } catch (error) {
    console.error(`Error calling OpenAI Vision Model: ${error}`);
    throw new Error(`Failed to get response from OpenAI: ${error instanceof Error ? error.message : String(error)}`);
  }
} 

export async function callOpenAIVisionModelBase64(
  prompt,
  imageBase64,
  context,
  promptType = PromptLogType.ACCURACY_VALIDATION
) {
  try {
    // Ensure the base64 string has the proper data URL format using our utility
    const imageUrl = safelyEncodeImageForOpenAI(imageBase64);
    
    const startTime = Date.now();

    const response = await openai.responses.create({
      model: OPENAI_CONFIG.VISION_MODEL_GPT4,
      input: [
        {
            "role": "user",
            "content": [
                { "type": "input_text", "text": `"${prompt}"` },
                {
                    "type": "input_image",
                    "image_url": imageUrl,
                },
            ],
        },
      ],
    });
    return await handleOpenAIResponse(response, context, promptType, prompt, startTime);
  } catch (error) {
    console.error(`Error calling OpenAI Vision Model: ${error}`);
    throw new Error(`Failed to get response from OpenAI: ${error instanceof Error ? error.message : String(error)}`);
  }
}

/**
 * Extracts components from an image using the OpenAI vision model.
 *
 * @param imageUrl The URL of the image to analyze.
 * @param context The tracking context containing batch, screenshot, and other IDs
 * @returns A promise resolving to the structured PromptResult.
 * @throws Throws an error if the API call fails.
 */
export async function extract_component_from_image(imageUrl, context) {
  const result = await callOpenAIVisionModelURL(
    EXTRACTION_PROMPT_v6, 
    imageUrl, 
    context,
    PromptLogType.COMPONENT_EXTRACTION
  );
  
  return processResponse(result, `Component extraction failed for URL: ${imageUrl}`);
}

/**
 * Validates bounding boxes in an image
 * 
 * @param {Buffer|string} imageData Buffer or base64 string of the image
 * @param {Object} context Tracking context
 * @param {string} elementsJson JSON string of elements to validate
 * @returns {Promise<import('@/types/OpenAIServiceResponse').OpenAIServiceResponse>} Validation results with parsedContent property
 */
export async function validate_bounding_boxes_base64(imageData, context, elementsJson) {
  // Create prompt with elements JSON included
  const prompt = elementsJson 
    ? `${ACCURACY_VALIDATION_PROMPT_v0}\n\nHere are the elements to evaluate:\n${elementsJson}`
    : ACCURACY_VALIDATION_PROMPT_v0;
    
  const result = await callOpenAIVisionModelBase64(
    prompt, 
    imageData, // This can be buffer or base64 string, handled by safelyEncodeImageForOpenAI
    context,
    PromptLogType.ACCURACY_VALIDATION
  );

  return processResponse(result, 'Bounding box validation failed for image');
}

/**
 * Extracts metadata from a component image and its elements
 *
 * @param {Buffer|string} imageData Buffer or base64 string of the component image
 * @param {string} inputPayload JSON string containing component_name and elements
 * @param {Object} context The tracking context containing batch, screenshot, and other IDs
 * @returns {Promise<import('@/types/OpenAIServiceResponse').OpenAIServiceResponse>} A promise resolving to the structured metadata result
 * @throws {Error} Throws an error if the API call fails
 */
export async function extract_component_metadata(imageData, inputPayload, context) {
  // Create prompt with component and elements JSON included
  const prompt = `${METADATA_EXTRACTION_PROMPT_FINAL}\n\nHere is the component information:\n${inputPayload}`;
    
  const result = await callOpenAIVisionModelBase64(
    prompt, 
    imageData, // This can be buffer or base64 string, handled by safelyEncodeImageForOpenAI
    context,
    PromptLogType.METADATA_EXTRACTION
  );

  return processResponse(result, 'Metadata extraction failed for image');
}

/**
 * Processes OpenAI response data
 * 
 * @param {Object} result - The response from OpenAI
 * @param {string} errorMessage - Error message to use if processing fails
 * @returns {import('@/types/OpenAIServiceResponse').OpenAIServiceResponse} Object containing parsedContent and usage data
 */
function processResponse(result, errorMessage) {
  if (!result || result.status !== 'completed') {
    throw new Error(errorMessage);
  }

  const { parsedContent, usage } = extractOpenAIResponseData(result);
  return { parsedContent, usage };
}

/**
 * Helper: Parses OpenAI's output_text string into a JSON object safely.
 *
 * @param rawText - The raw `output_text` returned from OpenAI.
 * @returns Parsed JSON object.
 */
function parseOpenAIOutputTextToJson(rawText) {
  try {
    // First, remove markdown code block delimiters if they exist
    const trimmedText = rawText.trim();
    let contentText = trimmedText;
    
    // Check for and remove markdown code blocks (```json or just ```)
    const codeBlockRegex = /^```(?:json)?\s*([\s\S]*?)```$/;
    const match = trimmedText.match(codeBlockRegex);
    
    if (match && match[1]) {
      contentText = match[1].trim();
    }
    
    // Clean up any trailing commas that might cause JSON parsing errors
    const cleaned = contentText
      .replace(/,\s*}/g, '}') // remove trailing commas in objects
      .replace(/,\s*]/g, ']'); // remove trailing commas in arrays

    return JSON.parse(cleaned);
  } catch (error) {
    console.error('Failed to parse OpenAI output_text as JSON:', error);
    console.error('Raw text was:', rawText);
    return [];
  }
}

/**
 * Extracts structured content and usage metadata from OpenAI's response.
 *
 * @param response - Full OpenAI response object.
 * @returns Object containing parsed output_text and token usage info.
 */
export function extractOpenAIResponseData(response) {
  const rawText = response?.output_text ?? '';
  
  try {
    const parsedContent = parseOpenAIOutputTextToJson(rawText);
    
    const usage = {
      input_tokens: response?.usage?.input_tokens,
      output_tokens: response?.usage?.output_tokens,
      total_tokens: response?.usage?.total_tokens
    };
  
    return {
      parsedContent,
      rawText,
      usage,
    };
  } catch (error) {
    console.error('Error extracting OpenAI response data:', error);
    console.error('Raw response output_text (first 200 chars):', rawText.substring(0, 200));
    
    // Return empty array as parsedContent to handle gracefully
    return {
      parsedContent: [],
      rawText,
      usage: {
        input_tokens: response?.usage?.input_tokens || 0,
        output_tokens: response?.usage?.output_tokens || 0,
        total_tokens: response?.usage?.total_tokens || 0
      }
    };
  }
}

/**
 * Prepares a Buffer for OpenAI API by converting it to a proper base64 data URL
 * 
 * @param {Buffer} imageBuffer Buffer containing the image data
 * @param {string} mimeType Optional MIME type (defaults to image/png)
 * @returns {string} Properly formatted base64 data URL for the OpenAI API
 */
export function prepareImageBufferForOpenAI(imageBuffer, mimeType = 'image/png') {
  if (!imageBuffer || !Buffer.isBuffer(imageBuffer)) {
    throw new Error('Invalid image buffer provided');
  }
  
  const base64 = imageBuffer.toString('base64');
  return `data:${mimeType};base64,${base64}`;
}

/**
 * Safely encodes an image buffer to a base64 data URL format expected by OpenAI
 * This handles validation and proper formatting
 * 
 * @param {Buffer|string} imageData Image buffer or potentially already-encoded base64 string
 * @param {string} mimeType MIME type to use (defaults to image/png)
 * @returns {string} Properly formatted base64 data URL
 */
export function safelyEncodeImageForOpenAI(imageData, mimeType = 'image/png') {
  // If already a properly formatted data URL, return as is
  if (typeof imageData === 'string' && imageData.startsWith('data:image/')) {
    return imageData;
  }
  
  // If it's a Buffer, convert properly
  if (Buffer.isBuffer(imageData)) {
    return prepareImageBufferForOpenAI(imageData, mimeType);
  }
  
  // If it's a string but not a data URL, assume it's raw base64
  if (typeof imageData === 'string') {
    return `data:${mimeType};base64,${imageData}`;
  }
  
  throw new Error('Unable to prepare image data for OpenAI API');
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/imageServices/BoundingBoxService.js
================
import Jimp from 'jimp';
import fs from 'fs';
import path from 'path';

// --- Constants ---
const BOX_COLOR = 0xFF0000FF; // RGBA Red color
const BOX_WIDTH = 2;
const OVERLAY_COLOR = 0x80808080; // RGBA semi-transparent gray
const OVERLAY_ALPHA = 0.5;
const REFERENCE_POINT_COLOR = 0x00FF00FF; // RGBA Green color
const REFERENCE_POINT_SIZE = 4;
const TEXT_COLOR = 0xFFFFFFFF; // RGBA White color
const TEXT_BACKGROUND = 0x000000AA; // RGBA Black color with some transparency

// Accuracy Colors matching ValidationService thresholds
const ACCURACY_COLORS = {
  HIGH: 0x00FF00FF,   // Green (≥85%)
  MEDIUM: 0xFFFF00FF, // Yellow (70-84%)
  LOW_MEDIUM: 0xFFA500FF, // Orange (50-69%)
  LOW: 0xFF0000FF,    // Red (<50%)
  SUGGESTED: 0xFFA500FF // Orange for suggested boxes
};

/**
 * Normalizes a label string into a file/key-friendly format
 * @param {string} label - The label to normalize
 * @returns {string} Normalized label string
 */
function normalizeLabel(label) {
  return label.toLowerCase().replace(/\s/g, '_').replace(/>/g, '_').replace(/\//g, '_');
}

/**
 * Draw a rectangle with a specific width
 * @param {Jimp} image - Jimp image to draw on
 * @param {number} x - X coordinate of top-left corner
 * @param {number} y - Y coordinate of top-left corner
 * @param {number} width - Width of the rectangle
 * @param {number} height - Height of the rectangle
 * @param {number} color - Color of the rectangle (RGBA hex)
 * @param {number} lineWidth - Width of the rectangle border
 */
function drawRect(image, x, y, width, height, color, lineWidth) {
  // Draw top line
  image.scan(x, y, width, lineWidth, function(cx, cy, idx) {
    this.bitmap.data[idx] = (color >> 24) & 0xFF;     // R
    this.bitmap.data[idx + 1] = (color >> 16) & 0xFF; // G
    this.bitmap.data[idx + 2] = (color >> 8) & 0xFF;  // B
    this.bitmap.data[idx + 3] = color & 0xFF;         // A
  });
  
  // Draw bottom line
  image.scan(x, y + height - lineWidth, width, lineWidth, function(cx, cy, idx) {
    this.bitmap.data[idx] = (color >> 24) & 0xFF;     // R
    this.bitmap.data[idx + 1] = (color >> 16) & 0xFF; // G
    this.bitmap.data[idx + 2] = (color >> 8) & 0xFF;  // B
    this.bitmap.data[idx + 3] = color & 0xFF;         // A
  });
  
  // Draw left line
  image.scan(x, y, lineWidth, height, function(cx, cy, idx) {
    this.bitmap.data[idx] = (color >> 24) & 0xFF;     // R
    this.bitmap.data[idx + 1] = (color >> 16) & 0xFF; // G
    this.bitmap.data[idx + 2] = (color >> 8) & 0xFF;  // B
    this.bitmap.data[idx + 3] = color & 0xFF;         // A
  });
  
  // Draw right line
  image.scan(x + width - lineWidth, y, lineWidth, height, function(cx, cy, idx) {
    this.bitmap.data[idx] = (color >> 24) & 0xFF;     // R
    this.bitmap.data[idx + 1] = (color >> 16) & 0xFF; // G
    this.bitmap.data[idx + 2] = (color >> 8) & 0xFF;  // B
    this.bitmap.data[idx + 3] = color & 0xFF;         // A
  });
}

/**
 * Draw a filled rectangle on the image
 * @param {Jimp} image - Jimp image to draw on
 * @param {number} x - X coordinate of top-left corner
 * @param {number} y - Y coordinate of top-left corner
 * @param {number} width - Width of the rectangle
 * @param {number} height - Height of the rectangle
 * @param {number} color - Color of the rectangle (RGBA hex)
 */
function fillRect(image, x, y, width, height, color) {
  // Make sure coordinates are integers and within bounds
  const startX = Math.max(0, Math.floor(x));
  const startY = Math.max(0, Math.floor(y));
  const imgWidth = image.getWidth();
  const imgHeight = image.getHeight();
  
  // Ensure width/height calculations don't exceed image boundaries
  const endX = Math.min(imgWidth, startX + width);
  const endY = Math.min(imgHeight, startY + height);
  
  const drawWidth = endX - startX;
  const drawHeight = endY - startY;
  
  if (drawWidth <= 0 || drawHeight <= 0) return;
  
  image.scan(startX, startY, drawWidth, drawHeight, function(cx, cy, idx) {
    this.bitmap.data[idx] = (color >> 24) & 0xFF;     // R
    this.bitmap.data[idx + 1] = (color >> 16) & 0xFF; // G
    this.bitmap.data[idx + 2] = (color >> 8) & 0xFF;  // B
    this.bitmap.data[idx + 3] = color & 0xFF;         // A
  });
}

/**
 * Creates a transparent area in the overlay for detected objects
 * @param {Jimp} overlay - Overlay image to modify
 * @param {number} x - X coordinate of top-left corner
 * @param {number} y - Y coordinate of top-left corner
 * @param {number} width - Width of the transparent area
 * @param {number} height - Height of the transparent area
 * @param {number} boxWidth - Width of the border that should remain opaque
 */
function createTransparentArea(overlay, x, y, width, height, boxWidth) {
   // Ensure x, y, width, height are within overlay bounds and integers
   const overlayWidth = overlay.getWidth();
   const overlayHeight = overlay.getHeight();

   const startX = Math.max(0, Math.floor(x + boxWidth));
   const startY = Math.max(0, Math.floor(y + boxWidth));
   const endX = Math.min(overlayWidth, Math.floor(x + width - boxWidth));
   const endY = Math.min(overlayHeight, Math.floor(y + height - boxWidth));

   const clearWidth = Math.max(0, endX - startX);
   const clearHeight = Math.max(0, endY - startY);

   if (clearWidth > 0 && clearHeight > 0) {
       overlay.scan(startX, startY, clearWidth, clearHeight, function(cx, cy, idx) {
           this.bitmap.data[idx + 3] = 0; // Set alpha to 0 (fully transparent)
       });
   }
}

/**
 * Draw a simple coordinate label directly with pixel rectangles
 * @param {Jimp} image - Jimp image to draw on
 * @param {number} x - X coordinate for text start
 * @param {number} y - Y coordinate for text start
 * @param {string} value - Value to display
 * @param {number} color - Text color
 */
function drawCoordinateLabel(image, x, y, value, color = TEXT_COLOR) {
  // First draw background for better visibility
  const padding = 2;
  const charWidth = 5;
  const charHeight = 7;
  const totalWidth = value.length * (charWidth + 1) + padding * 2;
  const totalHeight = charHeight + padding * 2;
  
  // Ensure text stays within image bounds
  const imgWidth = image.getWidth();
  const imgHeight = image.getHeight();
  
  const textX = Math.max(0, Math.min(imgWidth - totalWidth, x));
  const textY = Math.max(0, Math.min(imgHeight - totalHeight, y));
  
  // Draw background
  fillRect(image, textX, textY, totalWidth, totalHeight, TEXT_BACKGROUND);
  
  // Draw text value
  image.scan(textX, textY, totalWidth, totalHeight, function(cx, cy, idx) {
    // Set entire area to the background color with some transparency
    this.bitmap.data[idx + 3] = 180; // Alpha component
  });
  
  // Place coordinate values as plain text using a simple technique
  const text = value;
  let currentX = textX + padding;
  
  for (let i = 0; i < text.length; i++) {
    // Draw a small rectangle for each character with the text color
    // This is a simple way to "stamp" the presence of text without rendering actual fonts
    fillRect(image, currentX, textY + padding, charWidth, charHeight, color);
    currentX += charWidth + 1;
  }
}

/**
 * Draw reference points at 0%, 25%, 50%, 75%, and 100% positions on each edge of the image
 * @param {Jimp} image - Jimp image to draw on
 * @param {number} imgWidth - Width of the image
 * @param {number} imgHeight - Height of the image
 * @param {number} color - Color for reference points (RGBA hex)
 * @param {number} pointSize - Size of the reference points
 */
function drawReferencePoints(image, imgWidth, imgHeight, color = REFERENCE_POINT_COLOR, pointSize = REFERENCE_POINT_SIZE) {
  const percentages = [0, 0.25, 0.5, 0.75, 1.0];
  
  // Top edge points
  percentages.forEach(percent => {
    const x = Math.floor(percent * (imgWidth - 1));
    const y = 0;
    
    // Draw point (larger to be more visible)
    fillRect(image, x - pointSize/2, y - pointSize/2, pointSize, pointSize, color);
    
    // Draw coordinate label
    const labelText = `(${x},${y})`;
    drawCoordinateLabel(image, x - 20, y + pointSize * 2, labelText);
  });
  
  // Bottom edge points
  percentages.forEach(percent => {
    const x = Math.floor(percent * (imgWidth - 1));
    const y = imgHeight - 1;
    
    // Draw point
    fillRect(image, x - pointSize/2, y - pointSize/2, pointSize, pointSize, color);
    
    // Draw coordinate label
    const labelText = `(${x},${y})`;
    drawCoordinateLabel(image, x - 20, y - 20, labelText);
  });
  
  // Left edge points
  percentages.forEach(percent => {
    const x = 0;
    const y = Math.floor(percent * (imgHeight - 1));
    
    // Draw point
    fillRect(image, x - pointSize/2, y - pointSize/2, pointSize, pointSize, color);
    
    // Draw coordinate label
    const labelText = `(${x},${y})`;
    drawCoordinateLabel(image, x + pointSize * 2, y - 10, labelText);
  });
  
  // Right edge points
  percentages.forEach(percent => {
    const x = imgWidth - 1;
    const y = Math.floor(percent * (imgHeight - 1));
    
    // Draw point
    fillRect(image, x - pointSize/2, y - pointSize/2, pointSize, pointSize, color);
    
    // Draw coordinate label
    const labelText = `(${x},${y})`;
    drawCoordinateLabel(image, x - 50, y - 10, labelText);
  });
}



/**
 * Generates an annotated image buffer for a specific component/category
 * @param {Buffer} baseImageBuffer - The original image buffer
 * @param {Array} detectedItems - Detected items for this component
 * @param {number} color - Color for bounding boxes (defaults to BOX_COLOR)
 * @param {string} categoryName - Name of the category (for logging)
 * @returns {Promise<Buffer|null>} Buffer of the annotated image, or null on error
 */
async function generateAnnotatedImageBuffer(baseImageBuffer, detectedItems, color = BOX_COLOR, categoryName) {
  const itemsToDraw = detectedItems.filter(item => item.status === 'Detected' && item.bounding_box);

  try {
    const baseImage = await Jimp.read(baseImageBuffer);
    const imgWidth = baseImage.getWidth();
    const imgHeight = baseImage.getHeight();

    // Create a semi-transparent overlay
    const overlay = new Jimp(imgWidth, imgHeight, OVERLAY_COLOR);

    // Sort itemsToDraw by area (descending) to process larger boxes first
    // This prevents large-box clear from erasing smaller borders
    itemsToDraw.sort((a, b) => {
      const areaA = (a.bounding_box.x_max - a.bounding_box.x_min) * (a.bounding_box.y_max - a.bounding_box.y_min);
      const areaB = (b.bounding_box.x_max - b.bounding_box.x_min) * (b.bounding_box.y_max - b.bounding_box.y_min);
      return areaB - areaA; // Descending order
    });

    // Process each detection and draw boxes on the overlay
    for (const item of itemsToDraw) {
      const { x_min, y_min, x_max, y_max } = item.bounding_box;

      // Make sure coordinates are integers and within bounds
      const x = Math.max(0, Math.floor(x_min));
      const y = Math.max(0, Math.floor(y_min));
      // Ensure width/height calculations don't exceed image boundaries
      const potentialWidth = Math.ceil(x_max - x_min);
      const potentialHeight = Math.ceil(y_max - y_min);
      const width = Math.min(imgWidth - x, potentialWidth);
      const height = Math.min(imgHeight - y, potentialHeight);

      if (width <= 0 || height <= 0) {
        console.warn(`Invalid box dimensions for item '${item.label}' in category '${categoryName}': ${width}x${height}`);
        continue;
      }

      // Draw the box outline on the overlay
      drawRect(overlay, x, y, width, height, color, BOX_WIDTH);

      // Create transparent area inside the box
      createTransparentArea(overlay, x, y, width, height, BOX_WIDTH);
    }

    // Composite the overlay onto the base image
    baseImage.composite(overlay, 0, 0, {
      mode: Jimp.BLEND_SOURCE_OVER,
      opacitySource: 1, // Use overlay's alpha
      opacityDest: 1
    });

    // Return the buffer
    const annotatedBuffer = await baseImage.getBufferAsync(Jimp.MIME_PNG);
    // console.log(`Generated annotated image buffer for category '${categoryName}'.`);
    return annotatedBuffer;

  } catch (err) {
    console.error(`Error generating annotated image buffer for category '${categoryName}': ${err}`);
    return null;
  }
}

/**
 * Renders a category image (for debugging/local saving)
 * @param {Buffer} annotatedImageBuffer - The generated annotated image buffer
 * @param {string} categoryName - Name of the category
 * @param {string} outputDir - Directory to save output
 * @returns {Promise<void>}
 */
async function saveAnnotatedImageDebug(annotatedImageBuffer, categoryName, outputDir) {
   if (!annotatedImageBuffer || process.env.SAVE_DEBUG_FILES !== 'true') {
     return; // Don't save if buffer is null or debug saving is off
   }
   try {
     const normalizedKey = normalizeLabel(categoryName);
     const savePath = path.join(outputDir, `${normalizedKey}.png`);
     await fs.promises.writeFile(savePath, annotatedImageBuffer);
    //  console.log(`Debug image saved successfully to: ${savePath}`);
   } catch (err) {
     console.error(`Error saving debug image for '${categoryName}': ${err}`);
   }
}

export {
  BOX_COLOR,
  BOX_WIDTH,
  OVERLAY_COLOR,
  OVERLAY_ALPHA,
  REFERENCE_POINT_COLOR,
  REFERENCE_POINT_SIZE,
  TEXT_COLOR,
  TEXT_BACKGROUND,
  ACCURACY_COLORS,
  drawRect,
  fillRect,
  // drawDashedRect,
  createTransparentArea,
  drawCoordinateLabel,
  drawReferencePoints,
  generateAnnotatedImageBuffer,
  saveAnnotatedImageDebug,
  normalizeLabel
};

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/imageServices/ImageFetchingService.ts
================
import { BatchProcessingScreenshot as Screenshot } from '@/types/BatchProcessingScreenshot';
import { Buffer } from 'buffer';

/**
 * Fetches an image and returns it as a Buffer for direct use in image processing.
 * Provides raw binary data with minimal overhead compared to Blob or Base64.
 *
 * @param imageUrl - the signed URL (or any URL) of the image
 * @returns a promise resolving to a Buffer containing the image data
 */
export async function fetchImageAsBuffer(imageUrl: string): Promise<Buffer | null> {
  try {
    const res = await fetch(imageUrl);
    if (!res.ok) throw new Error(`HTTP ${res.status} ${res.statusText}`);
    
    // Get the raw ArrayBuffer first
    const arrayBuffer = await res.arrayBuffer();
    
    // Convert ArrayBuffer to Buffer
    const buffer = Buffer.from(arrayBuffer);
    
    // console.log(`Successfully fetched image buffer (${buffer.byteLength} bytes)`);
    return buffer;
  } catch (err) {
    console.error("Error fetching image as buffer:", err);
    return null;
  }
}

/**
 * Fetches image data as Buffer for multiple screenshots with signed URLs
 * @param screenshots Array of screenshot objects with screenshot_signed_url property
 * @returns The same array with screenshot_image_buffer property populated
 */
export async function fetchScreenshotBuffers(screenshots: Screenshot[]): Promise<Screenshot[]> {
  // console.log(`Fetching image buffers for ${screenshots.length} screenshots...`);
  
  // Create an array of promises for fetching each image
  const fetchPromises = screenshots.map(async (screenshot) => {
    // Skip screenshots without a signed URL
    if (!screenshot.screenshot_signed_url) {
      console.warn(`Screenshot ID ${screenshot.screenshot_id} has no signed URL, skipping buffer fetch`);
      screenshot.screenshot_image_buffer = null;
      return screenshot;
    }
    
    try {
      // Fetch the image buffer and attach it directly
      screenshot.screenshot_image_buffer = await fetchImageAsBuffer(screenshot.screenshot_signed_url);
      
      if (screenshot.screenshot_image_buffer) {
        console.log(`Successfully fetched buffer for screenshot ID ${screenshot.screenshot_id} (${screenshot.screenshot_image_buffer.byteLength} bytes)`);
      }
    } catch (error) {
      console.error(`Error fetching buffer for screenshot ID ${screenshot.screenshot_id}:`, error);
      screenshot.screenshot_image_buffer = null;
    }
    
    return screenshot;
  });
  
  // Wait for all fetch operations to complete
  const updatedScreenshots = await Promise.all(fetchPromises);
  
  // Log summary of results
  const successCount = updatedScreenshots.filter(s => s.screenshot_image_buffer !== null).length;
  // console.log(`Fetched ${successCount}/${screenshots.length} image buffers successfully`);
  
  return updatedScreenshots;
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/imageServices/ImageProcessor.ts
================
import sharp from 'sharp'
import fs from 'fs'
import path from 'path'
// import { v4 as uuidv4 } from 'uuid'

// Constants instead of magic numbers
const MAX_FILE_SIZE_MB = 1
const DEFAULT_TARGET_WIDTH = 800
const DEFAULT_TARGET_HEIGHT = 800
const DEFAULT_JPEG_QUALITY = 80

// Temporary directory for processed images
const TEMP_DIR = path.join(process.cwd(), 'tmp')

/**
 * Normalizes a filename by removing special characters and spaces
 * @param filename - Original filename to normalize
 * @returns Normalized filename with only alphanumeric characters, dots, and hyphens
 */
function sanitizeFilename(filename: string): string {
  // Remove file extension
  const { name, ext } = path.parse(filename)
  
  // Replace spaces and special characters with hyphens
  const normalized = name
    .toLowerCase()
    .replace(/[^a-z0-9]/g, '-') // Replace non-alphanumeric with hyphens
    .replace(/-+/g, '-')        // Replace multiple hyphens with single hyphen
    .replace(/^-|-$/g, '')      // Remove leading/trailing hyphens
  
  // Add timestamp to ensure uniqueness
  return `${normalized}${ext}`
}

// Ensure temp directory exists
if (!fs.existsSync(TEMP_DIR)) {
  fs.mkdirSync(TEMP_DIR, { recursive: true })
}

interface ProcessedImage {
  buffer: Buffer;
  filename: string;
}

/**
 * Compresses and pads an image to make it uniform in size
 * @param imageBuffer - The raw image buffer
 * @param originalFilename - Original filename to preserve
 * @param targetWidth - Desired width after processing
 * @param targetHeight - Desired height after processing
 * @returns Object containing the path and filename of the processed image
 */
export async function resizeAndPadImageBuffer(
  imageBuffer: Buffer,
  originalFilename: string,
  targetWidth: number = DEFAULT_TARGET_WIDTH,
  targetHeight: number = DEFAULT_TARGET_HEIGHT
): Promise<ProcessedImage> {
  const filename = sanitizeFilename(originalFilename)
  
  try {
    const metadata = await sharp(imageBuffer).metadata()
    
    // Calculate resize dimensions while maintaining aspect ratio
    let resizeWidth = targetWidth
    let resizeHeight = targetHeight
    
    if (metadata.width && metadata.height) {
      const aspectRatio = metadata.width / metadata.height
      
      if (aspectRatio > 1) {
        // Landscape image
        resizeHeight = Math.round(targetWidth / aspectRatio)
      } else {
        // Portrait image
        resizeWidth = Math.round(targetHeight * aspectRatio)
      }
    }
    
    // Process image and return buffer directly
    const processedBuffer = await sharp(imageBuffer)
      .resize(resizeWidth, resizeHeight, {
        fit: 'inside',
        withoutEnlargement: true
      })
      .jpeg({ quality: DEFAULT_JPEG_QUALITY }) // Compress to reduce file size
      .toBuffer()
      .then(resizedBuffer => {
        // Create a blank canvas with the target dimensions
        return sharp({
          create: {
            width: targetWidth,
            height: targetHeight,
            channels: 4,
            background: { r: 255, g: 255, b: 255, alpha: 1 }
          }
        })
        .composite([{
          input: resizedBuffer,
          gravity: 'center'
        }])
        .jpeg({ quality: DEFAULT_JPEG_QUALITY })
        .toBuffer()
      })

    return {
      buffer: processedBuffer,
      filename
    }
  } catch (error) {
    console.error('Error processing image:', error)
    throw error
  }
}

/**
 * Cleans up a temporary file
 */
export function deleteFile(filePath: string): void {
  try {
    if (fs.existsSync(filePath)) {
      fs.unlinkSync(filePath)
    }
  } catch (error) {
    console.error('Error cleaning up temp file:', error)
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/imageServices/ScreenshotProcessor.ts
================
import { File } from 'formidable';
import fs from 'fs';
import { resizeAndPadImageBuffer, deleteFile } from './ImageProcessor';
import { uploadImageToStorage } from '@/lib/storage';
import { supabase } from '@/lib/supabase'; // Assuming shared Supabase client
import { SupabaseClient } from '@supabase/supabase-js';

interface ProcessedImage {
  processedBlob: Blob;
  filename: string;
  processingTime?: number; // In seconds
}

export class ScreenshotProcessor {
  private supabaseClient: SupabaseClient;

  // Allow injecting Supabase client for testability/flexibility
  constructor(supabaseClient: SupabaseClient = supabase) {
    this.supabaseClient = supabaseClient;
  }

  /**
   * Processes a single uploaded file: resizes, pads, uploads to storage,
   * and saves the record in the database.
   * @param file The uploaded file object from formidable.
   * @param batchId The ID of the batch this screenshot belongs to.
   * @returns The name and URL of the uploaded screenshot.
   * @throws Error if any step fails.
   */
  public async processAndSave(file: File, batchId: number): Promise<{ name: string; url: string }> {
    if (!file || !file.filepath) {
        throw new Error('Invalid file provided to ScreenshotProcessor.');
    }
    
    const processedImage = await this.processUploadedFile(file);
    const savedRecord = await this.saveScreenshotRecord(processedImage, batchId);
    
    // Clean up the temporary file after successful processing and saving
    try {
        deleteFile(file.filepath);
    } catch (cleanupError) {
        // Log cleanup error but don't fail the operation
        console.error(`Failed to delete temporary file ${file.filepath}:`, cleanupError);
    }

    return savedRecord;
  }

  /**
   * Reads, resizes, and pads the image file.
   * @param file The uploaded file object.
   * @returns Processed image data.
   */
  private async processUploadedFile(file: File): Promise<ProcessedImage> {
    const fileBuffer = fs.readFileSync(file.filepath);
    const startTime = Date.now();
    const originalFilename = file.originalFilename ?? `unnamed_${Date.now()}`;

    // Perform image resizing and padding
    const processed = await resizeAndPadImageBuffer(fileBuffer, originalFilename);
    const processingTime = (Date.now() - startTime) / 1000; // Convert ms to seconds

    return {
      processedBlob: new Blob([processed.buffer], { type: 'image/jpeg' }), // Assuming JPEG output
      filename: processed.filename,
      processingTime,
    };
  }

  /**
   * Uploads the processed image to storage and saves the metadata to the database.
   * @param image Processed image data.
   * @param batchId The batch ID.
   * @returns The name and URL of the uploaded screenshot.
   */
  private async saveScreenshotRecord(
    image: ProcessedImage,
    batchId: number
  ): Promise<{ name: string; url: string }> {
    // Upload to Supabase storage
    const { fileUrl, error: uploadError } = await uploadImageToStorage(
      image.processedBlob,
      batchId,
      image.filename
    );
    if (uploadError) {
        console.error('Supabase storage upload error:', uploadError);
        throw new Error(`Failed to upload ${image.filename} to storage.`);
    }

    // Insert record into Supabase database
    const { error: dbError } = await this.supabaseClient
      .from('screenshot')
      .insert({
        batch_id: batchId,
        screenshot_file_name: image.filename,
        screenshot_file_url: fileUrl,
        screenshot_processing_status: 'pending', // Initial status before extraction
        screenshot_processing_time: image.processingTime ? `${image.processingTime.toFixed(2)} seconds` : null,
      });

    if (dbError) {
      console.error('Supabase screenshot insert error:', dbError);
      // Attempt to delete the uploaded file if DB insert fails to avoid orphans
      try {
          // TODO: Implement deletion from storage if needed
          console.warn(`DB insert failed for ${image.filename}, corresponding storage file might be orphaned: ${fileUrl}`);
      } catch (deleteError) {
          console.error(`Failed to delete orphaned storage file ${fileUrl}:`, deleteError);
      }
      throw new Error(`Failed to save screenshot record for ${image.filename}.`);
    }

    return {
      name: image.filename,
      url: fileUrl,
    };
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/AccuracyValidationService.ts
================
import { ComponentDetectionResult, ElementDetectionItem } from '@/types/DetectionResult';
// import { validate_bounding_boxes_base64 } from '@/lib/services/ai/OpenAIDirectService';
import { validate_bounding_boxes_base64 } from '@/lib/services/ai/OpenAIService';
import pLimit from 'p-limit';
import { createScreenshotTrackingContext } from '@/lib/logger';
import { VALIDATION_CONCURRENCY } from '@/lib/constants'


/**
 * AccuracyValidationService
 * 
 * This service validates the accuracy of detected UI elements by:
 * 1. Processing components in parallel with controlled concurrency
 * 2. For each component, validating the accuracy of bounding boxes using OpenAI
 * 3. Re-rendering annotated images with different colors based on accuracy
 * 4. Adding accuracy scores and suggested coordinates to each element
 */
export class AccuracyValidationService {
  /**
   * Validates the accuracy of detected UI elements and updates their metadata
   * 
   * @param batchId - The ID of the batch being processed
   * @param components - Array of ComponentDetectionResult to validate
   * @returns The components array with updated elements and re-annotated images
   */
  public static async performAccuracyValidation(
    batchId: number,
    components: ComponentDetectionResult[]
  ): Promise<ComponentDetectionResult[]> {
    console.log(`[Batch ${batchId}] Stage 3: Starting Accuracy Validation for ${components.length} components...`);
    
    // Create a concurrency limiter
    const validationLimit = pLimit(VALIDATION_CONCURRENCY);
    
    // Process each component in parallel
    const validationPromises = components.map(component => 
      validationLimit(async () => {
        const screenshotId = component.screenshot_id;
        // console.log(`[Batch ${batchId}] Stage 3: Validating component ${component.component_name} for screenshot ${screenshotId}...`);
        
        try {
          // Create tracking context for logging
          const context = createScreenshotTrackingContext(batchId, screenshotId);
          
          // Base64 encode the annotated image
          const imageBase64 = component.annotated_image_object.toString('base64');
          
          // Create elements JSON to send to OpenAI
          const elementsJson = JSON.stringify(component.elements);
          
          // Call OpenAI to validate bounding boxes
          const validationResult = await validate_bounding_boxes_base64(
            imageBase64,
            context,
            elementsJson
          );
            
          // Update elements with accuracy scores and suggested coordinates
          // This mutates the elements array in-place
          this.updateElementsWithValidation(component.elements, validationResult.parsedContent);

          // console.log(`[Batch ${batchId}] Stage 3: Updated elements for component ${component.component_name}:`, JSON.stringify(component.elements, null, 2));
        
          console.log(`[Batch ${batchId}] Stage 3: Completed validation for component ${component.component_name}`);
          
          return component;
        } catch (error) {
          console.error(`[Batch ${batchId}] Stage 3: Error validating component ${component.component_name}:`, error);
          // Return the original component if validation fails
          return component;
        }
      })
    );
    
    // Wait for all components to be validated
    const validatedComponents = await Promise.all(validationPromises);
    
    console.log(`[Batch ${batchId}] Stage 3: Completed Accuracy Validation for all components`);
    
    return validatedComponents;
  }
  
  /**
   * Updates elements with accuracy scores and suggested coordinates
   * 
   * @param elements - Array of elements to update
   * @param validationData - Validation data from OpenAI
   */
  private static updateElementsWithValidation(
    elements: ElementDetectionItem[],
    validationData: any
  ): void {
    // Ensure validation data has the expected format
    if (!validationData) {
      console.warn('Validation data is null or undefined');
      return;
    }
    
    // Handle both array and object with elements property formats
    let validatedElements = validationData;
    if (!Array.isArray(validationData)) {
      console.warn('Invalid validation data format, elements array not found');
      return;
    }
    
    // Create a map of elements by label for easier lookup
    const elementMap = new Map<string, ElementDetectionItem>();
    elements.forEach(element => {
      elementMap.set(element.label, element);
    });
    
    // Update elements with validation data
    validatedElements.forEach((validatedElement: any) => {
      const element = elementMap.get(validatedElement.label);
      
      if (element) {
        try {
          // Attempt to update element properties with validated data
          element.status = validatedElement.status? validatedElement.status : 'Error';
          element.accuracy_score = validatedElement.accuracy? validatedElement.accuracy : 0;
          element.hidden = validatedElement.hidden? validatedElement.hidden : false;
          element.explanation = validatedElement.explanation? validatedElement.explanation : '';
          element.suggested_coordinates = validatedElement.suggested_coordinates? validatedElement.suggested_coordinates : undefined;
        } catch (error) {
          console.error(`Failed to update element '${element.label}' with validation data:`, error);
        }
      }
    });
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/BatchAnalyticsService.ts
================
import { supabase } from '@/lib/supabase';
import { SupabaseClient } from '@supabase/supabase-js';
import { 
  PromptBatchSummaryRecord, 
  SimplifiedBatchAnalyticsSummary, 
  SimplifiedPromptBatchRecord, 
  DetailedBatchAnalytics
} from '@/types/BatchSummaries';

// Constants
const PROMPT_BATCH_SUMMARY_VIEW = 'prompt_batch_summary';
const VLM_LABELING_TYPE = 'vlm_labeling';

export class BatchAnalyticsService {
  private supabaseClient: SupabaseClient;

  constructor(supabaseClient: SupabaseClient = supabase) {
    this.supabaseClient = supabaseClient;
  }

  public async getAll(): Promise<DetailedBatchAnalytics[]> {
    const allRecords = await this.fetchRawAnalyticsData();
    return this.processRawData(allRecords);
  }

  public async getById(batchId: number): Promise<DetailedBatchAnalytics | null> {
    const recordsForBatch = await this.fetchRawAnalyticsData(batchId);
    const processedData = this.processRawData(recordsForBatch);
    return processedData.length > 0 ? processedData[0] : null;
  }

  private async fetchRawAnalyticsData(batchId?: number): Promise<PromptBatchSummaryRecord[]> {
    try {
      let query = this.supabaseClient
        .from(PROMPT_BATCH_SUMMARY_VIEW)
        .select('*');

      if (batchId !== undefined) {
        query = query.eq('batch_id', batchId);
      }

      const { data, error } = await query;

      if (error) {
        throw this.handleFetchError(error, batchId);
      }
      
      return (data as PromptBatchSummaryRecord[]) || [];
    } catch (error) {
      // Catch potential errors from handleFetchError or other issues
      console.error(`Error fetching raw analytics data${batchId ? ' for batch ' + batchId : ''}:`, error);
      throw error; // Re-throw to be handled by the caller
    }
  }

  private processRawData(allRecords: PromptBatchSummaryRecord[]): DetailedBatchAnalytics[] {
    if (!allRecords || allRecords.length === 0) {
      return [];
    }

    const recordsByBatch = this.groupRecordsByBatchId(allRecords);
    const detailedAnalytics: DetailedBatchAnalytics[] = [];

    for (const [_, recordsForBatch] of recordsByBatch.entries()) {
      const analytics = this.createDetailedAnalyticsForBatch(recordsForBatch);
      if (analytics) {
        detailedAnalytics.push(analytics);
      }
    }

    return detailedAnalytics;
  }

  private createDetailedAnalyticsForBatch(recordsForBatch: PromptBatchSummaryRecord[]): DetailedBatchAnalytics | null {
    const summary = this.calculateSimplifiedSummary(recordsForBatch);
    if (!summary) {
      return null;
    }
    
    const simplifiedBreakdown = recordsForBatch.map(record => this.mapToSimplifiedBreakdown(record));

    // Assign to renamed top-level fields
    return {
      batch_summary: summary,
      prompt_type_summary: simplifiedBreakdown
    };
  }

  private groupRecordsByBatchId(records: PromptBatchSummaryRecord[]): Map<number, PromptBatchSummaryRecord[]> {
    const batchMap = new Map<number, PromptBatchSummaryRecord[]>();
    for (const record of records) {
      const batchId = record.batch_id;
      if (!batchMap.has(batchId)) {
        batchMap.set(batchId, []);
      }
      batchMap.get(batchId)?.push(record);
    }
    return batchMap;
  }

  private calculateSimplifiedSummary(records: PromptBatchSummaryRecord[]): SimplifiedBatchAnalyticsSummary | null {
    if (!records || records.length === 0) {
      return null;
    }

    const sampleRecord = records[0];
    const batchId = sampleRecord.batch_id;

    let totalTimeSpan = 0;
    let vlmLabelingCount = 0;

    for (const record of records) {
      totalTimeSpan += record.total_time_span_for_type_seconds;

      if (record.prompt_log_type === VLM_LABELING_TYPE) {
        vlmLabelingCount += record.prompts_ran_for_type;
      }
    }

    const avgTimePerElement = vlmLabelingCount > 0 ? totalTimeSpan / vlmLabelingCount : 0;

    // Return object with renamed fields
    return {
      batch_id: batchId,
      total_batch_processing_time_seconds: Number(totalTimeSpan.toFixed(2)),
      total_elements_detected: vlmLabelingCount,
      avg_seconds_per_element: Number(avgTimePerElement.toFixed(2)), // Renamed field
      total_input_tokens: sampleRecord.total_input_tokens_batch_level, // Renamed field
      total_output_tokens: sampleRecord.total_output_tokens_batch_level, // Renamed field
    };
  }

  private mapToSimplifiedBreakdown(record: PromptBatchSummaryRecord): SimplifiedPromptBatchRecord {
    // Return object with renamed fields
    return {
      batch_id: record.batch_id,
      first_prompt_started: record.first_prompt_started,
      last_prompt_completed: record.last_prompt_completed,
      prompt_type_name: record.prompt_log_type, // Renamed field
      prompt_type_log_count: record.prompts_ran_for_type, // Renamed field
      total_input_tokens_for_type: record.total_input_tokens_for_type,
      total_output_tokens_for_type: record.total_output_tokens_for_type,
      avg_output_tokens_per_prompt: record.avg_output_tokens_for_type, // Renamed field
      avg_processing_seconds_per_prompt: record.avg_processing_time_seconds_for_type, // Renamed field
      total_processing_time_seconds: record.total_time_span_for_type_seconds, // Renamed field
    };
  }

  private handleFetchError(error: any, batchId?: number): Error {
    const context = batchId ? ` for batch ${batchId}` : '';
    const message = `Failed to fetch batch analytics${context}: ${error.message}`;
    console.error(`Database Error${context}:`, error);
    return new Error(message);
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/BatchComponentLoaderService.ts
================
import { supabase } from '@/lib/supabase';
import { SupabaseClient } from '@supabase/supabase-js';
import { ComponentDetectionResult, ElementDetectionItem } from '@/types/DetectionResult';
import { BatchProcessingScreenshot as Screenshot } from '@/types/BatchProcessingScreenshot';
import { getScreenshotPath, getSignedUrls } from '@/lib/supabaseUtils';
import fs from 'fs';

// Constants
const LOCAL_LOG_ENABLED = process.env.LOCAL_LOG === 'true';

export class BatchComponentLoaderService {
  private supabaseClient: SupabaseClient;

  constructor(supabaseClient: SupabaseClient = supabase) {
    this.supabaseClient = supabaseClient;
  }

  /**
   * Loads all components and elements for a batch
   * @param batchId The ID of the batch to load
   * @returns Array of ComponentDetectionResult objects with screenshot, component, and element data
   */
  public async loadBatchComponents(batchId: number): Promise<ComponentDetectionResult[]> {
    // Step 1: Fetch all screenshots for the batch
    const screenshots = await this.getBatchScreenshots(batchId);
    if (screenshots.length === 0) {
      console.log(`[Batch ${batchId}] No screenshots found.`);
      return [];
    }
    console.log(`[Batch ${batchId}] Found ${screenshots.length} screenshots.`);

    // Step 2: Process signed URLs for all screenshots
    await this.processSignedUrls(batchId, screenshots);

    // Step 3: Build component results for each screenshot
    const results = await this.buildComponentResults(screenshots);
    
    this.logResultsIfEnabled(batchId, results);
    return results;
  }

  /**
   * Builds component detection results for all screenshots
   * @param screenshots Array of screenshot objects
   * @returns Array of component detection results
   */
  private async buildComponentResults(screenshots: Screenshot[]): Promise<ComponentDetectionResult[]> {
    const results: ComponentDetectionResult[] = [];
    
    for (const screenshot of screenshots) {
      const components = await this.getScreenshotComponents(screenshot.screenshot_id);
      
      for (const component of components) {
        const elements = await this.getComponentElements(component.component_id);
        const result = this.createComponentResult(screenshot, component, elements);
        results.push(result);
      }
    }
    
    return results;
  }

  /**
   * Creates a single component detection result
   * @param screenshot Screenshot data
   * @param component Component data 
   * @param elements Element data
   * @returns A formatted ComponentDetectionResult
   */
  private createComponentResult(
    screenshot: Screenshot, 
    component: any, 
    elements: any[]
  ): ComponentDetectionResult {
    return {
      screenshot_id: screenshot.screenshot_id,
      component_id: component.component_id,
      component_name: component.component_name || 'Unnamed Component',
      annotated_image_object: Buffer.from([]), // Empty buffer since we don't have the actual image
      component_description: component.component_description || '',
      detection_status: component.detection_status || 'success',
      inference_time: component.inference_time || 0,
      screenshot_url: screenshot.screenshot_signed_url || undefined,
      component_ai_description: component.component_ai_description || undefined,
      component_metadata_extraction: component.component_metadata_extraction || undefined,
      elements: this.formatElementsData(elements)
    };
  }

  /**
   * Formats element data into the expected format
   * @param elements Raw element data from database
   * @returns Formatted element detection items
   */
  private formatElementsData(elements: any[]): ElementDetectionItem[] {
    return elements.map(element => ({
      element_id: element.element_id,
      label: element.element_label || '',
      description: element.element_description || '',
      bounding_box: element.bounding_box || { x_min: 0, y_min: 0, x_max: 0, y_max: 0 },
      status: element.element_status || 'Detected',
      element_inference_time: element.element_inference_time,
      accuracy_score: element.element_accuracy_score,
      suggested_coordinates: element.suggested_coordinates,
      hidden: element.element_hidden,
      explanation: element.element_explanation,
      element_metadata_extraction: element.element_metadata_extraction
    }));
  }

  /**
   * Logs results to a file if local logging is enabled
   * @param batchId The batch ID
   * @param results The results to log
   */
  private logResultsIfEnabled(batchId: number, results: ComponentDetectionResult[]): void {
    if (LOCAL_LOG_ENABLED) {
      fs.writeFileSync(`batch_${batchId}_components.json`, JSON.stringify(results, null, 2));
    }
  }

  /**
   * Fetches all screenshot records for a given batch ID
   * @param batchId The ID of the batch
   * @returns Array of screenshot records
   */
  private async getBatchScreenshots(batchId: number): Promise<Screenshot[]> {
    const { data, error } = await this.supabaseClient
      .from('screenshot')
      .select('*')
      .eq('batch_id', batchId);

    if (error) {
      console.error(`[Batch ${batchId}] Supabase screenshot fetch error:`, error);
      return [];
    }

    return (data as Screenshot[] | null) || [];
  }

  /**
   * Processes signed URLs for screenshots
   * @param batchId The ID of the batch
   * @param screenshots Array of screenshot objects
   */
  private async processSignedUrls(batchId: number, screenshots: Screenshot[]): Promise<void> {
    const filePaths = this.extractValidFilePaths(screenshots);

    if (filePaths.length === 0) {
      console.log(`[Batch ${batchId}] No valid file paths found. Skipping signed URL fetch.`);
      this.clearScreenshotUrls(screenshots);
      return;
    }

    const signedUrls = await this.fetchSignedUrls(batchId, filePaths);
    this.attachSignedUrlsToScreenshots(batchId, screenshots, signedUrls);
  }

  /**
   * Extracts valid file paths from screenshot objects
   * @param screenshots Array of screenshot objects
   * @returns Array of valid file paths
   */
  private extractValidFilePaths(screenshots: Screenshot[]): string[] {
    return screenshots
      .map(s => getScreenshotPath(s.screenshot_file_url))
      .filter((p): p is string => p !== null);
  }

  /**
   * Clears screenshot URLs (used when no valid paths are found)
   * @param screenshots Array of screenshot objects
   */
  private clearScreenshotUrls(screenshots: Screenshot[]): void {
    screenshots.forEach(s => {
      s.screenshot_signed_url = undefined;
      s.screenshot_bucket_path = undefined;
    });
  }

  /**
   * Fetches signed URLs for file paths
   * @param batchId The ID of the batch for logging
   * @param filePaths Array of file paths
   * @returns Map of file paths to signed URLs
   */
  private async fetchSignedUrls(batchId: number, filePaths: string[]): Promise<Map<string, string>> {
    let signedUrls = new Map<string, string>();
    try {
      signedUrls = await getSignedUrls(this.supabaseClient, filePaths);
    } catch (urlError) {
      console.error(`[Batch ${batchId}] Failed to get signed URLs:`, urlError);
    }
    return signedUrls;
  }

  /**
   * Attaches signed URLs to screenshot objects
   * @param batchId The ID of the batch for logging
   * @param screenshots Array of screenshot objects
   * @param signedUrls Map of file paths to signed URLs
   */
  private attachSignedUrlsToScreenshots(
    batchId: number,
    screenshots: Screenshot[],
    signedUrls: Map<string, string>
  ): void {
    let attachedCount = 0;
    screenshots.forEach(s => {
      const path = getScreenshotPath(s.screenshot_file_url);
      if (path && signedUrls.has(path)) {
        s.screenshot_signed_url = signedUrls.get(path)!;
        s.screenshot_bucket_path = path;
        attachedCount++;
      } else {
        s.screenshot_signed_url = undefined;
        s.screenshot_bucket_path = undefined;
      }
    });
    console.log(`[Batch ${batchId}] Attached signed URLs to ${attachedCount} out of ${screenshots.length} screenshots.`);
  }

  /**
   * Fetches all components for a screenshot
   * @param screenshotId The ID of the screenshot
   * @returns Array of component records
   */
  private async getScreenshotComponents(screenshotId: number): Promise<any[]> {
    const { data, error } = await this.supabaseClient
      .from('component')
      .select('*')
      .eq('screenshot_id', screenshotId);

    if (error) {
      console.error(`[Screenshot ${screenshotId}] Supabase component fetch error:`, error);
      return [];
    }

    return data || [];
  }

  /**
   * Fetches all elements for a component
   * @param componentId The ID of the component
   * @returns Array of element records
   */
  private async getComponentElements(componentId: number): Promise<any[]> {
    const { data, error } = await this.supabaseClient
      .from('element')
      .select('*')
      .eq('component_id', componentId);

    if (error) {
      console.error(`[Component ${componentId}] Supabase element fetch error:`, error);
      return [];
    }

    return data || [];
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/BatchProcessingService.ts
================
import { supabase } from '@/lib/supabase'; 
import { SupabaseClient } from '@supabase/supabase-js';
import { getScreenshotPath, getSignedUrls } from '@/lib/supabaseUtils';
import { fetchScreenshotBuffers } from '@/lib/services/imageServices/ImageFetchingService';
import { BatchProcessingScreenshot as Screenshot } from '@/types/BatchProcessingScreenshot';
import { AIExtractionService, Stage1Result } from '@/lib/services/ParallelExtractionService';
import { ParallelMoondreamDetectionService } from '@/lib/services/ParallelAnnotationService';
import { AccuracyValidationService } from '@/lib/services/AccuracyValidationService';
import { MetadataExtractionService } from '@/lib/services/MetadataExtractionService';
import { SaveAnnotationService } from '@/lib/services/SaveAnnotationService';
import { ProcessStatus } from '@/lib/constants';
import fs from 'fs';

// --- Constants ---
const ERROR_STAGES = {
  SETUP: 'setup',
  EXTRACTION: 'extraction',
  ANNOTATION: 'annotation',
  VALIDATION: 'validation',
  METADATA: 'metadata',
  PERSISTENCE: 'persistence'
};

export class BatchProcessingService {
  private supabaseClient: SupabaseClient;
  private saveAnnotationService: SaveAnnotationService;

  constructor(
    supabaseClient: SupabaseClient = supabase,
    saveAnnotationService?: SaveAnnotationService
  ) {
    this.supabaseClient = supabaseClient;
    this.saveAnnotationService = saveAnnotationService || new SaveAnnotationService(supabaseClient);
  }

  /**
   * Starts the processing pipeline for a given batch.
   * Changes status, fetches screenshots, gets signed URLs, processes each, and updates status.
   * @param batchId The ID of the batch to process.
   */
  public async start(batchId: number): Promise<void> {
    try {
      // --- Setup: Load Screenshots, URLs, Buffers ---
      const screenshotsToProcess = await this.runSetupStage(batchId);
      if (!screenshotsToProcess || screenshotsToProcess.length === 0) {
        return; // Early return handled in the helper function with appropriate status updates
      }

      // --- Stage 1: Parallel AI Component/Element/Anchor Extraction ---
      const stage1Results = await this.runExtractionStage(batchId, screenshotsToProcess);
      
      // --- Stage 2: Parallel Moondream Detection ---
      const allDetectionResults = await this.runAnnotationStage(batchId, screenshotsToProcess, stage1Results);
      if (!allDetectionResults) return;
      
      // --- Stage 3: Accuracy Validation ---
      const validatedResults = await this.runValidationStage(batchId, allDetectionResults);
      if (!validatedResults) return;
      
      // --- Stage 4: Metadata Extraction ---
      const enrichedResults = await this.runMetadataStage(batchId, validatedResults);
      if (!enrichedResults) return;
      
      // --- Stage 5: Persistence to Database ---
      await this.runPersistenceStage(batchId, enrichedResults);

      // --- Finalize ---
      await this.updateBatchStatus(batchId, ProcessStatus.DONE);
      console.log(`[Batch ${batchId}] Processing complete. Status set to done.`);

    } catch (error) {
      await this.handleProcessingError(batchId, error, 'unknown');
    }
  }

  /**
   * Runs the setup stage - loads screenshots and prepares them for processing
   */
  private async runSetupStage(batchId: number): Promise<Screenshot[]> {
    try {
      const screenshotsToProcess = await this.loadAndPrepareScreenshots(batchId);
      return screenshotsToProcess;
    } catch (error) {
      await this.handleProcessingError(batchId, error, ERROR_STAGES.SETUP);
      return [];
    }
  }

  /**
   * Runs the extraction stage - AI component/element/anchor extraction
   */
  private async runExtractionStage(batchId: number, screenshotsToProcess: Screenshot[]): Promise<Map<number, Stage1Result>> {
    try {
      await this.updateBatchStatus(batchId, ProcessStatus.EXTRACTING);
      console.log(`[Batch ${batchId}] Begin Parallel Extraction on ${screenshotsToProcess.length} screenshots`);
      
      return await AIExtractionService.performAIExtraction(batchId, screenshotsToProcess);
    } catch (error) {
      await this.handleProcessingError(batchId, error, ERROR_STAGES.EXTRACTION);
      return new Map();
    }
  }

  /**
   * Runs the annotation stage - Moondream detection
   */
  private async runAnnotationStage(
    batchId: number, 
    screenshotsToProcess: Screenshot[], 
    stage1Results: Map<number, Stage1Result>
  ) {
    try {
      const successfulScreenshotIds = this.filterSuccessfulStage1Results(stage1Results);
      const screenshotsForMoondream = screenshotsToProcess.filter(s => 
        successfulScreenshotIds.has(s.screenshot_id)
      );

      if (screenshotsForMoondream.length === 0) {
        console.warn(`[Batch ${batchId}] No screenshots successfully completed Stage 1. Cannot proceed to Moondream.`);
        await this.updateBatchStatus(batchId, ProcessStatus.FAILED);
        return null;
      }
      
      console.log(`[Batch ${batchId}] ${screenshotsForMoondream.length} screenshots proceeding to Stage 2 (Moondream).`);
      await this.updateBatchStatus(batchId, ProcessStatus.ANNOTATING);
      
      return await ParallelMoondreamDetectionService.performMoondreamDetection(
        batchId, 
        screenshotsForMoondream, 
        stage1Results
      );
    } catch (error) {
      await this.handleProcessingError(batchId, error, ERROR_STAGES.ANNOTATION);
      return null;
    }
  }

  /**
   * Filters and extracts IDs of screenshots that successfully completed Stage 1
   */
  private filterSuccessfulStage1Results(stage1Results: Map<number, Stage1Result>): Set<number> {
    return new Set(
      Array.from(stage1Results.entries())
        .filter(([_, result]) => !result.error)
        .map(([id, _]) => id)
    );
  }

  /**
   * Runs the validation stage - Accuracy validation
   */
  private async runValidationStage(batchId: number, detectionResults: any) {
    try {
      await this.updateBatchStatus(batchId, ProcessStatus.VALIDATING);
      console.log(`[Batch ${batchId}] Stage 3: Starting Accuracy Validation...`);
      
      const validatedResults = await AccuracyValidationService.performAccuracyValidation(
        batchId,
        detectionResults
      );
      console.log(`[Batch ${batchId}] Stage 3: Accuracy Validation complete.`);
      
      this.debugWriteResultsToFile(batchId, validatedResults, 'validation');
      
      return validatedResults;
    } catch (error) {
      await this.handleProcessingError(batchId, error, ERROR_STAGES.VALIDATION);
      return null;
    }
  }

  /**
   * Runs the metadata extraction stage
   */
  private async runMetadataStage(batchId: number, validatedResults: any) {
    try {
      console.log(`[Batch ${batchId}] Stage 4: Starting Metadata Extraction...`);
      
      const enrichedResults = await MetadataExtractionService.performMetadataExtraction(
        batchId,
        validatedResults
      );
      console.log(`[Batch ${batchId}] Stage 4: Metadata Extraction complete.`);
      
      this.debugWriteResultsToFile(batchId, enrichedResults, 'final');
      
      return enrichedResults;
    } catch (error) {
      await this.handleProcessingError(batchId, error, ERROR_STAGES.METADATA);
      return null;
    }
  }

  /**
   * Runs the persistence stage - saving results to database
   */
  private async runPersistenceStage(batchId: number, enrichedResults: any) {
    try {
      await this.updateBatchStatus(batchId, ProcessStatus.SAVING);
      console.log(`[Batch ${batchId}] Stage 5: Persisting results to database...`);
      
      await this.saveAnnotationService.persistResults(batchId, enrichedResults);
      console.log(`[Batch ${batchId}] Stage 5: Database persistence complete.`);
      
      return true;
    } catch (error) {
      await this.handleProcessingError(batchId, error, ERROR_STAGES.PERSISTENCE);
      return null;
    }
  }

  /**
   * Helper to write results to file for debugging
   */
  private debugWriteResultsToFile(batchId: number, results: any, stage: string) {
    try {
      const replacer = (key: string, value: any) => {
        if ((key === 'annotated_image_object' || key === 'original_image_object') && value && value.type === 'Buffer') {
          return `[Buffer data omitted: ${value.data ? value.data.length : 'N/A'} bytes]`;
        } else if (Buffer.isBuffer(value)) {
          return `[Buffer data omitted: ${value.length} bytes]`;
        }
        return value;
      };

      fs.writeFileSync(`batch_${batchId}_${stage}_results.json`, JSON.stringify(results, replacer, 2));
    } catch (error) {
      console.warn(`[Batch ${batchId}] Failed to write debug file for ${stage} stage:`, error);
    }
  }

  /**
   * Loads screenshots, processes signed URLs, and fetches screenshot buffers
   * @param batchId The ID of the batch to process
   * @returns Array of screenshots ready for processing with buffers and signed URLs
   */
  private async loadAndPrepareScreenshots(batchId: number): Promise<Screenshot[]> {
    try {
      const screenshots = await this.loadScreenshots(batchId);
      if (screenshots.length === 0) {
        console.log(`[Batch ${batchId}] No screenshots found. Setting status to done.`);
        await this.updateBatchStatus(batchId, ProcessStatus.DONE);
        return [];
      }
      console.log(`[Batch ${batchId}] Found ${screenshots.length} screenshots.`);

      await this.processSignedUrls(batchId, screenshots);
      await this.fetchScreenshotBuffers(screenshots);

      const screenshotsToProcess = screenshots.filter(
        s => s.screenshot_image_buffer && s.screenshot_signed_url
      );

      if (screenshotsToProcess.length === 0) {
        console.warn(`[Batch ${batchId}] No screenshots with image buffers and signed URLs found after fetching. Cannot proceed.`);
        await this.updateBatchStatus(batchId, ProcessStatus.FAILED);
        return [];
      }
      
      return screenshotsToProcess;
    } catch (error) {
      throw new Error(`Failed to load and prepare screenshots: ${error instanceof Error ? error.message : String(error)}`);
    }
  }

  private async loadScreenshots(batchId: number): Promise<Screenshot[]> {
    try {
      const screenshots = await this.getBatchScreenshots(batchId);
      return screenshots;
    } catch (error) {
      throw new Error(`Failed to load screenshots: ${error instanceof Error ? error.message : String(error)}`);
    }
  }

  private async processSignedUrls(batchId: number, screenshots: Screenshot[]): Promise<void> {
    try {
      const filePaths = screenshots
        .map(s => getScreenshotPath(s.screenshot_file_url))
        .filter((p): p is string => p !== null);

      if (filePaths.length !== screenshots.length) {
        console.warn(
          `[Batch ${batchId}] ${screenshots.length - filePaths.length} invalid screenshot file URLs found. Associated screenshots skipped for URL generation.`
        );
      }
      
      if (filePaths.length === 0) {
        console.log(`[Batch ${batchId}] No valid file paths found. Skipping signed URL fetch.`);
        screenshots.forEach(s => {
            s.screenshot_signed_url = undefined;
            s.screenshot_bucket_path = undefined;
        });
        return;
      }

      let signedUrls = new Map<string, string>();
      try {
        signedUrls = await getSignedUrls(this.supabaseClient, filePaths);
      } catch (urlError) {
        console.error(`[Batch ${batchId}] Failed to get signed URLs:`, urlError);
      }

      let attachedCount = 0;
      screenshots.forEach(s => {
        const path = getScreenshotPath(s.screenshot_file_url);
        if (path && signedUrls.has(path)) {
          s.screenshot_signed_url = signedUrls.get(path)!;
          s.screenshot_bucket_path = path;
          attachedCount++;
        } else {
          s.screenshot_signed_url = undefined;
          s.screenshot_bucket_path = undefined;
        }
      });
    } catch (error) {
      throw new Error(`Failed to process signed URLs: ${error instanceof Error ? error.message : String(error)}`);
    }
  }

  /**
   * Handles errors during batch processing
   * @param batchId The ID of the batch
   * @param error The error that occurred
   * @param stage The processing stage where the error occurred
   */
  private async handleProcessingError(batchId: number, error: unknown, stage: string): Promise<void> {
    const errorMessage = error instanceof Error ? error.message : String(error);
    console.error(`[Batch ${batchId}] Error during ${stage} stage: ${errorMessage}`);
    
    try {
      await this.updateBatchStatus(batchId, ProcessStatus.FAILED);
      console.error(`[Batch ${batchId}] Status set to failed due to error in ${stage} stage.`);
    } catch (statusError) {
      console.error(
        `[Batch ${batchId}] Failed to update status to failed after error in ${stage} stage:`,
        statusError
      );
    }
  }

  /**
   * Updates the status of a batch in the database.
   * @param batchId The ID of the batch.
   * @param status The new status string.
   */
  private async updateBatchStatus(batchId: number, status: string): Promise<void> {
    try {
      const { error } = await this.supabaseClient
        .from('batch')
        .update({ batch_status: status, updated_at: new Date().toISOString() })
        .eq('batch_id', batchId);

      if (error) {
        console.error(`[Batch ${batchId}] Supabase batch status update error to '${status}':`, error);
      } else {
        console.log(`[Batch ${batchId}] Status updated to '${status}'.`);
      }
    } catch (error) {
      console.error(`[Batch ${batchId}] Exception when updating batch status to '${status}':`, error);
    }
  }

  /**
   * Fetches all screenshot records for a given batch ID.
   * @param batchId The ID of the batch.
   * @returns An array of screenshot records.
   */
  private async getBatchScreenshots(batchId: number): Promise<Screenshot[]> {
    try {
      const { data, error } = await this.supabaseClient
        .from('screenshot')
        .select('*')
        .eq('batch_id', batchId);

      if (error) {
        console.error(`[Batch ${batchId}] Supabase screenshot fetch error:`, error);
        return [];
      }

      return (data as Screenshot[] | null) || [];
    } catch (error) {
      console.error(`[Batch ${batchId}] Exception when fetching screenshots:`, error);
      return [];
    }
  }

  /**
   * Fetches image data as ArrayBuffer for each screenshot with a valid signed URL
   * @param screenshots Array of screenshot objects with screenshot_signed_url property
   * @returns The same array of screenshots with screenshot_image_buffer property populated
   */
  public async fetchScreenshotBuffers(
    screenshots: Screenshot[]
  ): Promise<Screenshot[]> {
    try {
      return fetchScreenshotBuffers(screenshots);
    } catch (error) {
      console.error(`Failed to fetch screenshot buffers:`, error);
      throw new Error(`Failed to fetch screenshot buffers: ${error instanceof Error ? error.message : String(error)}`);
    }
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/DatabaseService.ts
================
import { createClient, SupabaseClient } from '@supabase/supabase-js';
import { SUPABASE_URL, SUPABASE_KEY } from '../../config'; // Adjust path as needed

export class DatabaseService {
  private static instance: DatabaseService;
  private client: SupabaseClient;

  private constructor() {
    if (!SUPABASE_URL || !SUPABASE_KEY) {
      throw new Error('Supabase URL or Key is missing in config.ts');
    }
    // Use Database type if you have generated types, otherwise use 'any'
    this.client = createClient<any>(SUPABASE_URL, SUPABASE_KEY);
  }

  public static getInstance(): DatabaseService {
    if (!DatabaseService.instance) {
      DatabaseService.instance = new DatabaseService();
    }
    return DatabaseService.instance;
  }

  public getClient(): SupabaseClient {
    return this.client;
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/MetadataExtractionService.ts
================
import { ComponentDetectionResult, ElementDetectionItem } from '@/types/DetectionResult';
import { extract_component_metadata } from '@/lib/services/ai/OpenAIService';
import pLimit from 'p-limit';
import { createScreenshotTrackingContext } from '@/lib/logger';
import { VALIDATION_CONCURRENCY } from '@/lib/constants';

// Constants
const METADATA_EXTRACTION_CONCURRENCY = VALIDATION_CONCURRENCY; // Maximum number of concurrent metadata extraction operations
const COMPONENT_LEVEL_PROPERTIES = ['patternName', 'facetTags', 'states', 'interaction', 'userFlowImpact']; // Keys for component-level metadata

/**
 * MetadataExtractionService
 * 
 * This service extracts metadata from validated components by:
 * 1. Processing components in parallel with controlled concurrency
 * 2. For each component, converting image buffer to base64 and preparing structured input
 * 3. Calling OpenAI to extract metadata based on component and its elements
 * 4. Updating component and element metadata fields with the extracted information
 */
export class MetadataExtractionService {

  // Performs metadata extraction for all validated components
  public static async performMetadataExtraction(
    batchId: number,
    components: ComponentDetectionResult[]
  ): Promise<ComponentDetectionResult[]> {
    
    const extractionLimit = pLimit(METADATA_EXTRACTION_CONCURRENCY); // Limit the number of concurrent extractions
    const extractionPromises = components.map(component => 
      extractionLimit(() => this.extractComponentMetadata(batchId, component)) // Schedule extraction for each component
    );
    
    const enrichedComponents = await Promise.all(extractionPromises); // Wait for all extractions to complete
    
    console.log(`[Batch ${batchId}] Stage 4: Completed Metadata Extraction for all components`);
    
    return enrichedComponents; // Return components with updated metadata
  }

  // Extracts metadata for a single component
  private static async extractComponentMetadata(
    batchId: number, 
    component: ComponentDetectionResult
  ): Promise<ComponentDetectionResult> {
    const screenshotId = component.screenshot_id; // Get screenshot ID
    const componentName = component.component_name; // Get component name
        
    try {
      // Create context for logging and tracking (Dev purposes)
      const context = createScreenshotTrackingContext(batchId, screenshotId); 
      
      const imageBase64 = this.convertImageToBase64(component); // Convert image buffer to base64
      const inputPayload = this.prepareInputPayload(component); // Prepare input payload for AI service
      
      const metadataResult = await extract_component_metadata(
        imageBase64,
        JSON.stringify(inputPayload),
        context
      );
      
      // Update component with extracted metadata
      this.updateComponentWithMetadata(component, metadataResult.parsedContent); 
      
      console.log(`[Batch ${batchId}] Stage 4: Completed metadata extraction for component ${componentName}`);
      
      return component; // Return the component with updated metadata
    } catch (error) {
      console.error(`[Batch ${batchId}] Stage 4: Error extracting metadata for component ${componentName}:`, error);
      return component; // Return the component even if there's an error
    }
  }

  // Converts component image to base64
  private static convertImageToBase64(component: ComponentDetectionResult): string {
    const imageBuffer = component.original_image_object || component.annotated_image_object; // Use original or annotated image
    return imageBuffer.toString('base64'); // Convert buffer to base64 string
  }

  // Prepares input payload for the AI service
  private static prepareInputPayload(component: ComponentDetectionResult): any {
    return {
      component_name: component.component_name, // Include component name
      elements: component.elements.map(element => ({
        label: element.label, // Include element label
        description: element.description // Include element description
      }))
    };
  }
  
  /**
   * Updates component and element metadata with extracted information
   * 
   * @param component - Component to update
   * @param metadataData - Extracted metadata from OpenAI
   */
  private static updateComponentWithMetadata(
    component: ComponentDetectionResult,
    metadataData: any
  ): void {
    if (!this.validateMetadata(component, metadataData)) {
      return; // Exit if metadata is invalid
    }
    
    const componentMetadata = metadataData[component.component_name]; // Get metadata for the specific component
    
    try {
      this.updateComponentLevelMetadata(component, componentMetadata); // Update component-level metadata
      this.updateElementLevelMetadata(component, componentMetadata); // Update element-level metadata
    } catch (error) {
      console.error(`Failed to update component ${component.component_name} with metadata:`, error);
    }
  }

  /**
   * Validates that metadata exists and has the expected format
   */
  private static validateMetadata(component: ComponentDetectionResult, metadataData: any): boolean {
    if (!metadataData) {
      console.warn('Metadata data is null or undefined'); // Warn if metadata is missing
      return false;
    }
    
    const componentMetadata = metadataData[component.component_name];
    if (!componentMetadata) {
      console.warn(`No metadata found for component ${component.component_name}`); // Warn if specific component metadata is missing
      return false;
    }
    
    return true; // Metadata is valid
  }

  /**
   * Updates the component-level metadata
   */
  private static updateComponentLevelMetadata(
    component: ComponentDetectionResult,
    componentMetadata: any
  ): void {
    const { componentDescription, patternName, facetTags, states, interaction, userFlowImpact, flowPosition } = componentMetadata;
    
    component.component_metadata_extraction = JSON.stringify({
      patternName,
      facetTags,
      states,
      interaction,
      userFlowImpact,
      flowPosition
    }); // Serialize component-level metadata

    component.component_ai_description = componentDescription; // Update AI-generated description
  }


  // Updates the element-level metadata for each element in the component
  private static updateElementLevelMetadata(
    component: ComponentDetectionResult,
    componentMetadata: any
  ): void {
    const elementMap = this.createElementMap(component); // Create a map for quick element lookup
    
    Object.keys(componentMetadata).forEach(key => {
      if (COMPONENT_LEVEL_PROPERTIES.includes(key)) {
        return; // Skip component-level properties
      }
      
      const element = elementMap.get(key); // Get element by label
      if (element) {
        const elementMetadata = componentMetadata[key];
        element.element_metadata_extraction = JSON.stringify(elementMetadata); // Serialize and update element metadata
      }
    });
  }


  // Creates a map of elements by label for easier lookup
  private static createElementMap(component: ComponentDetectionResult): Map<string, ElementDetectionItem> {
    const elementMap = new Map<string, ElementDetectionItem>();
    component.elements.forEach(element => {
      elementMap.set(element.label, element); // Map element label to element object
    });
    return elementMap; // Return the map for element lookup
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/ParallelAnnotationService.ts
================
import { BatchProcessingScreenshot as Screenshot } from '@/types/BatchProcessingScreenshot';
import type { ComponentDetectionResult } from '@/types/DetectionResult';
import { Stage1Result } from '@/lib/services/ParallelExtractionService';
import { processAndSaveByCategory } from '@/lib/services/ai/MoondreamDetectionService';
import pLimit from 'p-limit';
import { MOONDREAM_CONCURRENCY } from '@/lib/constants';
import { createScreenshotTrackingContext } from '@/lib/logger';

// Helper function to wait for specified milliseconds
const wait = (ms: number) => new Promise(resolve => setTimeout(resolve, ms));

const MOONDREAM_COOLDOWN_MS = 2000; // Cooldown duration between each screenshot to avoid rate limiting

// ParallelMoondreamDetectionService processes screenshots using Moondream's vision models.
// It uses anchor labels from previous AI steps for object detection in UI screenshots.
export class ParallelMoondreamDetectionService {
  // Performs Moondream detection on screenshots using extracted AI components/elements.
  // @param batchId - ID of the batch being processed
  // @param screenshots - Screenshots to process, must have image buffers
  // @param stage1Results - AI extraction results from Stage 1
  // @returns Array of detection results
  public static async performMoondreamDetection(
    batchId: number,
    screenshots: Screenshot[],
    stage1Results: Map<number, Stage1Result>
  ): Promise<ComponentDetectionResult[]> {
    console.log(`[Batch ${batchId}] Stage 2: Starting Moondream Detection for ${screenshots.length} screenshots...`);
    
    // Filter out screenshots without image buffers
    const screenshotsWithBuffers = screenshots.filter(s => s.screenshot_image_buffer);
    if (screenshotsWithBuffers.length < screenshots.length) {
      console.warn(`[Batch ${batchId}] ${screenshots.length - screenshotsWithBuffers.length} screenshots missing image buffers. Only processing ${screenshotsWithBuffers.length}.`);
      
      if (screenshotsWithBuffers.length === 0) {
        throw new Error('No screenshots have image buffers. Cannot proceed with Moondream detection.');
      }
    }
    
    // Limit concurrency to manage resource usage
    const moondreamLimit = pLimit(MOONDREAM_CONCURRENCY);
    const allDetectionResults: ComponentDetectionResult[] = [];
    
    // Process each screenshot with controlled concurrency and cooling period
    const detectionPromises = screenshotsWithBuffers.map((screenshot, index) =>
      moondreamLimit(async () => {
        // Wait 2 seconds before processing each screenshot except the first
        if (index > 0) {
          console.log(`[Batch ${batchId}] Stage 2: Cooling down for ${MOONDREAM_COOLDOWN_MS / 1000} seconds before processing screenshot ${screenshot.screenshot_id}...`);
          await wait(MOONDREAM_COOLDOWN_MS);
        }

        const screenshotId = screenshot.screenshot_id;
        const buffer = screenshot.screenshot_image_buffer!;
        const screenshotUrl = screenshot.screenshot_signed_url || '';
        const stage1Data = stage1Results.get(screenshotId)!;
        const anchorLabels = stage1Data.anchorLabels;

        console.log(`[Batch ${batchId}] Stage 2: Moondream labelling screenshot ${screenshotId}...`);

        try {
          const results: ComponentDetectionResult[] = await processAndSaveByCategory(
            screenshotId,
            buffer,
            anchorLabels,
            batchId,
            screenshotUrl
          );
          console.log(`[Batch ${batchId}] Stage 2: Finished Moondream labelling for screenshot ${screenshotId}. Results count: ${results.length}`);
          return results; // Return results for this screenshot
        } catch (error) {
          console.error(`[Batch ${batchId}] Stage 2: Error in Moondream labelling for screenshot ${screenshotId}:`, error);
          return [];
        }
      })
    );

    // Wait for all detection tasks to complete
    const settledMoondreamResults = await Promise.allSettled(detectionPromises);
    console.log(`[Batch ${batchId}] Stage 2: All Moondream detection promises settled.`);

    // Aggregate successful detection results
    settledMoondreamResults.forEach(result => {
      if (result.status === 'fulfilled' && Array.isArray(result.value)) {
        allDetectionResults.push(...result.value);
      }
    });

    console.log(`[Batch ${batchId}] Stage 2: All Moondream detection results aggregated. Total components: ${allDetectionResults.length}`);
    return allDetectionResults;
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/ParallelExtractionService.ts
================
import { BatchProcessingScreenshot as Screenshot } from '@/types/BatchProcessingScreenshot';
// import { extract_component_from_image } from '@/lib/services/ai/OpenAIDirectService';
import { extract_component_from_image } from '@/lib/services/ai/OpenAIService';
import { extract_element_from_image, anchor_elements_from_image } from '@/lib/services/ai/ClaudeAIService';
import pLimit from 'p-limit';
import { EXTRACTION_CONCURRENCY } from '@/lib/constants';
import { createScreenshotTrackingContext, PromptTrackingContext } from '@/lib/logger';

// --- Types for intermediate results ---
export interface Stage1Result {
    componentSummaries: string[];
    elementResultRawText: string;
    anchorLabels: Record<string, string>;
    error?: any; // error tracking per screenshot
}

/**
 * AIExtractionService
 * 
 * This service handles the parallel extraction of components, elements, and anchors from screenshots
 * using multiple AI systems (OpenAI and Claude).
 * 
 * DESIGN DECISIONS:
 * 1. Parallel Processing: Screenshots are processed in parallel, but independently,errors are captured per 
 *    screenshot rather than failing the entire batch. This allows partial batch success.
 * 
 * 2. Progressive Enhancement: The extraction pipeline builds incrementally, with each step using 
 *    the results of the previous step:
 *    - Component extraction identifies high-level UI patterns
 *    - Element extraction uses components to find specific elements
 *    - Anchor labeling uses element data to establish reference points
 * 
 * 3. Results include error tracking to allow downstream processes to filter out
 *    failed operations and proceed with successful ones.
 */
export class AIExtractionService {
  /**
   * Extracts components, elements, and anchors from screenshots in parallel
   * 
   * TECHNICAL DETAILS:
   * - Maps screenshot IDs to their extraction results for later processing stages
   * - Progressive extraction: Components → Elements → Anchors
   * - Comprehensive error capture to prevent batch failure from individual items
   * 
   * @param batchId The ID of the batch being processed (for logging)
   * @param screenshots Array of screenshots with buffers and signed URLs
   * @returns Map of screenshot IDs to Stage1Result objects
   */
  public static async performAIExtraction(batchId: number, screenshots: Screenshot[]): Promise<Map<number, Stage1Result>> {
    const extractionLimit = pLimit(EXTRACTION_CONCURRENCY);
    const stage1Results = new Map<number, Stage1Result>();

    const extractionPromises = screenshots.map(screenshot =>
      extractionLimit(async () => {
        const screenshotId = screenshot.screenshot_id;
        const signedUrl = screenshot.screenshot_signed_url!;
        console.log(`[Batch ${batchId}] Stage 1: Processing screenshot ${screenshotId}...`);

        const context = createScreenshotTrackingContext(batchId, screenshotId);

        try {
          console.log(`[Batch ${batchId}][Screenshot ${screenshotId}] Step 1.1 : Extracting High-Level Components...`);

          // Step 1.1: Extract high-level components  
          const componentSummaries = await this.extractComponents(signedUrl, context, batchId, screenshotId);

          // Step 1.2: Extract detailed elements
          const elementResult = await this.extractElements(signedUrl, componentSummaries, context, batchId, screenshotId);

          // Step 1.3: Optimise descriptions for VLM detection
          const anchorLabels = await this.optimizeDescriptions(signedUrl, elementResult.rawText, context, batchId, screenshotId);

          if (Object.keys(anchorLabels).length === 0) {
            console.warn(`[Batch ${batchId}][Screenshot ${screenshotId}] No anchor labels generated. Moondream detection might be ineffective.`);
          }

          stage1Results.set(screenshotId, {
            componentSummaries,
            elementResultRawText: elementResult.rawText || '',
            anchorLabels,
          });
          console.log(`[Batch ${batchId}][Screenshot ${screenshotId}]Successfully processed screenshot ${screenshotId}. Found ${componentSummaries.length} Main Components, ${elementResult.parsedContent.length} Detailed Elements, ${Object.keys(anchorLabels).length} Optimised Labels.`);

        } catch (error) {
          console.error(`[Batch ${batchId}][Screenshot ${screenshotId}] Step 1.4 : Error processing screenshot ${screenshotId}:`, error);
          stage1Results.set(screenshotId, {
            componentSummaries: [],
            elementResultRawText: '',
            anchorLabels: {},
            error: error,
          });
        }
      })
    );

    await Promise.allSettled(extractionPromises);
    console.log(`[Batch ${batchId}] Completed Stage 1 AI extraction for all applicable screenshots.`);
    // console.log(`[Batch ${batchId}] Stage 1: Extraction complete. Results ${JSON.stringify(stage1Results, null, 2)}\n`);

    return stage1Results;
  }

  // Extracts high-level components from the image
  private static async extractComponents(signedUrl: string, context: PromptTrackingContext, batchId: number, screenshotId: number): Promise<string[]> {
    const componentResult = await extract_component_from_image(signedUrl, context);
    const componentSummaries = this.extractComponentSummaries(componentResult.parsedContent || []);
    console.log(`[Batch ${batchId}][Screenshot ${screenshotId}] Step 1.1 Complete. Found ${componentSummaries.length} Main Components.`);
    return componentSummaries;
  }

  // Extracts detailed elements based on components
  private static async extractElements(signedUrl: string, componentSummaries: string[], context: PromptTrackingContext, batchId: number, screenshotId: number): Promise<any> {
    console.log(`[Batch ${batchId}][Screenshot ${screenshotId}] Step 1.2 : Extracting Detailed Elements...`);
    const elementResult = await extract_element_from_image(signedUrl, componentSummaries.join('\n'), context);
    console.log(`[Batch ${batchId}][Screenshot ${screenshotId}] Step 1.2 Complete. Found ${elementResult.parsedContent.length} Detailed Elements.`);
    return elementResult;
  }

  // Optimizes descriptions for VLM detection
  private static async optimizeDescriptions(signedUrl: string, rawText: string, context: PromptTrackingContext, batchId: number, screenshotId: number): Promise<Record<string, string>> {
    console.log(`[Batch ${batchId}][Screenshot ${screenshotId}] Step 1.3 : Optimising descriptions for VLM detection`);
    const anchorResult = await anchor_elements_from_image(signedUrl, rawText, context);
    const anchorLabels: Record<string, string> = anchorResult.parsedContent || {};
    console.log(`[Batch ${batchId}][Screenshot ${screenshotId}] Step 1.3 Complete. Optimised ${Object.keys(anchorLabels).length} labels.`);
    return anchorLabels;
  }

  /**
   * Helper function to extract component summaries from AI extraction results
   * This is to pass to the element extraction step
   * 
   * @param components Array of components from AI extraction
   * @returns Array of component summary strings (just names for now)
   */
  private static extractComponentSummaries(components: any[]): string[] {
    if (!Array.isArray(components)) {
      console.warn("ExtractComponentSummaries: Expected an array of components, received:", typeof components);
      return [];
    }

    return components
      // Ensure component is an object and has the required string properties
      .filter(component =>
          typeof component === 'object' &&
          component !== null &&
          typeof component.component_name === 'string' &&
          typeof component.description === 'string' // Keep description check even if not used in output
      )
      .map(component => component.component_name); // Just using name now
  }
}

================
File: /Users/jess/Desktop/personal git/mobbin/formobbin/lib/services/SaveAnnotationService.ts
================
import { supabase } from '@/lib/supabase';
import { SupabaseClient } from '@supabase/supabase-js';
import { ComponentDetectionResult } from '@/types/DetectionResult';
import { ProcessStatus } from '@/lib/constants';
import { SUPABASE_BUCKET_NAME } from '@/config';
// Storage bucket name for Supabase
const STORAGE_BUCKET = SUPABASE_BUCKET_NAME || 'v6';

export class SaveAnnotationService {
  private supabaseClient: SupabaseClient;

  constructor(supabaseClient: SupabaseClient = supabase) {
    this.supabaseClient = supabaseClient;
  }

  /**
   * Persists component and element detection results to the database
   * @param batchId The ID of the batch
   * @param enrichedResults Array of component detection results with metadata
   */
  public async persistResults(
    batchId: number,
    enrichedResults: ComponentDetectionResult[]
  ): Promise<void> {
    console.log(`[Batch ${batchId}] Persisting ${enrichedResults.length} component results...`);
    
    // Process each component result
    for (const result of enrichedResults) {
      await this.saveComponentAnnotations(result);
    }
    
    console.log(`[Batch ${batchId}] Successfully persisted all component and element data.`);
  }

  /**
   * Persists a single component and its elements to the database
   * @param result The component detection result with elements
   */
  private async saveComponentAnnotations(
    result: ComponentDetectionResult
  ): Promise<void> {
    try {
      // Get screenshot_id from result
      const { screenshot_id } = result;
      if (!screenshot_id) {
        console.error(`Missing screenshot_id in component result`, result);
        return;
      }

      // 1. Upload annotated image to storage if it exists
      let screenshot_url = null;
      if (result.annotated_image_object) {
        screenshot_url = await this.uploadAnnotatedImage(result);
      }

      // 2. Insert component record
      const componentData = {
        screenshot_id,
        component_name: result.component_name || 'Unnamed Component',
        component_description: result.component_description || null,
        detection_status: result.detection_status || 'success',
        inference_time: result.inference_time || null,
        screenshot_url,
        component_metadata_extraction: result.component_metadata_extraction || null,
        component_ai_description: result.component_ai_description || null
      };

      const { data: component, error: componentError } = await this.supabaseClient
        .from('component')
        .insert(componentData)
        .select('component_id')
        .single();

      if (componentError) {
        console.error(`Failed to insert component record:`, componentError);
        return;
      }

      const component_id = component.component_id;

      // 3. Insert elements
      if (result.elements && result.elements.length > 0) {
        await this.saveElements(component_id, screenshot_id, result.elements);
      }
    } catch (error) {
      console.error(`Error persisting component result:`, error);
    }
  }

  /**
   * Uploads annotated image to storage and returns the public URL
   * @param result The component detection result containing annotated_image_object
   * @returns Public URL of the uploaded image
   */
  private async uploadAnnotatedImage(
    result: ComponentDetectionResult
  ): Promise<string | null> {
    try {
      if (!result.annotated_image_object) return null;

      // Generate a unique path for the annotated image
      const path = `annotated/${result.screenshot_id}/${Date.now()}_component.png`;
      
      // Upload the buffer to storage
      const { data, error } = await this.supabaseClient
        .storage
        .from(STORAGE_BUCKET)
        .upload(path, result.annotated_image_object, {
          contentType: 'image/png',
          upsert: true
        });

      if (error) {
        console.error(`Failed to upload annotated image:`, error);
        return null;
      }

      // Get public URL
      const { data: publicUrl } = this.supabaseClient
        .storage
        .from(STORAGE_BUCKET)
        .getPublicUrl(path);

      return publicUrl.publicUrl;
    } catch (error) {
      console.error(`Error uploading annotated image:`, error);
      return null;
    }
  }

  /**
   * Persists elements to the database
   * @param component_id The ID of the parent component
   * @param screenshot_id The ID of the screenshot
   * @param elements Array of elements to persist
   */
  private async saveElements(
    component_id: number,
    screenshot_id: number,
    elements: any[]
  ): Promise<void> {
    try {
      const elementRecords = elements.map(element => ({
        component_id,
        screenshot_id,
        element_label: element.label || null,
        element_description: element.description || null,
        element_status: element.status || 'Detected',
        element_hidden: element.hidden || false,
        bounding_box: element.bounding_box || {},
        suggested_coordinates: element.suggested_coordinates || null,
        element_accuracy_score: element.accuracy_score || null,
        element_explanation: element.explanation || null,
        element_vlm_model: element.vlm_model || null,
        element_metadata_extraction: element.element_metadata_extraction || null,
        element_error: element.error || null,
        element_inference_time: element.element_inference_time || null
      }));

      const { error } = await this.supabaseClient
        .from('element')
        .insert(elementRecords);

      if (error) {
        console.error(`Failed to insert element records:`, error);
      }
    } catch (error) {
      console.error(`Error persisting elements:`, error);
    }
  }
}
